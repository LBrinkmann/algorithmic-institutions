{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch as th\n",
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArtificialHumanEnv():\n",
    "    \"\"\"\n",
    "    Environment that runs the virtual humans and calculuates the value of the common good.\n",
    "\n",
    "    Indices:\n",
    "        t: agent types [0..1]\n",
    "    \"\"\"\n",
    "    state_dimensions = {\n",
    "        'punishments': ['agent'],\n",
    "        'contributions': ['agent'],\n",
    "        'payoffs': ['agent'],\n",
    "        'valid': ['agent'],\n",
    "        'common_good': ['agent'],\n",
    "        'episode_step': ['agent'],\n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "            self, *, contributors_model, n_agents, max_contribution, max_punishment, episode_steps, device):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            asdasd\n",
    "        \"\"\"\n",
    "        self.episode = 0\n",
    "        self.episode_steps = episode_steps\n",
    "        self.device = device\n",
    "        self.max_contribution = max_contribution\n",
    "        self.max_punishment = max_punishment\n",
    "        self.contributors_model = contributors_model\n",
    "        self.n_agents = n_agents\n",
    "        self.reset_state()\n",
    "\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.state = {\n",
    "            'punishments': th.zeros(self.n_agents, dtype=th.int16),\n",
    "            'contributions': th.zeros(self.n_agents, dtype=th.int16),\n",
    "            'payoffs': th.zeros(self.n_agents, dtype=th.float32),\n",
    "            'valid': th.zeros(self.n_agents, dtype=th.bool),\n",
    "            'common_good': th.tensor(0, dtype=th.float32),\n",
    "            'episode_step': th.tensor(0, dtype=th.int16)\n",
    "        }\n",
    "\n",
    "\n",
    "    def __getattr__(self, name):\n",
    "        if 'state' in self.__dict__:\n",
    "            state = self.__dict__['state']\n",
    "            return state[name]\n",
    "\n",
    "\n",
    "    def __setattr__(self, name, value):\n",
    "        if 'state' in self.__dict__:\n",
    "            if name in self.__dict__['state']:\n",
    "                self.state[name] = value\n",
    "            else:\n",
    "                object.__setattr__(self, name, value)\n",
    "        else:\n",
    "            object.__setattr__(self, name, value)\n",
    "\n",
    "    @staticmethod\n",
    "    def calc_common_good(contributions, punishments):\n",
    "        return contributions.sum() * 1.6 + punishments.sum()\n",
    "\n",
    "    @staticmethod\n",
    "    def calc_payout(contributions, punishments, commond_good):\n",
    "        # TODO: check how to handle missing values\n",
    "        return 20 - contributions - punishments + 0.25 * commond_good\n",
    "\n",
    "    def get_contributions(self):\n",
    "        contributions = self.contributors_model.act(**self.state)\n",
    "        return contributions\n",
    "\n",
    "    def init_episode(self):\n",
    "        self.episode += 1\n",
    "        self.episode_step = 0\n",
    "        self.reset_state()\n",
    "        self.contributions = self.get_contributions()\n",
    "        return self.state\n",
    "\n",
    "    def step(self, punishments):\n",
    "        self.episode_step += 1\n",
    "\n",
    "        assert punishments.max() <= self.max_punishment\n",
    "        assert punishments.dtype == th.int64\n",
    "\n",
    "        if (self.episode_step == self.episode_steps):\n",
    "            done = True\n",
    "        elif self.episode_step > self.episode_steps:\n",
    "            raise ValueError('Environment is done already.')\n",
    "        else:\n",
    "            done = False\n",
    "\n",
    "        self.punishments = punishments\n",
    "        self.common_good = self.calc_common_good(self.contributions, self.punishments)\n",
    "        self.payoffs = self.calc_payout(self.contributions, self.punishments, self.common_good)\n",
    "        self.contributions = self.get_contributions()\n",
    "\n",
    "        return self.state, self.common_good, done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# memory\n",
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "class Memory():\n",
    "    def __init__(self, device, n_episodes, n_episode_steps, output_file):\n",
    "        self.memory = None\n",
    "        self.n_episodes = n_episodes\n",
    "        self.n_episode_steps = n_episode_steps\n",
    "        self.device = device\n",
    "        self.output_file = output_file\n",
    "        self.current_row = 0\n",
    "        self.episode_queue = collections.deque([], maxlen=self.n_episodes)\n",
    "\n",
    "\n",
    "    def init_store(self, state):\n",
    "        self.memory = {\n",
    "            k: th.empty((self.n_episodes, self.n_episode_steps, *t.shape), dtype=t.dtype, device=self.device)\n",
    "            for k, t in state\n",
    "        } + {\n",
    "            'episode': th.empty((self.n_episodes, self.n_episode_steps), dtype=th.int64, device=self.device),\n",
    "            'episode_steps': th.empty((self.n_episodes, self.n_episode_steps), dtype=th.int64, device=self.device)\n",
    "        }\n",
    "\n",
    "    def next_episode(self, episode):\n",
    "        if self.current_row == (self.n_episodes - 1):\n",
    "            self.write()\n",
    "        self.current_row = (self.current_row + 1) % self.n_episodes\n",
    "        self.episode = episode\n",
    "        self.episode_queue.appendleft(self.current_row)\n",
    "\n",
    "    def add(self, state, episode_step):\n",
    "        self.memory['episode'][self.current_row,episode_step] = self.episode\n",
    "        self.memory['episode_steps'][self.current_row,episode_step] = episode_step\n",
    "        for k, t in state.items():\n",
    "            self.memory[k][self.current_row,episode_step] = t\n",
    "\n",
    "    def sample(self, batch_size, horizon, **kwargs):\n",
    "        eff_horizon = min(len(self), horizon)\n",
    "        relative_episode = np.random.choice(eff_horizon, batch_size, replace=False)\n",
    "        return self.get_relative(relative_episode, **kwargs)\n",
    "\n",
    "    def last(self, batch_size, **kwargs):\n",
    "        assert batch_size <= self.n_episodes\n",
    "        relative_episodes = np.arange(batch_size)\n",
    "        return self.get_relative(relative_episodes, **kwargs)\n",
    "\n",
    "    def get_relative(self, relative_episode, keys=None):\n",
    "        if keys is None:\n",
    "            keys = self.memory.keys()\n",
    "        hist_idx = th.tensor(\n",
    "            [self.episode_queue[rp] for rp in relative_episode], dtype=th.int64, device=self.device)\n",
    "        return {k: v[hist_idx] for k, v in self.memory.items() if k in keys}\n",
    "\n",
    "    def rec(self, state, episode, episode_steps):\n",
    "        if self.memory is None:\n",
    "            self.init_store(state)\n",
    "        self.add_state(state, episode, episode_steps)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.episode_queue)\n",
    "\n",
    "    def write(self):\n",
    "        if self.output_file:\n",
    "            th.save(\n",
    "                {\n",
    "                    k: t[:self.current_row] for k, t in self.memory.items()\n",
    "                },\n",
    "                f'{self.output_file}_{self.episode}.pt'\n",
    "            )\n",
    "\n",
    "    def __del__(self):\n",
    "        if hasattr(self, 'memory') and (self.current_row != 0):\n",
    "            self.write()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "\n",
    "def shift_obs(tensor_dict):\n",
    "    \"\"\"\n",
    "    Creates previous and current observations.\n",
    "\n",
    "    Args:\n",
    "        tensor_dict: each tensor need to have the episode_step dimension at second position\n",
    "    \"\"\"\n",
    "    previous = {k: t[:, :-1] for k, t in tensor_dict.items()}\n",
    "    current = {k: t[:, 1:] for k, t in tensor_dict.items()}\n",
    "    return previous, current\n",
    "\n",
    "\n",
    "class DQN():\n",
    "    def __init__(\n",
    "            self, *, manager_model_args, opt_args, gamma, target_update_freq, device):\n",
    "        self.device = device\n",
    "\n",
    "        self.policy_model = get_manager_model(evice=device, **manager_model_args).to(device)\n",
    "        self.target_model = get_manager_model(evice=device, **manager_model_args).to(device)\n",
    "\n",
    "        self.target_model.eval()\n",
    "        self.optimizer = th.optim.RMSprop(self.policy_model.parameters(), **opt_args)\n",
    "        self.gamma = gamma\n",
    "        self.target_update_freq = target_update_freq\n",
    "\n",
    "    def init_episode(self, episode):\n",
    "        if (episode % self.target_update_freq == 0):\n",
    "            # copy policy net to target net\n",
    "            self.target_model.load_state_dict(self.policy_model.state_dict())\n",
    "\n",
    "        # TODO: add for rnn\n",
    "        # self.policy_model.reset()\n",
    "        # self.target_model.reset()\n",
    "\n",
    "    def get_q(self, **observations):\n",
    "        with th.no_grad():\n",
    "            return self.policy_model(**observations)\n",
    "\n",
    "    def update(self, observations, actions, rewards):\n",
    "        previous_obs, current_obs = shift_obs(observations)\n",
    "\n",
    "        self.policy_model.reset()\n",
    "        self.target_model.reset()\n",
    "\n",
    "        policy_state_action_values = self.policy_model(\n",
    "            **previous_obs).gather(-1, actions.unsqueeze(-1))\n",
    "\n",
    "        next_state_values = th.zeros_like(rewards, device=self.device)\n",
    "        next_state_values = self.target_model(**current_obs).max(-1)[0].detach()\n",
    "\n",
    "        # Compute the expected Q values\n",
    "        expected_state_action_values = (next_state_values * self.gamma) + rewards\n",
    "\n",
    "        # Compute Huber loss\n",
    "        loss = th.nn.functional.smooth_l1_loss(policy_state_action_values,\n",
    "                                               expected_state_action_values.unsqueeze(-1))\n",
    "\n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        for param in self.policy_model.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "\n",
    "\n",
    "def int_to_ordinal(arr, n_levels):\n",
    "    \"\"\"\n",
    "    Turns a integer series into an ordinal encoding. \n",
    "    \"\"\"\n",
    "    encoding = np.array(\n",
    "        [[1]*i + [0]*(n_levels - i - 1)\n",
    "        for i in range(n_levels)]\n",
    "    )\n",
    "\n",
    "    return encoding[arr]\n",
    "\n",
    "\n",
    "def int_to_onehot(arr, n_levels):\n",
    "    out = np.zeros((arr.size, n_levels))\n",
    "    out[np.arange(arr.size),arr] = 1\n",
    "    return out\n",
    "\n",
    "\n",
    "def encoder(state, encodings):\n",
    "    return {\n",
    "        **state,\n",
    "        **{\n",
    "            k: encoder(state, **encoding) for k, encoding in encodings\n",
    "        }\n",
    "    }\n",
    "\n",
    "def joined_encoder(state, encoding):\n",
    "    encoding = [\n",
    "        encoder(df, **e)\n",
    "        for e in encodings\n",
    "    ]\n",
    "    return th.cat(encoding, axis=-1) \n",
    "\n",
    "\n",
    "def single_encode(tensor, encoding='numeric', n_levels=None):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    if encoding == 'ordinal':\n",
    "        assert n_levels is not None, 'Number of levels not provided.'\n",
    "        return int_to_ordinal(data.astype(int).values, n_levels)\n",
    "    elif encoding == 'onehot':\n",
    "        n_levels = len(data.cat.categories)\n",
    "        return int_to_onehot(data.astype(int).values, n_levels)\n",
    "    elif encoding == 'numeric':\n",
    "        val = data.astype(int).values\n",
    "        if add_axis:\n",
    "            val = val[:,np.newaxis]\n",
    "        return val\n",
    "    else:\n",
    "        raise ValueError(f\"Encoding type {encoding} is unknown.\")\n",
    "\n",
    "\n",
    "\n",
    "def eps_greedy(q_values, eps, device):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        q_values: Tensor of type `th.float` and arbitrary shape, last dimension reflect the actions.\n",
    "        eps: fraction of actions sampled at random\n",
    "    Returns:\n",
    "        actions: Tensor of type `th.long` and the same dimensions then q_values, besides of the last.\n",
    "    \"\"\"\n",
    "    n_actions = q_values.shape[-1]\n",
    "    actions_shape = q_values.shape[:-1]\n",
    "\n",
    "    greedy_actions = q_values.argmax(-1)\n",
    "    random_actions = th.randint(0, n_actions, size=actions_shape, device=device)\n",
    "\n",
    "    # random number which determine whether to take the random action\n",
    "    random_numbers = th.rand(size=actions_shape, device=device)\n",
    "    select_random = (random_numbers < eps).long()\n",
    "    picked_actions = select_random * random_actions + (1 - select_random) * greedy_actions\n",
    "\n",
    "    return picked_actions\n",
    "\n",
    "\n",
    "def run(env, controller, encoder, memory, n_episodes, eps, sample_args, device):\n",
    "    for episode in range(n_episodes):\n",
    "        print(f'Start episode {episode} in mode {episode}.')\n",
    "        state, rewards, done = env.init_episode()\n",
    "\n",
    "        # initialize episode for all controller\n",
    "        controller.init_episode(episode)\n",
    "\n",
    "        for step in count():\n",
    "            # Get observations\n",
    "            state_enc = encoder(state)\n",
    "\n",
    "            # Get q values from controller\n",
    "            q_values = controller.get_q(**state_enc)\n",
    "\n",
    "            # Sample a action\n",
    "            selected_action = eps_greedy(q_values=q_values, eps=eps, device=device)\n",
    "            # pass actions to environment and advance by one step\n",
    "            state, rewards, done = env.step(selected_action)\n",
    "            memory.add(episode_step=step, action=selected_action, rewards=rewards, **state_enc)\n",
    "\n",
    "            if done:\n",
    "                # allow all controller to update themself\n",
    "                sample = memory.random(**sample_args)\n",
    "                if sample is not None:\n",
    "                    controller.update(sample)\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from aimanager.model.neural.random import RandomArtificialHumans\n",
    "\n",
    "device = th.device('cpu')\n",
    "rec_device = th.device('cpu')\n",
    "rah = RandomArtificialHumans(device=device, max_contribution=20)\n",
    "\n",
    "env = ArtificialHumanEnv(\n",
    "    contributors_model=rah, n_agents=4, max_contribution=20, max_punishment=30, episode_steps=16, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'punishments': tensor([0, 0, 0, 0], dtype=torch.int16), 'contributions': tensor([ 6,  5, 19,  8]), 'payoffs': tensor([0., 0., 0., 0.]), 'valid': tensor([False, False, False, False]), 'common_good': tensor(0.), 'episode_step': tensor(0, dtype=torch.int16)}\n",
      "{'punishments': tensor([ 8,  7, 27,  8]), 'contributions': tensor([ 7, 14,  1, 19]), 'payoffs': tensor([33.7000, 35.7000,  1.7000, 31.7000]), 'valid': tensor([False, False, False, False]), 'common_good': tensor(110.8000), 'episode_step': tensor(1, dtype=torch.int16)} tensor(110.8000) False\n",
      "{'punishments': tensor([28,  8,  7,  0]), 'contributions': tensor([15, 18,  2, 16]), 'payoffs': tensor([12.1500, 25.1500, 39.1500, 28.1500]), 'valid': tensor([False, False, False, False]), 'common_good': tensor(108.6000), 'episode_step': tensor(2, dtype=torch.int16)} tensor(108.6000) False\n",
      "{'punishments': tensor([10, 27, 30, 10]), 'contributions': tensor([18,  6, 14, 17]), 'payoffs': tensor([34.6500, 14.6500, 27.6500, 33.6500]), 'valid': tensor([False, False, False, False]), 'common_good': tensor(158.6000), 'episode_step': tensor(3, dtype=torch.int16)} tensor(158.6000) False\n",
      "{'punishments': tensor([ 8,  7, 20, 12]), 'contributions': tensor([15, 13,  0,  0]), 'payoffs': tensor([27.7500, 40.7500, 19.7500, 24.7500]), 'valid': tensor([False, False, False, False]), 'common_good': tensor(135.), 'episode_step': tensor(4, dtype=torch.int16)} tensor(135.) False\n",
      "{'punishments': tensor([15, 18,  9, 14]), 'contributions': tensor([ 5, 19, 11, 18]), 'payoffs': tensor([15.2000, 14.2000, 36.2000, 31.2000]), 'valid': tensor([False, False, False, False]), 'common_good': tensor(100.8000), 'episode_step': tensor(5, dtype=torch.int16)} tensor(100.8000) False\n",
      "{'punishments': tensor([ 7, 16, 24, 25]), 'contributions': tensor([ 7,  1, 14, 15]), 'payoffs': tensor([47.2000, 24.2000, 24.2000, 16.2000]), 'valid': tensor([False, False, False, False]), 'common_good': tensor(156.8000), 'episode_step': tensor(6, dtype=torch.int16)} tensor(156.8000) False\n",
      "{'punishments': tensor([27, 18, 19, 13]), 'contributions': tensor([ 0, 13,  8, 17]), 'payoffs': tensor([20.0500, 35.0500, 21.0500, 26.0500]), 'valid': tensor([False, False, False, False]), 'common_good': tensor(136.2000), 'episode_step': tensor(7, dtype=torch.int16)} tensor(136.2000) False\n",
      "{'punishments': tensor([15, 26, 28, 11]), 'contributions': tensor([4, 5, 3, 2]), 'payoffs': tensor([40.2000, 16.2000, 19.2000, 27.2000]), 'valid': tensor([False, False, False, False]), 'common_good': tensor(140.8000), 'episode_step': tensor(8, dtype=torch.int16)} tensor(140.8000) False\n",
      "{'punishments': tensor([10,  9,  7,  4]), 'contributions': tensor([ 5,  9, 19, 10]), 'payoffs': tensor([19.1000, 19.1000, 23.1000, 27.1000]), 'valid': tensor([False, False, False, False]), 'common_good': tensor(52.4000), 'episode_step': tensor(9, dtype=torch.int16)} tensor(52.4000) False\n",
      "{'punishments': tensor([10,  0, 10,  0]), 'contributions': tensor([1, 7, 9, 7]), 'payoffs': tensor([27.2000, 33.2000, 13.2000, 32.2000]), 'valid': tensor([False, False, False, False]), 'common_good': tensor(88.8000), 'episode_step': tensor(10, dtype=torch.int16)} tensor(88.8000) False\n",
      "{'punishments': tensor([ 9, 16,  4, 14]), 'contributions': tensor([ 4, 16, 14, 14]), 'payoffs': tensor([30.3500, 17.3500, 27.3500, 19.3500]), 'valid': tensor([False, False, False, False]), 'common_good': tensor(81.4000), 'episode_step': tensor(11, dtype=torch.int16)} tensor(81.4000) False\n",
      "{'punishments': tensor([26, 25, 21, 10]), 'contributions': tensor([16, 19, 11, 11]), 'payoffs': tensor([29.7000, 18.7000, 24.7000, 35.7000]), 'valid': tensor([False, False, False, False]), 'common_good': tensor(158.8000), 'episode_step': tensor(12, dtype=torch.int16)} tensor(158.8000) False\n",
      "{'punishments': tensor([11, 11, 14, 28]), 'contributions': tensor([ 8,  2, 12, 14]), 'payoffs': tensor([31.8000, 28.8000, 33.8000, 19.8000]), 'valid': tensor([False, False, False, False]), 'common_good': tensor(155.2000), 'episode_step': tensor(13, dtype=torch.int16)} tensor(155.2000) False\n",
      "{'punishments': tensor([ 3, 26, 12,  4]), 'contributions': tensor([17, 12, 17,  8]), 'payoffs': tensor([34.6500, 17.6500, 21.6500, 27.6500]), 'valid': tensor([False, False, False, False]), 'common_good': tensor(102.6000), 'episode_step': tensor(14, dtype=torch.int16)} tensor(102.6000) False\n",
      "{'punishments': tensor([ 0, 18,  0, 24]), 'contributions': tensor([ 4, 16, 18,  8]), 'payoffs': tensor([35.1000, 22.1000, 35.1000, 20.1000]), 'valid': tensor([False, False, False, False]), 'common_good': tensor(128.4000), 'episode_step': tensor(15, dtype=torch.int16)} tensor(128.4000) False\n",
      "{'punishments': tensor([ 5, 19,  8, 21]), 'contributions': tensor([14, 13,  9,  9]), 'payoffs': tensor([42.6500, 16.6500, 25.6500, 22.6500]), 'valid': tensor([False, False, False, False]), 'common_good': tensor(126.6000), 'episode_step': tensor(16, dtype=torch.int16)} tensor(126.6000) True\n"
     ]
    }
   ],
   "source": [
    "state = env.init_episode()\n",
    "print(state)\n",
    "done = False\n",
    "while not done:\n",
    "    punishments = th.randint(0, 31, (4,), device=device)\n",
    "    state, reward, done = env.step(punishments)\n",
    "    print(state, reward, done)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b1198fd9370ee0cf82025240fa26724f68bfab1e3f74dbb4acdc06e7861d0dbe"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
