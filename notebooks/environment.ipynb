{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "686847be",
   "metadata": {
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "artificial_humans = \"../data/dev/data/model.pt\"\n",
    "output_file = \"../data/dev/data/manager_run.pt\"\n",
    "model_args = {\"n_layers\": 2, \"hidden_size\": 5}\n",
    "opt_args = {'lr': 0.01}\n",
    "gamma = 0.50\n",
    "eps = 0.1\n",
    "target_update_freq = 10\n",
    "n_episodes = 1000\n",
    "sample_args = {\n",
    "    'batch_size': 10,\n",
    "    'horizon': 100\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "42c92424",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch as th\n",
    "from aimanager.model.memory import Memory\n",
    "from aimanager.model.environment import ArtificialHumanEnv\n",
    "from aimanager.model.artificial_humans import ArtificialHuman\n",
    "from aimanager.model.neural.mlp import MultiLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "633a9dcd",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import numpy as np\n",
    "\n",
    "def shift_obs(tensor_dict):\n",
    "    \"\"\"\n",
    "    Creates previous and current observations.\n",
    "\n",
    "    Args:\n",
    "        tensor_dict: each tensor need to have the episode_step dimension at second position\n",
    "    \"\"\"\n",
    "    previous = {k: t[:, :-1] for k, t in tensor_dict.items()}\n",
    "    current = {k: t[:, 1:] for k, t in tensor_dict.items()}\n",
    "    return previous, current\n",
    "\n",
    "\n",
    "class ArtificalManager():\n",
    "    def __init__(\n",
    "            self, *, n_contributions, n_punishments, model_args, opt_args, gamma, target_update_freq, device):\n",
    "        self.device = device\n",
    "        input_size = n_contributions\n",
    "        self.policy_model = MultiLayer(output_size=n_punishments, input_size=input_size, **model_args).to(device)\n",
    "        self.target_model = MultiLayer(output_size=n_punishments, input_size=input_size, **model_args).to(device)\n",
    "\n",
    "        self.target_model.eval()\n",
    "        self.optimizer = th.optim.RMSprop(self.policy_model.parameters(), **opt_args)\n",
    "        self.gamma = gamma\n",
    "        self.target_update_freq = target_update_freq\n",
    "        self.n_contributions = n_contributions\n",
    "        self.n_punishments = n_punishments\n",
    "\n",
    "    def init_episode(self, episode):\n",
    "        if (episode % self.target_update_freq == 0):\n",
    "            # copy policy net to target net\n",
    "            self.target_model.load_state_dict(self.policy_model.state_dict())\n",
    "\n",
    "        # TODO: add for rnn\n",
    "        # self.policy_model.reset()\n",
    "        # self.target_model.reset()\n",
    "\n",
    "    def encode_obs(self, contributions, **_):\n",
    "        return {\n",
    "            'manager_observations': th.nn.functional.one_hot(contributions, num_classes=self.n_contributions).float()\n",
    "        }\n",
    "\n",
    "    def get_q(self, manager_observations, **_):\n",
    "        with th.no_grad():\n",
    "            return self.policy_model(manager_observations)\n",
    "\n",
    "    def act(self, **state):\n",
    "        obs = self.encode_obs(**state)\n",
    "        q = self.get_q(**obs)\n",
    "        return q.argmax(dim=-1)\n",
    "\n",
    "    def eps_greedy(self, q_values, eps):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            q_values: Tensor of type `th.float` and arbitrary shape, last dimension reflect the actions.\n",
    "            eps: fraction of actions sampled at random\n",
    "        Returns:\n",
    "            actions: Tensor of type `th.long` and the same dimensions then q_values, besides of the last.\n",
    "        \"\"\"\n",
    "        n_actions = q_values.shape[-1]\n",
    "        actions_shape = q_values.shape[:-1]\n",
    "\n",
    "        greedy_actions = q_values.argmax(-1)\n",
    "        random_actions = th.randint(0, n_actions, size=actions_shape, device=self.device)\n",
    "\n",
    "        # random number which determine whether to take the random action\n",
    "        random_numbers = th.rand(size=actions_shape, device=device)\n",
    "        select_random = (random_numbers < eps).long()\n",
    "        picked_actions = select_random * random_actions + (1 - select_random) * greedy_actions\n",
    "\n",
    "        return picked_actions\n",
    "\n",
    "\n",
    "    def update(self, actions, rewards, manager_observations, **_):\n",
    "        previous_obs = manager_observations[:, :-1]\n",
    "        current_obs = manager_observations[:, 1:]\n",
    "        actions = actions[:, 1:]\n",
    "        rewards = rewards[:, 1:]\n",
    "        # current = {k: t[:, 1:] for k, t in tensor_dict.items()}\n",
    "\n",
    "\n",
    "        # t[:, :-1]t[:, :-1]\n",
    "        # previous_obs, current_obs = shift_obs(obs)\n",
    "\n",
    "        # self.policy_model.reset()\n",
    "        # self.target_model.reset()\n",
    "\n",
    "        policy_state_action_values = self.policy_model(\n",
    "            previous_obs).gather(-1, actions.unsqueeze(-1))\n",
    "\n",
    "        next_state_values = th.zeros_like(rewards, device=self.device)\n",
    "        next_state_values = self.target_model(current_obs).max(-1)[0].detach()\n",
    "\n",
    "        # Compute the expected Q values\n",
    "        expected_state_action_values = (next_state_values * self.gamma) + rewards[:,:,np.newaxis] * 0.25\n",
    "\n",
    "        # Compute Huber loss\n",
    "        loss = th.nn.functional.smooth_l1_loss(policy_state_action_values,\n",
    "                                               expected_state_action_values.unsqueeze(-1))\n",
    "\n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        for param in self.policy_model.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "d49886d5",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from aimanager.model.neural.random import RandomArtificialHumans\n",
    "\n",
    "device = th.device('cpu')\n",
    "rec_device = th.device('cpu')\n",
    "# rah = RandomArtificialHumans(device=device, max_contribution=20)\n",
    "artifical_humans = ArtificialHuman.load(artificial_humans)\n",
    "\n",
    "env = ArtificialHumanEnv(\n",
    "    artifical_humans=artifical_humans, n_agents=4, n_contributions=21, n_punishments=31, episode_steps=16, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "4aa5ac07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# state = env.init_episode()\n",
    "# print(state)\n",
    "# done = False\n",
    "# while not done:\n",
    "#     punishments = th.randint(0, 31, (4,), device=device)\n",
    "#     state, reward, done = env.step(punishments)\n",
    "#     print(state, reward, done)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "02d293a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start episode 0.\n",
      "Start episode 10.\n",
      "Start episode 20.\n",
      "Start episode 30.\n",
      "Start episode 40.\n",
      "Start episode 50.\n",
      "Start episode 60.\n",
      "Start episode 70.\n",
      "Start episode 80.\n",
      "Start episode 90.\n",
      "Start episode 100.\n",
      "Start episode 110.\n",
      "Start episode 120.\n",
      "Start episode 130.\n",
      "Start episode 140.\n",
      "Start episode 150.\n",
      "Start episode 160.\n",
      "Start episode 170.\n",
      "Start episode 180.\n",
      "Start episode 190.\n",
      "Start episode 200.\n",
      "Start episode 210.\n",
      "Start episode 220.\n",
      "Start episode 230.\n",
      "Start episode 240.\n",
      "Start episode 250.\n",
      "Start episode 260.\n",
      "Start episode 270.\n",
      "Start episode 280.\n",
      "Start episode 290.\n",
      "Start episode 300.\n",
      "Start episode 310.\n",
      "Start episode 320.\n",
      "Start episode 330.\n",
      "Start episode 340.\n",
      "Start episode 350.\n",
      "Start episode 360.\n",
      "Start episode 370.\n",
      "Start episode 380.\n",
      "Start episode 390.\n",
      "Start episode 400.\n",
      "Start episode 410.\n",
      "Start episode 420.\n",
      "Start episode 430.\n",
      "Start episode 440.\n",
      "Start episode 450.\n",
      "Start episode 460.\n",
      "Start episode 470.\n",
      "Start episode 480.\n",
      "Start episode 490.\n",
      "Start episode 500.\n",
      "Start episode 510.\n",
      "Start episode 520.\n",
      "Start episode 530.\n",
      "Start episode 540.\n",
      "Start episode 550.\n",
      "Start episode 560.\n",
      "Start episode 570.\n",
      "Start episode 580.\n",
      "Start episode 590.\n",
      "Start episode 600.\n",
      "Start episode 610.\n",
      "Start episode 620.\n",
      "Start episode 630.\n",
      "Start episode 640.\n",
      "Start episode 650.\n",
      "Start episode 660.\n",
      "Start episode 670.\n",
      "Start episode 680.\n",
      "Start episode 690.\n",
      "Start episode 700.\n",
      "Start episode 710.\n",
      "Start episode 720.\n",
      "Start episode 730.\n",
      "Start episode 740.\n",
      "Start episode 750.\n",
      "Start episode 760.\n",
      "Start episode 770.\n",
      "Start episode 780.\n",
      "Start episode 790.\n",
      "Start episode 800.\n",
      "Start episode 810.\n",
      "Start episode 820.\n",
      "Start episode 830.\n",
      "Start episode 840.\n",
      "Start episode 850.\n",
      "Start episode 860.\n",
      "Start episode 870.\n",
      "Start episode 880.\n",
      "Start episode 890.\n",
      "Start episode 900.\n",
      "Start episode 910.\n",
      "Start episode 920.\n",
      "Start episode 930.\n",
      "Start episode 940.\n",
      "Start episode 950.\n",
      "Start episode 960.\n",
      "Start episode 970.\n",
      "Start episode 980.\n",
      "Start episode 990.\n"
     ]
    }
   ],
   "source": [
    "from itertools import count\n",
    "\n",
    "manager = ArtificalManager(\n",
    "    n_contributions=21, n_punishments=31, model_args=model_args, opt_args=opt_args, gamma=gamma, \n",
    "    target_update_freq=target_update_freq, device=device)\n",
    "\n",
    "memory = Memory(n_episodes=n_episodes, n_episode_steps=16, output_file=output_file, device=device)\n",
    "\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "    if episode % 10 == 0:\n",
    "        print(f'Start episode {episode}.')\n",
    "    state = env.init_episode()\n",
    "\n",
    "    # initialize episode for all controller\n",
    "    manager.init_episode(episode)\n",
    "    memory.next_episode(episode)\n",
    "\n",
    "    for step in count():\n",
    "        # Get observations\n",
    "        state_enc = manager.encode_obs(**state)\n",
    "\n",
    "        # Get q values from controller\n",
    "        q_values = manager.get_q(**state_enc)\n",
    "\n",
    "        # Sample a action\n",
    "        selected_actions = manager.eps_greedy(q_values=q_values, eps=eps)\n",
    "        # pass actions to environment and advance by one step\n",
    "        state, rewards, done = env.step(selected_actions)\n",
    "        memory.add(episode_step=step, actions=selected_actions, rewards=rewards, **state_enc)\n",
    "\n",
    "        if done:\n",
    "            # allow all controller to update themself\n",
    "            sample = memory.sample(**sample_args)\n",
    "            if sample is not None:\n",
    "                manager.update(**sample)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "3c4b0de5",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = {\n",
    "    'contributions': th.arange(0,21)\n",
    "}\n",
    "\n",
    "# obs = manager.\n",
    "\n",
    "data['punishments'] = manager.act(**data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "1f8deff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='contributions', ylabel='punishments'>"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "sns.set(rc={'axes.facecolor':'white', 'figure.facecolor':'white'})\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "sns.scatterplot(data=data, x='contributions', y='punishments')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b1198fd9370ee0cf82025240fa26724f68bfab1e3f74dbb4acdc06e7861d0dbe"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "papermill": {
   "default_parameters": {},
   "environment_variables": {},
   "input_path": "environment.ipynb",
   "output_path": "environment.ipynb",
   "parameters": {
    "artificial_humans": "../data/dev/data/model.pt"
   },
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
