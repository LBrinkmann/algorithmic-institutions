{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c786fc7",
   "metadata": {
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "artificial_humans = \"../../data/training/ah_10/data/model.pt\"\n",
    "artificial_humans_model = \"graph\"\n",
    "output_path = \"../../data/manager/v2/dev/\"\n",
    "manager_args = {\n",
    "    \"opt_args\": {\"lr\": 0.003},\n",
    "    \"gamma\": 1.0,\n",
    "    \"eps\": 0.2,\n",
    "    \"target_update_freq\": 20,\n",
    "    \"model_args\": {\n",
    "        \"hidden_size\": 5,\n",
    "        \"add_rnn\": True,\n",
    "        \"add_edge_model\": True,\n",
    "        \"add_global_model\": False,\n",
    "        \"x_encoding\": [\n",
    "            {\"name\": \"prev_contributions\", \"n_levels\": 21, \"encoding\": \"numeric\"},\n",
    "            {\"name\": \"prev_punishments\", \"n_levels\": 31, \"encoding\": \"numeric\"},\n",
    "        ],\n",
    "        \"u_encoding\": [\n",
    "            {\"name\": \"round_number\", \"n_levels\": 16, \"encoding\": \"numeric\"},\n",
    "            {\"name\": \"prev_common_good\", \"norm\": 128, \"etype\": \"float\"},\n",
    "        ],\n",
    "    },\n",
    "}\n",
    "replay_memory_args = {\"n_episodes\": 20, \"sample_size\": 5}\n",
    "n_update_steps = 10\n",
    "eval_args = {\"test_period\": 10, \"batch_size\": 1000}\n",
    "env_args = {\n",
    "    \"n_agents\": 4,\n",
    "    \"n_contributions\": 21,\n",
    "    \"n_punishments\": 31,\n",
    "    \"episode_steps\": 16,\n",
    "    \"batch_size\": 10,\n",
    "}\n",
    "device = \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42c92424",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch as th\n",
    "import os\n",
    "from itertools import count\n",
    "\n",
    "from aimanager.manager.memory import Memory\n",
    "from aimanager.manager.environment import ArtificialHumanEnv\n",
    "from aimanager.artificial_humans import AH_MODELS\n",
    "from aimanager.manager.manager import ArtificalManager\n",
    "from aimanager.manager.graph_memory import GraphMemory\n",
    "from aimanager.manager.evaluation import ManagerEvaluator\n",
    "\n",
    "from torch_geometric.data import Batch\n",
    "\n",
    "\n",
    "output_file =  os.path.join(output_path, 'data', \"eval\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73950fb5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'state' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/levinbrinkmann/repros/algorithmic-institutions/notebooks/manager/rl_manager_v2.ipynb Cell 3'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/levinbrinkmann/repros/algorithmic-institutions/notebooks/manager/rl_manager_v2.ipynb#ch0000003?line=0'>1</a>\u001b[0m state\n",
      "\u001b[0;31mNameError\u001b[0m: name 'state' is not defined"
     ]
    }
   ],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02d293a0",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 1])\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "The shape of the mask [10, 4] at index 0 does not match the shape of the indexed tensor [40, 1] at index 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/levinbrinkmann/repros/algorithmic-institutions/notebooks/manager/rl_manager_v2.ipynb Cell 4'\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/levinbrinkmann/repros/algorithmic-institutions/notebooks/manager/rl_manager_v2.ipynb#ch0000002?line=29'>30</a>\u001b[0m \u001b[39m# Sample a action\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/levinbrinkmann/repros/algorithmic-institutions/notebooks/manager/rl_manager_v2.ipynb#ch0000002?line=30'>31</a>\u001b[0m selected_action \u001b[39m=\u001b[39m manager\u001b[39m.\u001b[39meps_greedy(q_values\u001b[39m=\u001b[39mq_values)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/levinbrinkmann/repros/algorithmic-institutions/notebooks/manager/rl_manager_v2.ipynb#ch0000002?line=32'>33</a>\u001b[0m state \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mpunish(selected_action)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/levinbrinkmann/repros/algorithmic-institutions/notebooks/manager/rl_manager_v2.ipynb#ch0000002?line=33'>34</a>\u001b[0m recorder\u001b[39m.\u001b[39madd(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mstate, episode_step\u001b[39m=\u001b[39mstep)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/levinbrinkmann/repros/algorithmic-institutions/notebooks/manager/rl_manager_v2.ipynb#ch0000002?line=35'>36</a>\u001b[0m \u001b[39m# pass actions to environment and advance by one step\u001b[39;00m\n",
      "File \u001b[0;32m~/repros/algorithmic-institutions/aimanager/manager/environment.py:109\u001b[0m, in \u001b[0;36mArtificialHumanEnv.punish\u001b[0;34m(self, punishments)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[39massert\u001b[39;00m punishments\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m th\u001b[39m.\u001b[39mint64\n\u001b[1;32m    108\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpunishments \u001b[39m=\u001b[39m punishments\n\u001b[0;32m--> 109\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommon_good \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcalc_common_good(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcontributions, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpunishments, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvalid)\n\u001b[1;32m    110\u001b[0m \u001b[39m# self.payoffs = self.calc_payout(self.contributions, self.punishments, self.common_good, self.valid)\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\n",
      "File \u001b[0;32m~/repros/algorithmic-institutions/aimanager/manager/environment.py:76\u001b[0m, in \u001b[0;36mArtificialHumanEnv.calc_common_good\u001b[0;34m(contributions, punishments, valid)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m     75\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcalc_common_good\u001b[39m(contributions, punishments, valid):\n\u001b[0;32m---> 76\u001b[0m     \u001b[39mreturn\u001b[39;00m (contributions[valid] \u001b[39m*\u001b[39m \u001b[39m1.6\u001b[39m \u001b[39m-\u001b[39m punishments[valid])\u001b[39m.\u001b[39mmean() \u001b[39m*\u001b[39m th\u001b[39m.\u001b[39mones_like(punishments, dtype\u001b[39m=\u001b[39mth\u001b[39m.\u001b[39mfloat)\n",
      "\u001b[0;31mIndexError\u001b[0m: The shape of the mask [10, 4] at index 0 does not match the shape of the indexed tensor [40, 1] at index 0"
     ]
    }
   ],
   "source": [
    "device = th.device(device)\n",
    "artifical_humans = AH_MODELS[artificial_humans_model].load(artificial_humans).to(device)\n",
    "\n",
    "env = ArtificialHumanEnv(\n",
    "    artifical_humans=artifical_humans, device=device, **env_args)\n",
    "\n",
    "n_episode_steps = env.episode_steps\n",
    "\n",
    "manager = ArtificalManager(\n",
    "    n_contributions=env.n_contributions, n_punishments=env.n_punishments, \n",
    "    default_values=artifical_humans.default_values, device=device, **manager_args)\n",
    "\n",
    "# replay_mem = GraphMemory(n_episode_steps=env.episode_steps, n_nodes=env.n_agents, device=device, **replay_memory_args)\n",
    "# recorder = Memory(n_episodes=n_episodes, n_episode_steps=env.episode_steps, device=device)\n",
    "# evaluator = ManagerEvaluator(n_update_steps, env.episode_steps, output_file=output_file, **eval_args)\n",
    "\n",
    "for update_step in range(n_update_steps):\n",
    "\n",
    "    state = env.init_episode()\n",
    "\n",
    "    manager.init_episode(update_step)\n",
    "\n",
    "    for step in count():\n",
    "        state_ = {k: v.unsqueeze(-1) for k, v in state.items()}\n",
    "        obs = Batch.from_data_list(manager.encode(state_, edge_index=env.edge_index))\n",
    "\n",
    "        # Get q values from controller\n",
    "        q_values = manager.get_q(manager_observations=obs, first=step == 0).squeeze(1)\n",
    "\n",
    "        # Sample a action\n",
    "        selected_action = manager.eps_greedy(q_values=q_values)\n",
    "\n",
    "        state = env.punish(selected_action)\n",
    "        recorder.add(**state, episode_step=step)\n",
    "\n",
    "        # pass actions to environment and advance by one step\n",
    "        state, reward, done = env.step()\n",
    "        replay_mem.add(\n",
    "            action=selected_action, reward=reward, \n",
    "            obs=obs)\n",
    " \n",
    "        if done:\n",
    "            replay_mem.next_episode(episode)\n",
    "            \n",
    "            # allow manager to update itself\n",
    "            sample = replay_mem.sample(**sample_args)\n",
    "            \n",
    "\n",
    "            if sample is not None:\n",
    "                manager.update(**sample)\n",
    "            break\n",
    "    # recorder.add(episode_step=step, **state)\n",
    "    # recorder.next_episode(episode)\n",
    "    evaluator.eval_episode(manager, env, episode)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "papermill": {
   "default_parameters": {},
   "environment_variables": {},
   "input_path": "notebooks/manager/rl_manager_v2.ipynb",
   "output_path": "notebooks/manager/rl_manager_v2.ipynb",
   "parameters": {
    "artificial_humans": "../../data/training/ah_10/data/model.pt",
    "artificial_humans_model": "graph",
    "device": "cpu",
    "env_args": {
     "batch_size": 10,
     "episode_steps": 16,
     "n_agents": 4,
     "n_contributions": 21,
     "n_punishments": 31
    },
    "eval_args": {
     "batch_size": 1000,
     "test_period": 10
    },
    "manager_args": {
     "eps": 0.2,
     "gamma": 1,
     "model_args": {
      "add_edge_model": true,
      "add_global_model": false,
      "add_rnn": true,
      "hidden_size": 5,
      "u_encoding": [
       {
        "encoding": "numeric",
        "n_levels": 16,
        "name": "round_number"
       },
       {
        "etype": "float",
        "name": "prev_common_good",
        "norm": 128
       }
      ],
      "x_encoding": [
       {
        "encoding": "numeric",
        "n_levels": 21,
        "name": "prev_contributions"
       },
       {
        "encoding": "numeric",
        "n_levels": 31,
        "name": "prev_punishments"
       }
      ]
     },
     "opt_args": {
      "lr": 0.003
     },
     "target_update_freq": 20
    },
    "n_update_steps": 10,
    "output_path": "../../data/manager/v2/dev/",
    "replay_memory_args": {
     "n_episodes": 20,
     "sample_size": 5
    }
   },
   "version": "2.3.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "b1198fd9370ee0cf82025240fa26724f68bfab1e3f74dbb4acdc06e7861d0dbe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
