{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec2d06f5",
   "metadata": {
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "artificial_humans = \"../../data/training/ah_10/data/model.pt\"\n",
    "artificial_humans_model = \"graph\"\n",
    "output_file = \"../../data/training/dev/data/manager_run.pt\"\n",
    "model_args = {\n",
    "    \"hidden_size\": 5,\n",
    "    \"add_rnn\": False,\n",
    "    \"add_edge_model\": False,\n",
    "    \"add_global_model\": False,\n",
    "    \"x_encoding\": [\n",
    "        {\"name\": \"prev_contributions\", \"n_levels\": 21, \"encoding\": \"numeric\"},\n",
    "        {\"name\": \"prev_punishments\", \"n_levels\": 31, \"encoding\": \"numeric\"},\n",
    "    ],\n",
    "    \"u_encoding\": [\n",
    "        {\"name\": \"round_number\", \"n_levels\": 16, \"encoding\": \"numeric\"},\n",
    "        {\"name\": \"prev_common_good\", \"norm\": 128, \"etype\": \"float\"},\n",
    "    ],\n",
    "}\n",
    "opt_args = {\"lr\": 0.001}\n",
    "gamma = 0.5\n",
    "eps = 0.2\n",
    "target_update_freq = 50\n",
    "n_episode_steps = 16\n",
    "n_episodes = 1000\n",
    "memory_size = 100\n",
    "sample_args = {\"batch_size\": 10}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42c92424",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch as th\n",
    "from itertools import count\n",
    "\n",
    "from aimanager.manager.memory import Memory\n",
    "from aimanager.manager.environment import ArtificialHumanEnv\n",
    "from aimanager.artificial_humans import AH_MODELS\n",
    "from aimanager.manager.manager import ArtificalManager\n",
    "from aimanager.manager.graph_memory import GraphMemory\n",
    "\n",
    "from torch_geometric.data import Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02d293a0",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start episode 20. Avg common good 11.939531326293945. Avg reward -0.9015752673149109\n",
      "Start episode 40. Avg common good 9.67984390258789. Avg reward -1.0572751760482788\n",
      "Start episode 60. Avg common good 8.387187957763672. Avg reward -1.1661503314971924\n",
      "Start episode 80. Avg common good 9.258281707763672. Avg reward -1.1279315948486328\n",
      "Start episode 100. Avg common good 8.581250190734863. Avg reward -1.1378252506256104\n",
      "Start episode 120. Avg common good 8.917187690734863. Avg reward -1.1289315223693848\n",
      "Start episode 140. Avg common good 8.75718879699707. Avg reward -1.079856514930725\n",
      "Start episode 160. Avg common good 8.820467948913574. Avg reward -1.0949627161026\n",
      "Start episode 180. Avg common good 9.9701566696167. Avg reward -1.0554752349853516\n",
      "Start episode 200. Avg common good 9.670937538146973. Avg reward -1.072250247001648\n",
      "Start episode 220. Avg common good 9.470781326293945. Avg reward -1.0571690797805786\n",
      "Start episode 240. Avg common good 9.994531631469727. Avg reward -0.9980939626693726\n",
      "Start episode 260. Avg common good 9.319844245910645. Avg reward -0.9208565950393677\n",
      "Start episode 280. Avg common good 10.733124732971191. Avg reward -0.7048441171646118\n",
      "Start episode 300. Avg common good 10.148125648498535. Avg reward -0.7724190354347229\n",
      "Start episode 320. Avg common good 10.436250686645508. Avg reward -0.7488940358161926\n",
      "Start episode 340. Avg common good 10.164687156677246. Avg reward -0.7384815216064453\n",
      "Start episode 360. Avg common good 11.09375. Avg reward -0.6927627325057983\n",
      "Start episode 380. Avg common good 10.205781936645508. Avg reward -0.7212377786636353\n",
      "Start episode 400. Avg common good 11.194531440734863. Avg reward -0.6048377752304077\n",
      "Start episode 420. Avg common good 10.535625457763672. Avg reward -0.6785377264022827\n",
      "Start episode 440. Avg common good 11.238438606262207. Avg reward -0.598787784576416\n",
      "Start episode 460. Avg common good 11.075469017028809. Avg reward -0.6346815228462219\n",
      "Start episode 480. Avg common good 10.369531631469727. Avg reward -0.6640564799308777\n",
      "Start episode 500. Avg common good 11.176876068115234. Avg reward -0.6577252149581909\n",
      "Start episode 520. Avg common good 10.844843864440918. Avg reward -0.6015752553939819\n",
      "Start episode 540. Avg common good 11.776718139648438. Avg reward -0.656612753868103\n",
      "Start episode 560. Avg common good 10.666406631469727. Avg reward -0.6325440406799316\n",
      "Start episode 580. Avg common good 10.317968368530273. Avg reward -0.6845940351486206\n",
      "Start episode 600. Avg common good 10.926717758178711. Avg reward -0.6338814496994019\n",
      "Start episode 620. Avg common good 11.227968215942383. Avg reward -0.6379252672195435\n",
      "Start episode 640. Avg common good 11.701562881469727. Avg reward -0.5927128195762634\n",
      "Start episode 660. Avg common good 11.00374984741211. Avg reward -0.6378502249717712\n",
      "Start episode 680. Avg common good 10.652655601501465. Avg reward -0.6528939604759216\n",
      "Start episode 700. Avg common good 11.53515625. Avg reward -0.5845877528190613\n",
      "Start episode 720. Avg common good 11.20484447479248. Avg reward -0.5889690518379211\n",
      "Start episode 740. Avg common good 11.140469551086426. Avg reward -0.6252315640449524\n",
      "Start episode 760. Avg common good 10.438438415527344. Avg reward -0.7225252389907837\n",
      "Start episode 780. Avg common good 11.275781631469727. Avg reward -0.5507878065109253\n",
      "Start episode 800. Avg common good 10.651407241821289. Avg reward -0.5378003120422363\n",
      "Start episode 820. Avg common good 10.544843673706055. Avg reward -0.671581506729126\n",
      "Start episode 840. Avg common good 11.131874084472656. Avg reward -0.5868127942085266\n",
      "Start episode 860. Avg common good 10.856405258178711. Avg reward -0.6302377581596375\n",
      "Start episode 880. Avg common good 11.120156288146973. Avg reward -0.6440378427505493\n",
      "Start episode 900. Avg common good 11.011406898498535. Avg reward -0.6026752591133118\n",
      "Start episode 920. Avg common good 10.45093822479248. Avg reward -0.6773003339767456\n",
      "Start episode 940. Avg common good 11.244218826293945. Avg reward -0.5744940042495728\n",
      "Start episode 960. Avg common good 10.515312194824219. Avg reward -0.6247628331184387\n",
      "Start episode 980. Avg common good 11.100781440734863. Avg reward -0.6323190331459045\n"
     ]
    }
   ],
   "source": [
    "device = th.device('cpu')\n",
    "rec_device = th.device('cpu')\n",
    "artifical_humans = AH_MODELS[artificial_humans_model].load(artificial_humans).to(device)\n",
    "\n",
    "env = ArtificialHumanEnv(\n",
    "    artifical_humans=artifical_humans, n_agents=4, n_contributions=21, n_punishments=31, episode_steps=n_episode_steps, device=device)\n",
    "\n",
    "manager = ArtificalManager(\n",
    "    n_contributions=21, n_punishments=31, model_args=model_args, opt_args=opt_args, gamma=gamma, \n",
    "    target_update_freq=target_update_freq, default_values=artifical_humans.default_values, device=device)\n",
    "\n",
    "replay_mem = GraphMemory(n_episodes=memory_size, n_episode_steps=n_episode_steps, n_nodes=4, device=device)\n",
    "recorder = Memory(n_episodes=n_episodes, n_episode_steps=n_episode_steps, output_file=output_file, device=device)\n",
    "\n",
    "display_freq = 20\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "    if (episode > 1) and (episode % display_freq == 0):\n",
    "        avg_common_good = recorder.last(display_freq)['common_good'].mean()\n",
    "        avg_reward = replay_mem.last(display_freq)['reward'].mean()\n",
    "        print(f'Start episode {episode}. Avg common good {avg_common_good}. Avg reward {avg_reward}')\n",
    "\n",
    "    state = env.init_episode()\n",
    "\n",
    "    manager.init_episode(episode)\n",
    "\n",
    "    for step in count():\n",
    "        state_ = {k: v.unsqueeze(0).unsqueeze(-1) for k, v in state.items()}\n",
    "        obs = Batch.from_data_list(manager.encode(state_, edge_index=env.edge_index))\n",
    "\n",
    "        # Get q values from controller\n",
    "        q_values = manager.get_q(manager_observations=obs, first=step == 0).squeeze(1)\n",
    "\n",
    "        # Sample a action\n",
    "        selected_action = manager.eps_greedy(q_values=q_values, eps=eps)\n",
    "\n",
    "        state = env.punish(selected_action)\n",
    "        recorder.add(**state, episode_step=step)\n",
    "\n",
    "        # pass actions to environment and advance by one step\n",
    "        state, reward, done = env.step()\n",
    "        replay_mem.add(\n",
    "            action=selected_action, reward=reward, \n",
    "            obs=obs)\n",
    " \n",
    "        if done:\n",
    "            replay_mem.next_episode(episode)\n",
    "            \n",
    "            # allow manager to update itself\n",
    "            sample = replay_mem.sample(**sample_args)\n",
    "            \n",
    "\n",
    "            if sample is not None:\n",
    "                manager.update(**sample)\n",
    "            break\n",
    "    recorder.add(episode_step=step, **state)\n",
    "    recorder.next_episode(episode)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23622cd",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "## Investigate policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2069758d",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ArtificalManager' object has no attribute 'encode_obs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/levinbrinkmann/repros/algorithmic-institutions/notebooks/manager/rl_manager_v2.ipynb Cell 5'\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/levinbrinkmann/repros/algorithmic-institutions/notebooks/manager/rl_manager_v2.ipynb#ch0000006?line=2'>3</a>\u001b[0m data \u001b[39m=\u001b[39m {\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/levinbrinkmann/repros/algorithmic-institutions/notebooks/manager/rl_manager_v2.ipynb#ch0000006?line=3'>4</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mcontributions\u001b[39m\u001b[39m'\u001b[39m: th\u001b[39m.\u001b[39marange(\u001b[39m0\u001b[39m,\u001b[39m21\u001b[39m),\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/levinbrinkmann/repros/algorithmic-institutions/notebooks/manager/rl_manager_v2.ipynb#ch0000006?line=4'>5</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mepisode_step\u001b[39m\u001b[39m'\u001b[39m: th\u001b[39m.\u001b[39mtensor(\u001b[39m0\u001b[39m),\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/levinbrinkmann/repros/algorithmic-institutions/notebooks/manager/rl_manager_v2.ipynb#ch0000006?line=5'>6</a>\u001b[0m }\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/levinbrinkmann/repros/algorithmic-institutions/notebooks/manager/rl_manager_v2.ipynb#ch0000006?line=7'>8</a>\u001b[0m \u001b[39m# obs = manager.\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/levinbrinkmann/repros/algorithmic-institutions/notebooks/manager/rl_manager_v2.ipynb#ch0000006?line=9'>10</a>\u001b[0m data[\u001b[39m'\u001b[39m\u001b[39mpunishments\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m manager\u001b[39m.\u001b[39;49mact(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mdata)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/levinbrinkmann/repros/algorithmic-institutions/notebooks/manager/rl_manager_v2.ipynb#ch0000006?line=11'>12</a>\u001b[0m obs \u001b[39m=\u001b[39m manager\u001b[39m.\u001b[39mencode_obs(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdata)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/levinbrinkmann/repros/algorithmic-institutions/notebooks/manager/rl_manager_v2.ipynb#ch0000006?line=12'>13</a>\u001b[0m q \u001b[39m=\u001b[39m manager\u001b[39m.\u001b[39mget_q(manager_observations\u001b[39m=\u001b[39mobs)\n",
      "File \u001b[0;32m~/repros/algorithmic-institutions/aimanager/manager/manager.py:43\u001b[0m, in \u001b[0;36mArtificalManager.act\u001b[0;34m(self, **state)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mact\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mstate):\n\u001b[0;32m---> 43\u001b[0m     obs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencode_obs(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mstate)\n\u001b[1;32m     44\u001b[0m     q \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_q(manager_observations\u001b[39m=\u001b[39mobs)\n\u001b[1;32m     45\u001b[0m     \u001b[39mreturn\u001b[39;00m q\u001b[39m.\u001b[39margmax(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ArtificalManager' object has no attribute 'encode_obs'"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "data = {\n",
    "    'contributions': th.arange(0,21),\n",
    "    'episode_step': th.tensor(0),\n",
    "}\n",
    "\n",
    "# obs = manager.\n",
    "\n",
    "data['punishments'] = manager.act(**data)\n",
    "\n",
    "obs = manager.encode_obs(**data)\n",
    "q = manager.get_q(manager_observations=obs)\n",
    "sns.heatmap(q)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb25c01d",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "## Investigate trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b138dca1",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/levinbrinkmann/repros/algorithmic-institutions/notebooks/manager/rl_manager_v2.ipynb Cell 8'\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/levinbrinkmann/repros/algorithmic-institutions/notebooks/manager/rl_manager_v2.ipynb#ch0000008?line=11'>12</a>\u001b[0m dfs \u001b[39m=\u001b[39m [df\u001b[39m.\u001b[39mreindex(dfs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mindex) \u001b[39mfor\u001b[39;00m df \u001b[39min\u001b[39;00m dfs]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/levinbrinkmann/repros/algorithmic-institutions/notebooks/manager/rl_manager_v2.ipynb#ch0000008?line=12'>13</a>\u001b[0m rec_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat(dfs, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mreset_index(drop\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/levinbrinkmann/repros/algorithmic-institutions/notebooks/manager/rl_manager_v2.ipynb#ch0000008?line=14'>15</a>\u001b[0m dfs \u001b[39m=\u001b[39m [to_series(k, v)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/levinbrinkmann/repros/algorithmic-institutions/notebooks/manager/rl_manager_v2.ipynb#ch0000008?line=15'>16</a>\u001b[0m     \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m replay_mem\u001b[39m.\u001b[39mmemory\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(v\u001b[39m.\u001b[39mshape) \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m3\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/levinbrinkmann/repros/algorithmic-institutions/notebooks/manager/rl_manager_v2.ipynb#ch0000008?line=16'>17</a>\u001b[0m ]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/levinbrinkmann/repros/algorithmic-institutions/notebooks/manager/rl_manager_v2.ipynb#ch0000008?line=17'>18</a>\u001b[0m dfs\u001b[39m.\u001b[39mappend(to_series(\u001b[39m'\u001b[39m\u001b[39mcurrent_obs\u001b[39m\u001b[39m'\u001b[39m, th\u001b[39m.\u001b[39margmax(replay_mem\u001b[39m.\u001b[39mmemory[\u001b[39m'\u001b[39m\u001b[39mcurrent_obs\u001b[39m\u001b[39m'\u001b[39m], \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/levinbrinkmann/repros/algorithmic-institutions/notebooks/manager/rl_manager_v2.ipynb#ch0000008?line=18'>19</a>\u001b[0m dfs \u001b[39m=\u001b[39m [df\u001b[39m.\u001b[39mreindex(dfs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mindex) \u001b[39mfor\u001b[39;00m df \u001b[39min\u001b[39;00m dfs]\n",
      "\u001b[1;32m/Users/levinbrinkmann/repros/algorithmic-institutions/notebooks/manager/rl_manager_v2.ipynb Cell 8'\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/levinbrinkmann/repros/algorithmic-institutions/notebooks/manager/rl_manager_v2.ipynb#ch0000008?line=11'>12</a>\u001b[0m dfs \u001b[39m=\u001b[39m [df\u001b[39m.\u001b[39mreindex(dfs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mindex) \u001b[39mfor\u001b[39;00m df \u001b[39min\u001b[39;00m dfs]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/levinbrinkmann/repros/algorithmic-institutions/notebooks/manager/rl_manager_v2.ipynb#ch0000008?line=12'>13</a>\u001b[0m rec_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat(dfs, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mreset_index(drop\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/levinbrinkmann/repros/algorithmic-institutions/notebooks/manager/rl_manager_v2.ipynb#ch0000008?line=14'>15</a>\u001b[0m dfs \u001b[39m=\u001b[39m [to_series(k, v)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/levinbrinkmann/repros/algorithmic-institutions/notebooks/manager/rl_manager_v2.ipynb#ch0000008?line=15'>16</a>\u001b[0m     \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m replay_mem\u001b[39m.\u001b[39mmemory\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(v\u001b[39m.\u001b[39;49mshape) \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m3\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/levinbrinkmann/repros/algorithmic-institutions/notebooks/manager/rl_manager_v2.ipynb#ch0000008?line=16'>17</a>\u001b[0m ]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/levinbrinkmann/repros/algorithmic-institutions/notebooks/manager/rl_manager_v2.ipynb#ch0000008?line=17'>18</a>\u001b[0m dfs\u001b[39m.\u001b[39mappend(to_series(\u001b[39m'\u001b[39m\u001b[39mcurrent_obs\u001b[39m\u001b[39m'\u001b[39m, th\u001b[39m.\u001b[39margmax(replay_mem\u001b[39m.\u001b[39mmemory[\u001b[39m'\u001b[39m\u001b[39mcurrent_obs\u001b[39m\u001b[39m'\u001b[39m], \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/levinbrinkmann/repros/algorithmic-institutions/notebooks/manager/rl_manager_v2.ipynb#ch0000008?line=18'>19</a>\u001b[0m dfs \u001b[39m=\u001b[39m [df\u001b[39m.\u001b[39mreindex(dfs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mindex) \u001b[39mfor\u001b[39;00m df \u001b[39min\u001b[39;00m dfs]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "from aimanager.utils.array_to_df import using_multiindex\n",
    "import pandas as pd\n",
    "\n",
    "def to_series(k, v):\n",
    "    dims = ['episode', 'step', 'agent'][:len(v.shape)]\n",
    "    sr = using_multiindex(v, dims).set_index(dims).rename(columns={\"value\": k})\n",
    "    sr.name = k\n",
    "    return sr\n",
    "dfs = [to_series(k, v)\n",
    "    for k, v in recorder.memory.items() if len(v.shape) <= 3\n",
    "]\n",
    "dfs = [df.reindex(dfs[0].index) for df in dfs]\n",
    "rec_df = pd.concat(dfs, axis=1).reset_index(drop=True)\n",
    "\n",
    "dfs = [to_series(k, v)\n",
    "    for k, v in replay_mem.memory.items() if len(v.shape) <= 3\n",
    "]\n",
    "dfs.append(to_series('current_obs', th.argmax(replay_mem.memory['current_obs'], -1)))\n",
    "dfs = [df.reindex(dfs[0].index) for df in dfs]\n",
    "repm_df = pd.concat(dfs, axis=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9befc486",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.countplot(data=rec_df, x='punishments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5185882c",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.countplot(data=rec_df, x='prev_punishments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d97f020",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_count_pt = rec_df.pivot_table(index=['prev_punishments'], columns='prev_contributions', values='contributions', aggfunc=len)\n",
    "df_mean_pt = rec_df.pivot_table(index=['prev_punishments'], columns='prev_contributions', values='contributions', aggfunc=np.mean)\n",
    "\n",
    "\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(14,6))\n",
    "ax1.set_title('count')\n",
    "sns.heatmap(df_count_pt, ax=ax1)\n",
    "ax2.set_title('mean')\n",
    "sns.heatmap(df_mean_pt, ax=ax2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15071b75",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_count_pt = repm_df.pivot_table(index=['current_obs'], columns='actions', values='rewards', aggfunc=len)\n",
    "df_mean_pt = repm_df.pivot_table(index=['current_obs'], columns='actions', values='rewards', aggfunc=np.mean)\n",
    "\n",
    "\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(14,6))\n",
    "ax1.set_title('count')\n",
    "sns.heatmap(df_count_pt, ax=ax1)\n",
    "ax2.set_title('mean')\n",
    "sns.heatmap(df_mean_pt, ax=ax2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a7b691",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "## Simulate managment of artificial humans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65b9c56",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "rec = {\n",
    "    'value': [],\n",
    "    'name': [],\n",
    "    'round_number': [],\n",
    "    'episode': [],\n",
    "    'player_id': [],\n",
    "}\n",
    "\n",
    "for i in range(45):\n",
    "    state = env.init_episode()\n",
    "    done = False\n",
    "    next_done = False\n",
    "    rn = 0\n",
    "    while not next_done:\n",
    "        next_done = done\n",
    "        contributions = state['contributions']\n",
    "        punishments = manager.act(**state)\n",
    "        if not next_done:\n",
    "            env.punish(punishments)\n",
    "            state, reward, done = env.step()\n",
    "\n",
    "        common_good = contributions * 1.6 - punishments\n",
    "\n",
    "        rec['value'].extend(contributions.tolist()+punishments.tolist()+common_good.tolist())\n",
    "        rec['name'].extend(['contributions']*4 + ['punishments']*4 + ['common_good']*4)\n",
    "        rec['episode'].extend([i]*12)\n",
    "        rec['round_number'].extend([rn]*12)\n",
    "        rec['player_id'].extend([0,1,2,3]*3)\n",
    "        rn += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0a625a",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e2cd8f",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "sns.lineplot(data=df, x='round_number', hue='name', y='value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b4b57a",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "folder = '../data/dev/data'\n",
    "df.to_parquet(os.path.join(folder, 'artificial_human_rl_simulation.parquet'))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b1198fd9370ee0cf82025240fa26724f68bfab1e3f74dbb4acdc06e7861d0dbe"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "papermill": {
   "default_parameters": {},
   "environment_variables": {},
   "input_path": "notebooks/manager/rl_manager_v2.ipynb",
   "output_path": "notebooks/manager/rl_manager_v2.ipynb",
   "parameters": {
    "artificial_humans": "../../data/training/ah_11/data/model.pt",
    "artificial_humans_model": "graph",
    "eps": 0.2,
    "gamma": 0.8,
    "model_args": {
     "add_edge_model": true,
     "add_global_model": false,
     "add_rnn": true,
     "hidden_size": 5,
     "u_encoding": [
      {
       "encoding": "numeric",
       "n_levels": 16,
       "name": "round_number"
      },
      {
       "etype": "float",
       "name": "prev_common_good",
       "norm": 128
      }
     ],
     "x_encoding": [
      {
       "encoding": "numeric",
       "n_levels": 21,
       "name": "prev_contributions"
      },
      {
       "encoding": "numeric",
       "n_levels": 31,
       "name": "prev_punishments"
      }
     ]
    },
    "n_episode_steps": 16,
    "n_episodes": 10000,
    "opt_args": {
     "lr": 0.003
    },
    "output_file": "../../data/training/dev/data/manager_run.pt",
    "sample_args": {
     "batch_size": 10,
     "horizon": 1000
    },
    "target_update_freq": 10
   },
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
