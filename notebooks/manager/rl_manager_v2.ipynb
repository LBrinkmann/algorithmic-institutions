{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec2d06f5",
   "metadata": {
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "artificial_humans = \"../../data/training/ah_11/data/model.pt\"\n",
    "artificial_humans_model = \"graph\"\n",
    "output_file = \"../../data/training/dev/data/manager_run.pt\"\n",
    "model_args = {\n",
    "    \"hidden_size\": 5,\n",
    "    \"add_rnn\": True,\n",
    "    \"add_edge_model\": True,\n",
    "    \"add_global_model\": False,\n",
    "    \"x_encoding\": [\n",
    "        {\"name\": \"prev_contributions\", \"n_levels\": 21, \"encoding\": \"numeric\"},\n",
    "        {\"name\": \"prev_punishments\", \"n_levels\": 31, \"encoding\": \"numeric\"},\n",
    "    ],\n",
    "    \"u_encoding\": [\n",
    "        {\"name\": \"round_number\", \"n_levels\": 16, \"encoding\": \"numeric\"},\n",
    "        {\"name\": \"prev_common_good\", \"norm\": 128, \"etype\": \"float\"},\n",
    "    ],\n",
    "}\n",
    "opt_args = {\"lr\": 0.001}\n",
    "gamma = 1.\n",
    "eps = 0.2\n",
    "target_update_freq = 50\n",
    "n_episode_steps = 16\n",
    "n_episodes = 1000\n",
    "memory_size = 100\n",
    "sample_args = {\"batch_size\": 10}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42c92424",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/levinbrinkmann/repros/algorithmic-institutions/.venv/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch as th\n",
    "from itertools import count\n",
    "\n",
    "from aimanager.manager.memory import Memory\n",
    "from aimanager.manager.environment import ArtificialHumanEnv\n",
    "from aimanager.artificial_humans import AH_MODELS\n",
    "from aimanager.manager.manager import ArtificalManager\n",
    "from aimanager.manager.graph_memory import GraphMemory\n",
    "\n",
    "from torch_geometric.data import Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02d293a0",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start episode 20. Avg common good 0.7628130912780762. Avg reward -2.200925350189209\n",
      "Start episode 40. Avg common good 4.132187843322754. Avg reward -1.5573065280914307\n",
      "Start episode 60. Avg common good 10.194843292236328. Avg reward -0.631500244140625\n",
      "Start episode 80. Avg common good 11.142499923706055. Avg reward -0.5464627742767334\n",
      "Start episode 100. Avg common good 11.6665620803833. Avg reward -0.47846275568008423\n",
      "Start episode 120. Avg common good 11.449529647827148. Avg reward -0.650975227355957\n",
      "Start episode 140. Avg common good 11.577186584472656. Avg reward -0.5598815083503723\n",
      "Start episode 160. Avg common good 11.585468292236328. Avg reward -0.5739752054214478\n",
      "Start episode 180. Avg common good 11.162343978881836. Avg reward -0.5798127055168152\n",
      "Start episode 200. Avg common good 11.583593368530273. Avg reward -0.552875280380249\n",
      "Start episode 220. Avg common good 10.410311698913574. Avg reward -0.68670654296875\n",
      "Start episode 240. Avg common good 10.295782089233398. Avg reward -0.7616940140724182\n",
      "Start episode 260. Avg common good 11.172186851501465. Avg reward -0.5915127992630005\n",
      "Start episode 280. Avg common good 11.109999656677246. Avg reward -0.5917189717292786\n",
      "Start episode 300. Avg common good 11.44218635559082. Avg reward -0.5667127370834351\n",
      "Start episode 320. Avg common good 11.436717987060547. Avg reward -0.5884940028190613\n",
      "Start episode 340. Avg common good 11.513750076293945. Avg reward -0.513237714767456\n",
      "Start episode 360. Avg common good 10.351874351501465. Avg reward -0.7433502674102783\n",
      "Start episode 380. Avg common good 8.517343521118164. Avg reward -0.8836377263069153\n",
      "Start episode 400. Avg common good 11.198281288146973. Avg reward -0.6711690425872803\n",
      "Start episode 420. Avg common good 11.218437194824219. Avg reward -0.6044314503669739\n",
      "Start episode 440. Avg common good 11.075624465942383. Avg reward -0.63797527551651\n",
      "Start episode 460. Avg common good 11.200780868530273. Avg reward -0.6483002305030823\n",
      "Start episode 480. Avg common good 11.09999942779541. Avg reward -0.6776439547538757\n",
      "Start episode 500. Avg common good 10.958280563354492. Avg reward -0.6330190300941467\n",
      "Start episode 520. Avg common good 9.18375015258789. Avg reward -0.7813877463340759\n",
      "Start episode 540. Avg common good 9.824843406677246. Avg reward -0.7013376951217651\n",
      "Start episode 560. Avg common good 10.958436965942383. Avg reward -0.6793314814567566\n",
      "Start episode 580. Avg common good 10.936248779296875. Avg reward -0.6889939904212952\n",
      "Start episode 600. Avg common good 10.995156288146973. Avg reward -0.6954127550125122\n",
      "Start episode 620. Avg common good 11.21953010559082. Avg reward -0.7048565149307251\n",
      "Start episode 640. Avg common good 11.31718635559082. Avg reward -0.6239315271377563\n",
      "Start episode 660. Avg common good 11.126718521118164. Avg reward -0.6136127710342407\n",
      "Start episode 680. Avg common good 8.63453197479248. Avg reward -0.8916252255439758\n",
      "Start episode 700. Avg common good 10.005156517028809. Avg reward -0.7125127911567688\n",
      "Start episode 720. Avg common good 10.715624809265137. Avg reward -0.7045565843582153\n",
      "Start episode 740. Avg common good 10.807812690734863. Avg reward -0.6616752743721008\n",
      "Start episode 760. Avg common good 11.196718215942383. Avg reward -0.685869038105011\n",
      "Start episode 780. Avg common good 11.307968139648438. Avg reward -0.6105002164840698\n",
      "Start episode 800. Avg common good 11.110467910766602. Avg reward -0.656169056892395\n",
      "Start episode 820. Avg common good 10.329687118530273. Avg reward -0.7142127752304077\n",
      "Start episode 840. Avg common good 9.714374542236328. Avg reward -0.7506378293037415\n",
      "Start episode 860. Avg common good 10.85531234741211. Avg reward -0.7110689878463745\n",
      "Start episode 880. Avg common good 10.419843673706055. Avg reward -0.7076939940452576\n",
      "Start episode 900. Avg common good 11.066718101501465. Avg reward -0.6522003412246704\n",
      "Start episode 920. Avg common good 11.171562194824219. Avg reward -0.6132815480232239\n",
      "Start episode 940. Avg common good 10.609375. Avg reward -0.6841064691543579\n",
      "Start episode 960. Avg common good 10.839376449584961. Avg reward -0.6772440075874329\n",
      "Start episode 980. Avg common good 9.472187995910645. Avg reward -0.7441627383232117\n"
     ]
    }
   ],
   "source": [
    "device = th.device('cpu')\n",
    "rec_device = th.device('cpu')\n",
    "artifical_humans = AH_MODELS[artificial_humans_model].load(artificial_humans).to(device)\n",
    "\n",
    "env = ArtificialHumanEnv(\n",
    "    artifical_humans=artifical_humans, n_agents=4, n_contributions=21, n_punishments=31, episode_steps=n_episode_steps, device=device)\n",
    "\n",
    "manager = ArtificalManager(\n",
    "    n_contributions=21, n_punishments=31, model_args=model_args, opt_args=opt_args, gamma=gamma, \n",
    "    target_update_freq=target_update_freq, default_values=artifical_humans.default_values, device=device)\n",
    "\n",
    "replay_mem = GraphMemory(n_episodes=memory_size, n_episode_steps=n_episode_steps, n_nodes=4, device=device)\n",
    "recorder = Memory(n_episodes=n_episodes, n_episode_steps=n_episode_steps, output_file=output_file, device=device)\n",
    "\n",
    "display_freq = 20\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "    if (episode > 1) and (episode % display_freq == 0):\n",
    "        avg_common_good = recorder.last(display_freq)['common_good'].mean()\n",
    "        avg_reward = replay_mem.last(display_freq)['reward'].mean()\n",
    "        print(f'Start episode {episode}. Avg common good {avg_common_good}. Avg reward {avg_reward}')\n",
    "\n",
    "    state = env.init_episode()\n",
    "\n",
    "    manager.init_episode(episode)\n",
    "\n",
    "    for step in count():\n",
    "        state_ = {k: v.unsqueeze(0).unsqueeze(-1) for k, v in state.items()}\n",
    "        obs = Batch.from_data_list(manager.encode(state_, edge_index=env.edge_index))\n",
    "\n",
    "        # Get q values from controller\n",
    "        q_values = manager.get_q(manager_observations=obs, first=step == 0).squeeze(1)\n",
    "\n",
    "        # Sample a action\n",
    "        selected_action = manager.eps_greedy(q_values=q_values, eps=eps)\n",
    "\n",
    "        state = env.punish(selected_action)\n",
    "        recorder.add(**state, episode_step=step)\n",
    "\n",
    "        # pass actions to environment and advance by one step\n",
    "        state, reward, done = env.step()\n",
    "        replay_mem.add(\n",
    "            action=selected_action, reward=reward, \n",
    "            obs=obs)\n",
    " \n",
    "        if done:\n",
    "            replay_mem.next_episode(episode)\n",
    "            \n",
    "            # allow manager to update itself\n",
    "            sample = replay_mem.sample(**sample_args)\n",
    "            \n",
    "\n",
    "            if sample is not None:\n",
    "                manager.update(**sample)\n",
    "            break\n",
    "    recorder.add(episode_step=step, **state)\n",
    "    recorder.next_episode(episode)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23622cd",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "## Investigate policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2069758d",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ArtificalManager' object has no attribute 'encode_obs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/levinbrinkmann/repros/algorithmic-institutions/notebooks/manager/rl_manager_v2.ipynb Cell 6'\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/levinbrinkmann/repros/algorithmic-institutions/notebooks/manager/rl_manager_v2.ipynb#ch0000006?line=2'>3</a>\u001b[0m data \u001b[39m=\u001b[39m {\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/levinbrinkmann/repros/algorithmic-institutions/notebooks/manager/rl_manager_v2.ipynb#ch0000006?line=3'>4</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mcontributions\u001b[39m\u001b[39m'\u001b[39m: th\u001b[39m.\u001b[39marange(\u001b[39m0\u001b[39m,\u001b[39m21\u001b[39m),\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/levinbrinkmann/repros/algorithmic-institutions/notebooks/manager/rl_manager_v2.ipynb#ch0000006?line=4'>5</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mepisode_step\u001b[39m\u001b[39m'\u001b[39m: th\u001b[39m.\u001b[39mtensor(\u001b[39m0\u001b[39m),\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/levinbrinkmann/repros/algorithmic-institutions/notebooks/manager/rl_manager_v2.ipynb#ch0000006?line=5'>6</a>\u001b[0m }\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/levinbrinkmann/repros/algorithmic-institutions/notebooks/manager/rl_manager_v2.ipynb#ch0000006?line=7'>8</a>\u001b[0m \u001b[39m# obs = manager.\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/levinbrinkmann/repros/algorithmic-institutions/notebooks/manager/rl_manager_v2.ipynb#ch0000006?line=9'>10</a>\u001b[0m data[\u001b[39m'\u001b[39m\u001b[39mpunishments\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m manager\u001b[39m.\u001b[39;49mact(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mdata)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/levinbrinkmann/repros/algorithmic-institutions/notebooks/manager/rl_manager_v2.ipynb#ch0000006?line=11'>12</a>\u001b[0m obs \u001b[39m=\u001b[39m manager\u001b[39m.\u001b[39mencode_obs(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mdata)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/levinbrinkmann/repros/algorithmic-institutions/notebooks/manager/rl_manager_v2.ipynb#ch0000006?line=12'>13</a>\u001b[0m q \u001b[39m=\u001b[39m manager\u001b[39m.\u001b[39mget_q(manager_observations\u001b[39m=\u001b[39mobs)\n",
      "File \u001b[0;32m~/repros/algorithmic-institutions/aimanager/manager/manager.py:43\u001b[0m, in \u001b[0;36mArtificalManager.act\u001b[0;34m(self, **state)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mact\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mstate):\n\u001b[0;32m---> 43\u001b[0m     obs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencode_obs(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mstate)\n\u001b[1;32m     44\u001b[0m     q \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_q(manager_observations\u001b[39m=\u001b[39mobs)\n\u001b[1;32m     45\u001b[0m     \u001b[39mreturn\u001b[39;00m q\u001b[39m.\u001b[39margmax(dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ArtificalManager' object has no attribute 'encode_obs'"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "data = {\n",
    "    'contributions': th.arange(0,21),\n",
    "    'episode_step': th.tensor(0),\n",
    "}\n",
    "\n",
    "# obs = manager.\n",
    "\n",
    "data['punishments'] = manager.act(**data)\n",
    "\n",
    "obs = manager.encode_obs(**data)\n",
    "q = manager.get_q(manager_observations=obs)\n",
    "sns.heatmap(q)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb25c01d",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "## Investigate trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b138dca1",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/levinbrinkmann/repros/algorithmic-institutions/notebooks/manager/rl_manager_v2.ipynb Cell 8'\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/levinbrinkmann/repros/algorithmic-institutions/notebooks/manager/rl_manager_v2.ipynb#ch0000008?line=11'>12</a>\u001b[0m dfs \u001b[39m=\u001b[39m [df\u001b[39m.\u001b[39mreindex(dfs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mindex) \u001b[39mfor\u001b[39;00m df \u001b[39min\u001b[39;00m dfs]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/levinbrinkmann/repros/algorithmic-institutions/notebooks/manager/rl_manager_v2.ipynb#ch0000008?line=12'>13</a>\u001b[0m rec_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat(dfs, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mreset_index(drop\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/levinbrinkmann/repros/algorithmic-institutions/notebooks/manager/rl_manager_v2.ipynb#ch0000008?line=14'>15</a>\u001b[0m dfs \u001b[39m=\u001b[39m [to_series(k, v)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/levinbrinkmann/repros/algorithmic-institutions/notebooks/manager/rl_manager_v2.ipynb#ch0000008?line=15'>16</a>\u001b[0m     \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m replay_mem\u001b[39m.\u001b[39mmemory\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(v\u001b[39m.\u001b[39mshape) \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m3\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/levinbrinkmann/repros/algorithmic-institutions/notebooks/manager/rl_manager_v2.ipynb#ch0000008?line=16'>17</a>\u001b[0m ]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/levinbrinkmann/repros/algorithmic-institutions/notebooks/manager/rl_manager_v2.ipynb#ch0000008?line=17'>18</a>\u001b[0m dfs\u001b[39m.\u001b[39mappend(to_series(\u001b[39m'\u001b[39m\u001b[39mcurrent_obs\u001b[39m\u001b[39m'\u001b[39m, th\u001b[39m.\u001b[39margmax(replay_mem\u001b[39m.\u001b[39mmemory[\u001b[39m'\u001b[39m\u001b[39mcurrent_obs\u001b[39m\u001b[39m'\u001b[39m], \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/levinbrinkmann/repros/algorithmic-institutions/notebooks/manager/rl_manager_v2.ipynb#ch0000008?line=18'>19</a>\u001b[0m dfs \u001b[39m=\u001b[39m [df\u001b[39m.\u001b[39mreindex(dfs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mindex) \u001b[39mfor\u001b[39;00m df \u001b[39min\u001b[39;00m dfs]\n",
      "\u001b[1;32m/Users/levinbrinkmann/repros/algorithmic-institutions/notebooks/manager/rl_manager_v2.ipynb Cell 8'\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/levinbrinkmann/repros/algorithmic-institutions/notebooks/manager/rl_manager_v2.ipynb#ch0000008?line=11'>12</a>\u001b[0m dfs \u001b[39m=\u001b[39m [df\u001b[39m.\u001b[39mreindex(dfs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mindex) \u001b[39mfor\u001b[39;00m df \u001b[39min\u001b[39;00m dfs]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/levinbrinkmann/repros/algorithmic-institutions/notebooks/manager/rl_manager_v2.ipynb#ch0000008?line=12'>13</a>\u001b[0m rec_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat(dfs, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mreset_index(drop\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/levinbrinkmann/repros/algorithmic-institutions/notebooks/manager/rl_manager_v2.ipynb#ch0000008?line=14'>15</a>\u001b[0m dfs \u001b[39m=\u001b[39m [to_series(k, v)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/levinbrinkmann/repros/algorithmic-institutions/notebooks/manager/rl_manager_v2.ipynb#ch0000008?line=15'>16</a>\u001b[0m     \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m replay_mem\u001b[39m.\u001b[39mmemory\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(v\u001b[39m.\u001b[39;49mshape) \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m3\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/levinbrinkmann/repros/algorithmic-institutions/notebooks/manager/rl_manager_v2.ipynb#ch0000008?line=16'>17</a>\u001b[0m ]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/levinbrinkmann/repros/algorithmic-institutions/notebooks/manager/rl_manager_v2.ipynb#ch0000008?line=17'>18</a>\u001b[0m dfs\u001b[39m.\u001b[39mappend(to_series(\u001b[39m'\u001b[39m\u001b[39mcurrent_obs\u001b[39m\u001b[39m'\u001b[39m, th\u001b[39m.\u001b[39margmax(replay_mem\u001b[39m.\u001b[39mmemory[\u001b[39m'\u001b[39m\u001b[39mcurrent_obs\u001b[39m\u001b[39m'\u001b[39m], \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/levinbrinkmann/repros/algorithmic-institutions/notebooks/manager/rl_manager_v2.ipynb#ch0000008?line=18'>19</a>\u001b[0m dfs \u001b[39m=\u001b[39m [df\u001b[39m.\u001b[39mreindex(dfs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mindex) \u001b[39mfor\u001b[39;00m df \u001b[39min\u001b[39;00m dfs]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "from aimanager.utils.array_to_df import using_multiindex\n",
    "import pandas as pd\n",
    "\n",
    "def to_series(k, v):\n",
    "    dims = ['episode', 'step', 'agent'][:len(v.shape)]\n",
    "    sr = using_multiindex(v, dims).set_index(dims).rename(columns={\"value\": k})\n",
    "    sr.name = k\n",
    "    return sr\n",
    "dfs = [to_series(k, v)\n",
    "    for k, v in recorder.memory.items() if len(v.shape) <= 3\n",
    "]\n",
    "dfs = [df.reindex(dfs[0].index) for df in dfs]\n",
    "rec_df = pd.concat(dfs, axis=1).reset_index(drop=True)\n",
    "\n",
    "dfs = [to_series(k, v)\n",
    "    for k, v in replay_mem.memory.items() if len(v.shape) <= 3\n",
    "]\n",
    "dfs.append(to_series('current_obs', th.argmax(replay_mem.memory['current_obs'], -1)))\n",
    "dfs = [df.reindex(dfs[0].index) for df in dfs]\n",
    "repm_df = pd.concat(dfs, axis=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9befc486",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.countplot(data=rec_df, x='punishments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5185882c",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.countplot(data=rec_df, x='prev_punishments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d97f020",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_count_pt = rec_df.pivot_table(index=['prev_punishments'], columns='prev_contributions', values='contributions', aggfunc=len)\n",
    "df_mean_pt = rec_df.pivot_table(index=['prev_punishments'], columns='prev_contributions', values='contributions', aggfunc=np.mean)\n",
    "\n",
    "\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(14,6))\n",
    "ax1.set_title('count')\n",
    "sns.heatmap(df_count_pt, ax=ax1)\n",
    "ax2.set_title('mean')\n",
    "sns.heatmap(df_mean_pt, ax=ax2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15071b75",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_count_pt = repm_df.pivot_table(index=['current_obs'], columns='actions', values='rewards', aggfunc=len)\n",
    "df_mean_pt = repm_df.pivot_table(index=['current_obs'], columns='actions', values='rewards', aggfunc=np.mean)\n",
    "\n",
    "\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, figsize=(14,6))\n",
    "ax1.set_title('count')\n",
    "sns.heatmap(df_count_pt, ax=ax1)\n",
    "ax2.set_title('mean')\n",
    "sns.heatmap(df_mean_pt, ax=ax2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a7b691",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "source": [
    "## Simulate managment of artificial humans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65b9c56",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "rec = {\n",
    "    'value': [],\n",
    "    'name': [],\n",
    "    'round_number': [],\n",
    "    'episode': [],\n",
    "    'player_id': [],\n",
    "}\n",
    "\n",
    "for i in range(45):\n",
    "    state = env.init_episode()\n",
    "    done = False\n",
    "    next_done = False\n",
    "    rn = 0\n",
    "    while not next_done:\n",
    "        next_done = done\n",
    "        contributions = state['contributions']\n",
    "        punishments = manager.act(**state)\n",
    "        if not next_done:\n",
    "            env.punish(punishments)\n",
    "            state, reward, done = env.step()\n",
    "\n",
    "        common_good = contributions * 1.6 - punishments\n",
    "\n",
    "        rec['value'].extend(contributions.tolist()+punishments.tolist()+common_good.tolist())\n",
    "        rec['name'].extend(['contributions']*4 + ['punishments']*4 + ['common_good']*4)\n",
    "        rec['episode'].extend([i]*12)\n",
    "        rec['round_number'].extend([rn]*12)\n",
    "        rec['player_id'].extend([0,1,2,3]*3)\n",
    "        rn += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0a625a",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e2cd8f",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "sns.lineplot(data=df, x='round_number', hue='name', y='value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b4b57a",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "folder = '../data/dev/data'\n",
    "df.to_parquet(os.path.join(folder, 'artificial_human_rl_simulation.parquet'))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b1198fd9370ee0cf82025240fa26724f68bfab1e3f74dbb4acdc06e7861d0dbe"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "papermill": {
   "default_parameters": {},
   "environment_variables": {},
   "input_path": "notebooks/manager/rl_manager_v2.ipynb",
   "output_path": "notebooks/manager/rl_manager_v2.ipynb",
   "parameters": {
    "artificial_humans": "../../data/training/ah_11/data/model.pt",
    "artificial_humans_model": "graph",
    "eps": 0.2,
    "gamma": 0.8,
    "model_args": {
     "add_edge_model": true,
     "add_global_model": false,
     "add_rnn": true,
     "hidden_size": 5,
     "u_encoding": [
      {
       "encoding": "numeric",
       "n_levels": 16,
       "name": "round_number"
      },
      {
       "etype": "float",
       "name": "prev_common_good",
       "norm": 128
      }
     ],
     "x_encoding": [
      {
       "encoding": "numeric",
       "n_levels": 21,
       "name": "prev_contributions"
      },
      {
       "encoding": "numeric",
       "n_levels": 31,
       "name": "prev_punishments"
      }
     ]
    },
    "n_episode_steps": 16,
    "n_episodes": 10000,
    "opt_args": {
     "lr": 0.003
    },
    "output_file": "../../data/training/dev/data/manager_run.pt",
    "sample_args": {
     "batch_size": 10,
     "horizon": 1000
    },
    "target_update_freq": 10
   },
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
