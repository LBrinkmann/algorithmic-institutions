{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7f5068fc",
   "metadata": {
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "artificial_humans = \"../../data/training/ah_10/data/model.pt\"\n",
    "artificial_humans_model = \"graph\"\n",
    "output_path = \"../../data/manager/v2/dev/\"\n",
    "model_args = {\n",
    "    \"hidden_size\": 5,\n",
    "    \"add_rnn\": True,\n",
    "    \"add_edge_model\": True,\n",
    "    \"add_global_model\": False,\n",
    "    \"x_encoding\": [\n",
    "        {\"name\": \"prev_contributions\", \"n_levels\": 21, \"encoding\": \"numeric\"},\n",
    "        {\"name\": \"prev_punishments\", \"n_levels\": 31, \"encoding\": \"numeric\"},\n",
    "    ],\n",
    "    \"u_encoding\": [\n",
    "        {\"name\": \"round_number\", \"n_levels\": 16, \"encoding\": \"numeric\"},\n",
    "        {\"name\": \"prev_common_good\", \"norm\": 128, \"etype\": \"float\"},\n",
    "    ],\n",
    "}\n",
    "opt_args = {\"lr\": 0.003}\n",
    "gamma = 1.0\n",
    "eps = 0.2\n",
    "target_update_freq = 20\n",
    "n_episode_steps = 16\n",
    "n_episodes = 100\n",
    "memory_size = 50\n",
    "sample_args = {\"batch_size\": 10}\n",
    "n_test_episodes = 10\n",
    "eval_freq = 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "42c92424",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch as th\n",
    "import os\n",
    "from itertools import count\n",
    "\n",
    "from aimanager.manager.memory import Memory\n",
    "from aimanager.manager.environment import ArtificialHumanEnv\n",
    "from aimanager.artificial_humans import AH_MODELS\n",
    "from aimanager.manager.manager import ArtificalManager\n",
    "from aimanager.manager.graph_memory import GraphMemory\n",
    "\n",
    "from torch_geometric.data import Batch\n",
    "\n",
    "\n",
    "output_file =  os.path.join(output_path, \"eval\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3e03a4b1",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ManagerEvaluator():\n",
    "    def __init__(self, n_episodes, n_test_episodes, eval_freq, n_episode_steps, output_file):\n",
    "        self.device = th.device('cpu')\n",
    "        self.n_test_episodes = n_test_episodes\n",
    "        self.eval_freq = eval_freq\n",
    "        self.t_episode = 0\n",
    "        self.recorder = Memory(n_episodes= n_episodes // eval_freq * n_test_episodes, n_episode_steps=n_episode_steps, output_file=output_file, device=device)\n",
    "\n",
    "    def eval(self, manager, env, episode):\n",
    "        if episode % self.eval_freq == 0:\n",
    "            episode_ = th.tensor(episode, device=self.device)\n",
    "\n",
    "            for i in range(self.n_test_episodes):\n",
    "                state = env.init_episode()\n",
    "                for step in count():\n",
    "                    state_ = {k: v.unsqueeze(0).unsqueeze(-1) for k, v in state.items()}\n",
    "                    obs = Batch.from_data_list(manager.encode(state_, edge_index=env.edge_index))\n",
    "\n",
    "                    # Get q values from controller\n",
    "                    q_values = manager.get_q(manager_observations=obs, first=step == 0).squeeze(1)\n",
    "\n",
    "                    # Sample a action\n",
    "                    selected_action = manager.eps_greedy(q_values=q_values, eps=0)\n",
    "\n",
    "                    state = env.punish(selected_action)\n",
    "                    self.recorder.add(**state, episode_step=step, episode=episode_)\n",
    "\n",
    "                    # pass actions to environment and advance by one step\n",
    "                    state, reward, done = env.step()\n",
    "                    if done:\n",
    "                        break\n",
    "\n",
    "                self.recorder.next_episode(self.t_episode)\n",
    "                self.t_episode += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "02d293a0",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start episode 5. Avg common good 9.103124618530273. Avg reward -1.1431190967559814\n",
      "Start episode 10. Avg common good 10.698750495910645. Avg reward -0.8390940427780151\n",
      "Start episode 15. Avg common good 7.6050004959106445. Avg reward -0.9586941003799438\n",
      "Start episode 20. Avg common good 10.288125991821289. Avg reward -0.8202440142631531\n",
      "Start episode 25. Avg common good 10.150625228881836. Avg reward -0.7964940071105957\n",
      "Start episode 30. Avg common good 9.775625228881836. Avg reward -0.630469024181366\n",
      "Start episode 35. Avg common good 8.559374809265137. Avg reward -0.8459939956665039\n",
      "Start episode 40. Avg common good 8.257499694824219. Avg reward -0.8263691067695618\n",
      "Start episode 45. Avg common good 9.595624923706055. Avg reward -0.7702189683914185\n",
      "Start episode 50. Avg common good 9.863750457763672. Avg reward -0.6136690378189087\n",
      "Start episode 55. Avg common good 10.235624313354492. Avg reward -0.5958940386772156\n",
      "Start episode 60. Avg common good 9.095000267028809. Avg reward -0.6987689733505249\n",
      "Start episode 65. Avg common good 9.643125534057617. Avg reward -0.6147440671920776\n",
      "Start episode 70. Avg common good 8.666250228881836. Avg reward -0.7619439363479614\n",
      "Start episode 75. Avg common good 9.608125686645508. Avg reward -0.6498690247535706\n",
      "Start episode 80. Avg common good 11.535624504089355. Avg reward -0.6469440460205078\n",
      "Start episode 85. Avg common good 9.181875228881836. Avg reward -0.6983940005302429\n",
      "Start episode 90. Avg common good 5.101874828338623. Avg reward -1.378419041633606\n",
      "Start episode 95. Avg common good 7.546250343322754. Avg reward -0.8921440243721008\n"
     ]
    }
   ],
   "source": [
    "device = th.device('cpu')\n",
    "rec_device = th.device('cpu')\n",
    "artifical_humans = AH_MODELS[artificial_humans_model].load(artificial_humans).to(device)\n",
    "\n",
    "env = ArtificialHumanEnv(\n",
    "    artifical_humans=artifical_humans, n_agents=4, n_contributions=21, n_punishments=31, episode_steps=n_episode_steps, device=device)\n",
    "\n",
    "manager = ArtificalManager(\n",
    "    n_contributions=21, n_punishments=31, model_args=model_args, opt_args=opt_args, gamma=gamma, \n",
    "    target_update_freq=target_update_freq, default_values=artifical_humans.default_values, device=device)\n",
    "\n",
    "replay_mem = GraphMemory(n_episodes=memory_size, n_episode_steps=n_episode_steps, n_nodes=4, device=device)\n",
    "recorder = Memory(n_episodes=n_episodes, n_episode_steps=n_episode_steps, device=device)\n",
    "evaluator = ManagerEvaluator(n_episodes, n_test_episodes, eval_freq, n_episode_steps, output_file)\n",
    "\n",
    "display_freq = n_episodes // 20\n",
    "\n",
    "for episode in range(n_episodes):\n",
    "    if (episode > 1) and (episode % display_freq == 0):\n",
    "        avg_common_good = recorder.last(display_freq)['common_good'].mean()\n",
    "        avg_reward = replay_mem.last(display_freq)['reward'].mean()\n",
    "        print(f'Start episode {episode}. Avg common good {avg_common_good}. Avg reward {avg_reward}')\n",
    "\n",
    "    state = env.init_episode()\n",
    "\n",
    "    manager.init_episode(episode)\n",
    "\n",
    "    for step in count():\n",
    "        state_ = {k: v.unsqueeze(0).unsqueeze(-1) for k, v in state.items()}\n",
    "        obs = Batch.from_data_list(manager.encode(state_, edge_index=env.edge_index))\n",
    "\n",
    "        # Get q values from controller\n",
    "        q_values = manager.get_q(manager_observations=obs, first=step == 0).squeeze(1)\n",
    "\n",
    "        # Sample a action\n",
    "        selected_action = manager.eps_greedy(q_values=q_values, eps=eps)\n",
    "\n",
    "        state = env.punish(selected_action)\n",
    "        recorder.add(**state, episode_step=step)\n",
    "\n",
    "        # pass actions to environment and advance by one step\n",
    "        state, reward, done = env.step()\n",
    "        replay_mem.add(\n",
    "            action=selected_action, reward=reward, \n",
    "            obs=obs)\n",
    " \n",
    "        if done:\n",
    "            replay_mem.next_episode(episode)\n",
    "            \n",
    "            # allow manager to update itself\n",
    "            sample = replay_mem.sample(**sample_args)\n",
    "            \n",
    "\n",
    "            if sample is not None:\n",
    "                manager.update(**sample)\n",
    "            break\n",
    "    recorder.add(episode_step=step, **state)\n",
    "    recorder.next_episode(episode)\n",
    "    evaluator.eval(manager, env, episode)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b1198fd9370ee0cf82025240fa26724f68bfab1e3f74dbb4acdc06e7861d0dbe"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "papermill": {
   "default_parameters": {},
   "environment_variables": {},
   "input_path": "notebooks/manager/rl_manager_v2.ipynb",
   "output_path": "notebooks/manager/rl_manager_v2.ipynb",
   "parameters": {
    "artificial_humans": "../../data/training/ah_10/data/model.pt",
    "artificial_humans_model": "graph",
    "eps": 0.2,
    "eval_freq": 10,
    "gamma": 1,
    "memory_size": 50,
    "model_args": {
     "add_edge_model": true,
     "add_global_model": false,
     "add_rnn": true,
     "hidden_size": 5,
     "u_encoding": [
      {
       "encoding": "numeric",
       "n_levels": 16,
       "name": "round_number"
      },
      {
       "etype": "float",
       "name": "prev_common_good",
       "norm": 128
      }
     ],
     "x_encoding": [
      {
       "encoding": "numeric",
       "n_levels": 21,
       "name": "prev_contributions"
      },
      {
       "encoding": "numeric",
       "n_levels": 31,
       "name": "prev_punishments"
      }
     ]
    },
    "n_episode_steps": 16,
    "n_episodes": 100,
    "n_test_episodes": 10,
    "opt_args": {
     "lr": 0.003
    },
    "output_path": "../../data/manager/v2/dev/",
    "sample_args": {
     "batch_size": 10
    },
    "target_update_freq": 20
   },
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
