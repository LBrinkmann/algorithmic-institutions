{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "85c77e56",
   "metadata": {
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "x_encoding = [\n",
    "    {\"encoding\": \"ordinal\", \"column\": \"prev_contribution\"},\n",
    "    {\"encoding\": \"ordinal\", \"column\": \"prev_punishment\"},\n",
    "]\n",
    "y_encoding = \"numeric\"\n",
    "n_contributions = 21\n",
    "n_punishments = 31\n",
    "n_cross_val = 10\n",
    "fraction_training = 1.0\n",
    "data = \"../data/pilot1_player_round_slim.csv\"\n",
    "output_path = \"../data/dev\"\n",
    "labels = {}\n",
    "model_args = {\"n_layers\": 1, \"hidden_size\": None}\n",
    "optimizer_args = {\"lr\": 0.0001, \"weight_decay\": 1e-05}\n",
    "train_args = {\"epochs\": 100, \"batch_size\": 40, \"clamp_grad\": None, \"eval_period\": 10}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "44582683",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-05T12:51:49.618792Z",
     "iopub.status.busy": "2022-02-05T12:51:49.617907Z",
     "iopub.status.idle": "2022-02-05T12:51:50.435900Z",
     "shell.execute_reply": "2022-02-05T12:51:50.436267Z"
    },
    "papermill": {
     "duration": 0.836397,
     "end_time": "2022-02-05T12:51:50.436437",
     "exception": false,
     "start_time": "2022-02-05T12:51:49.600040",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from aimanager.model.cross_validation import split_xy, get_cross_validations, get_fraction_of_groups\n",
    "from aimanager.model.encoder import int_to_ordinal, ordinal_to_int, onehot_to_int, int_to_onehot, joined_encoder, int_encode\n",
    "from aimanager.model.metrics import create_metrics, create_confusion_matrix\n",
    "from aimanager.model.synthesize_data import syn_con_pun\n",
    "from aimanager.utils.array_to_df import add_labels, using_multiindex\n",
    "from aimanager.utils.utils import make_dir\n",
    "\n",
    "output_path = os.path.join(output_path, 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d3669dc3",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch as th\n",
    "from typing import Literal, Optional\n",
    "\n",
    "\n",
    "class FeedForwardLayer(th.nn.Module):\n",
    "    def __init__(\n",
    "            self, *,\n",
    "            input_size: int, \n",
    "            hidden_size: int, \n",
    "            dropout: Optional[float], \n",
    "            activation: Optional[Literal['relu', 'logit', 'softmax']]):\n",
    "        super(FeedForwardLayer, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size  = hidden_size\n",
    "        self.lin = th.nn.Linear(self.input_size, self.hidden_size)\n",
    "\n",
    "        if activation == 'relu':\n",
    "            self.activation = th.nn.ReLU()\n",
    "        elif activation == 'logit':\n",
    "            self.activation = th.nn.Logit()\n",
    "        elif activation == 'softmax':\n",
    "            self.activation = th.nn.Softmax()\n",
    "        else:\n",
    "            self.activation = None\n",
    "\n",
    "        if dropout:\n",
    "            self.dropout = th.nn.Dropout(dropout)\n",
    "        else:\n",
    "            self.dropout = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.lin(x)\n",
    "        if self.dropout:\n",
    "            x = self.dropout(x)\n",
    "        if self.activation:\n",
    "            x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class MultiLayer(th.nn.Module):\n",
    "    def __init__(self, *, \n",
    "            n_layers: int, \n",
    "            hidden_size: Optional[int]=None, \n",
    "            input_size: int, \n",
    "            output_size: int, \n",
    "            dropout: Optional[float]=None):\n",
    "        super(MultiLayer, self).__init__()\n",
    "        \n",
    "        assert not ((hidden_size == None) and (n_layers > 1))\n",
    "\n",
    "        self.layers = th.nn.Sequential(\n",
    "            *(FeedForwardLayer(\n",
    "                input_size=hidden_size if i > 0 else input_size,\n",
    "                hidden_size=output_size if i == (n_layers - 1) else hidden_size,\n",
    "                dropout=dropout,\n",
    "                activation=None if i == (n_layers - 1) else 'relu'\n",
    "            )\n",
    "            for i in range(n_layers))\n",
    "        )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# weight_decay == L2 regularisation\n",
    "# https://stackoverflow.com/questions/42704283/adding-l1-l2-regularization-in-pytorch\n",
    "\n",
    "# def train(\n",
    "#         x, y, *, model_args, optimizer_args, y_encoding, \n",
    "#         epochs, batch_size, clamp_grad=None, eval_period=0, eval_func=None):\n",
    "\n",
    "#     return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e6659c46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-05T12:51:50.450815Z",
     "iopub.status.busy": "2022-02-05T12:51:50.450261Z",
     "iopub.status.idle": "2022-02-05T12:51:50.478946Z",
     "shell.execute_reply": "2022-02-05T12:51:50.478495Z"
    },
    "papermill": {
     "duration": 0.038317,
     "end_time": "2022-02-05T12:51:50.479107",
     "exception": false,
     "start_time": "2022-02-05T12:51:50.440790",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "def predict(model, x_enc, y_encoding: Literal[\"ordinal\", \"onehot\", \"numeric\"]):\n",
    "    y_pred_logit = model(x_enc)\n",
    "    if y_encoding == 'ordinal':\n",
    "        y_pred_proba = th.sigmoid(y_pred_logit).detach().cpu().numpy()\n",
    "        y_pred = ordinal_to_int(y_pred_proba)\n",
    "    elif y_encoding in ['onehot', 'numeric']: \n",
    "        y_pred_proba = th.nn.functional.softmax(y_pred_logit, dim=-1).detach().cpu().numpy()\n",
    "        y_pred = onehot_to_int(y_pred_proba)\n",
    "    return y_pred, y_pred_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "1f144b88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-05T12:51:50.496909Z",
     "iopub.status.busy": "2022-02-05T12:51:50.496286Z",
     "iopub.status.idle": "2022-02-05T12:51:59.402627Z",
     "shell.execute_reply": "2022-02-05T12:51:59.403024Z"
    },
    "papermill": {
     "duration": 8.919725,
     "end_time": "2022-02-05T12:51:59.403222",
     "exception": false,
     "start_time": "2022-02-05T12:51:50.483497",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(data)\n",
    "\n",
    "df['contribution'] = pd.Categorical(\n",
    "    df['contribution'], categories=np.arange(n_contributions), ordered=True\n",
    ")\n",
    "df['punishment'] = pd.Categorical(\n",
    "    df['punishment'], categories=np.arange(n_punishments), ordered=True\n",
    ")\n",
    "\n",
    "metrics = []\n",
    "confusion_matrix = []\n",
    "syn_pred = []\n",
    "\n",
    "x_df, y_sr = split_xy(df)\n",
    "for i, split in enumerate(get_cross_validations(x_df, y_sr, n_cross_val)):\n",
    "    x_train_df, y_train_sr, x_test_df, y_test_sr = split\n",
    "    x_train_df, y_train_sr = get_fraction_of_groups(x_train_df, y_train_sr, fraction_training)\n",
    "\n",
    "    y_test = int_encode(y_test_sr, encoding='numeric')[:,0]\n",
    "    y_train = int_encode(y_train_sr, encoding='numeric')[:,0]\n",
    "\n",
    "    x_train_enc = joined_encoder(x_train_df, x_encoding)\n",
    "    x_test_enc = joined_encoder(x_test_df, x_encoding)\n",
    "    y_test_enc = int_encode(y_test_sr, encoding=y_encoding)\n",
    "    y_train_enc = int_encode(y_train_sr, encoding=y_encoding)\n",
    "\n",
    "    x_train_enc = th.tensor(x_train_enc, dtype=th.float)\n",
    "    x_test_enc = th.tensor(x_test_enc, dtype=th.float)\n",
    "\n",
    "    if y_encoding == 'numeric':\n",
    "        y_test_enc = th.tensor(y_test_enc[:,0], dtype=th.long)\n",
    "        y_train_enc = th.tensor(y_train_enc[:,0], dtype=th.long)\n",
    "    else:\n",
    "        y_test_enc = th.tensor(y_test_enc, dtype=th.float)\n",
    "        y_train_enc = th.tensor(y_train_enc, dtype=th.float)\n",
    "\n",
    "    # train func\n",
    "\n",
    "    epochs = train_args['epochs']\n",
    "    batch_size = train_args['batch_size']\n",
    "    clamp_grad = train_args['clamp_grad']\n",
    "    eval_period = train_args['eval_period']\n",
    "    x = x_train_enc\n",
    "    y = y_train_enc\n",
    "#         epochs, batch_size, clamp_grad=None, eval_period=0\n",
    "\n",
    "    output_size = n_contributions if not y_encoding == 'ordinal' else n_contributions - 1\n",
    "\n",
    "    model = MultiLayer(input_size=x.shape[1], output_size=output_size, **model_args)\n",
    "    optimizer = th.optim.Adam(model.parameters(), **optimizer_args)\n",
    "\n",
    "    if y_encoding == 'ordinal':\n",
    "        loss_fn = th.nn.BCEWithLogitsLoss()\n",
    "    elif y_encoding in ['onehot', 'numeric']:\n",
    "        loss_fn = th.nn.CrossEntropyLoss()\n",
    "    \n",
    "    sum_loss = 0\n",
    "    n_steps = 0\n",
    "\n",
    "    for e in range(epochs):\n",
    "        model.eval()\n",
    "        for start_idx in range(0, len(x), batch_size):\n",
    "            tx = x[start_idx:start_idx+batch_size]\n",
    "            ty = y[start_idx:start_idx+batch_size]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            py = model(tx)\n",
    "            loss = loss_fn(py, ty)\n",
    "            loss.backward()\n",
    "\n",
    "            if clamp_grad:\n",
    "                for param in model.parameters():\n",
    "                    param.grad.data.clamp_(-clamp_grad, clamp_grad)\n",
    "            optimizer.step()\n",
    "            sum_loss += loss.item()\n",
    "            n_steps +=1\n",
    "        \n",
    "        if e % eval_period == 0:\n",
    "            metrics.append({\n",
    "                'name': 'loss',\n",
    "                'value': sum_loss / n_steps,\n",
    "                'cv_split': i,\n",
    "                'epoch': e\n",
    "            })\n",
    "\n",
    "            # training set performance\n",
    "            y_pred, y_pred_proba = predict(model, x_test_enc, y_encoding=y_encoding)\n",
    "            metrics += create_metrics(y_test, y_pred, set='test', cv_split=i)\n",
    "            confusion_matrix += create_confusion_matrix(y_test, y_pred, set='test', cv_split=i, epoch=e)\n",
    "\n",
    "            # test set performance\n",
    "            y_pred, y_pred_proba = predict(model, x_train_enc, y_encoding=y_encoding)\n",
    "            metrics += create_metrics(y_train, y_pred, set='train', cv_split=i)\n",
    "            confusion_matrix += create_confusion_matrix(y_train, y_pred, set='train', cv_split=i, epoch=e)\n",
    "            sum_loss = 0\n",
    "            n_steps = 0   \n",
    "    \n",
    "    # break\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # eval synthesized data\n",
    "    x_syn_df = syn_con_pun(n_contributions, n_punishments)\n",
    "    x_syn = joined_encoder(x_syn_df, x_encoding)\n",
    "\n",
    "    x_syn = th.tensor(x_syn, dtype=th.float)\n",
    "    \n",
    "    y_pred, y_pred_proba = predict(model, x_syn, y_encoding=y_encoding)\n",
    "    if y_encoding == 'ordinal':\n",
    "        y_pred_proba = np.concatenate([np.ones_like(y_pred_proba[:,[0]]), y_pred_proba[:,:]], axis=1)\n",
    "    proba_df = using_multiindex(y_pred_proba, ['sample_idx', 'contribution']).rename(columns={'value': 'proba'})\n",
    "    x_syn_df['contribution_pred'] = y_pred\n",
    "    proba_df =  x_syn_df.merge(proba_df)\n",
    "    proba_df['predicted'] = proba_df['contribution_pred'] == proba_df['contribution']\n",
    "    proba_df = proba_df.drop(columns = ['contribution_pred'])\n",
    "    proba_df = add_labels(proba_df, {'set': 'train', 'cv_split': i})\n",
    "    syn_pred += proba_df.to_dict('records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577d496c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-05T12:51:59.416705Z",
     "iopub.status.busy": "2022-02-05T12:51:59.416056Z",
     "iopub.status.idle": "2022-02-05T12:51:59.707187Z",
     "shell.execute_reply": "2022-02-05T12:51:59.707548Z"
    },
    "papermill": {
     "duration": 0.300417,
     "end_time": "2022-02-05T12:51:59.707707",
     "exception": false,
     "start_time": "2022-02-05T12:51:59.407290",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "make_dir(output_path)\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics)\n",
    "metrics_df = add_labels(metrics_df, labels)\n",
    "metrics_df.to_parquet(os.path.join(output_path, 'metrics.parquet'))\n",
    "\n",
    "confusion_matrix_df = pd.DataFrame(confusion_matrix)\n",
    "confusion_matrix_df = add_labels(confusion_matrix_df, labels)\n",
    "confusion_matrix_df.to_parquet(os.path.join(output_path, 'confusion_matrix.parquet'))\n",
    "\n",
    "syn_pred_df = pd.DataFrame(syn_pred)\n",
    "syn_pred_df = add_labels(syn_pred_df, labels)\n",
    "syn_pred_df.to_parquet(os.path.join(output_path, 'synthetic_predicitions.parquet'))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b1198fd9370ee0cf82025240fa26724f68bfab1e3f74dbb4acdc06e7861d0dbe"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 11.770634,
   "end_time": "2022-02-05T12:52:00.032430",
   "environment_variables": {},
   "exception": null,
   "input_path": "neural.ipynb",
   "output_path": "neural.ipynb",
   "parameters": {
    "data": "../data/pilot1_player_round_slim.csv",
    "fraction_training": 1,
    "labels": {},
    "model_args": {
     "hidden_size": null,
     "n_layers": 1
    },
    "n_contributions": 21,
    "n_cross_val": 10,
    "n_punishments": 31,
    "optimizer_args": {
     "lr": 0.0001,
     "weight_decay": 0.00001
    },
    "output_path": "../data/dev",
    "train_args": {
     "batch_size": 40,
     "clamp_grad": null,
     "epochs": 100,
     "eval_period": 10
    },
    "x_encoding": [
     {
      "column": "prev_contribution",
      "encoding": "ordinal"
     },
     {
      "column": "prev_punishment",
      "encoding": "ordinal"
     }
    ],
    "y_encoding": "ordinal"
   },
   "start_time": "2022-02-05T12:51:48.261796",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
