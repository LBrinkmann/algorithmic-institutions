{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b91c1359",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-15T16:13:42.882229Z",
     "iopub.status.busy": "2022-02-15T16:13:42.881700Z",
     "iopub.status.idle": "2022-02-15T16:13:42.901064Z",
     "shell.execute_reply": "2022-02-15T16:13:42.900182Z"
    },
    "papermill": {
     "duration": 0.040083,
     "end_time": "2022-02-15T16:13:42.903748",
     "exception": false,
     "start_time": "2022-02-15T16:13:42.863665",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "x_encoding = [{\"encoding\": \"ordinal\", \"column\": \"prev_contribution\"}, {\"encoding\": \"ordinal\", \"column\": \"prev_punishment\"}]\n",
    "y_encoding = \"onehot\"\n",
    "n_contributions = 21\n",
    "n_punishments = 31\n",
    "n_cross_val = 2\n",
    "fraction_training = 1.0\n",
    "data = \"../data/pilot1_player_round_slim.csv\"\n",
    "output_path = \"../data/dev\"\n",
    "labels = {}\n",
    "model_args = {\"n_layers\": 2, \"hidden_size\": 40}\n",
    "optimizer_args = {\"lr\": 0.0001, \"weight_decay\": 1e-05}\n",
    "train_args = {\"epochs\": 100, \"batch_size\": 40, \"clamp_grad\": 1, \"eval_period\": 10}\n",
    "device = \"cuda\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44582683",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-15T16:13:42.924460Z",
     "iopub.status.busy": "2022-02-15T16:13:42.923968Z",
     "iopub.status.idle": "2022-02-15T16:13:43.653779Z",
     "shell.execute_reply": "2022-02-15T16:13:43.651937Z"
    },
    "papermill": {
     "duration": 0.743457,
     "end_time": "2022-02-15T16:13:43.657200",
     "exception": false,
     "start_time": "2022-02-15T16:13:42.913743",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from aimanager.model.cross_validation import split_xy, get_cross_validations, get_fraction_of_groups\n",
    "from aimanager.model.encoder import ordinal_to_int, onehot_to_int, joined_encoder, int_encode\n",
    "from aimanager.model.metrics import create_metrics, create_confusion_matrix\n",
    "from aimanager.model.synthesize_data import syn_con_pun\n",
    "from aimanager.utils.array_to_df import add_labels, using_multiindex\n",
    "from aimanager.utils.utils import make_dir\n",
    "\n",
    "output_path = os.path.join(output_path, 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3669dc3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-15T16:13:43.680006Z",
     "iopub.status.busy": "2022-02-15T16:13:43.679574Z",
     "iopub.status.idle": "2022-02-15T16:13:44.192758Z",
     "shell.execute_reply": "2022-02-15T16:13:44.192157Z"
    },
    "papermill": {
     "duration": 0.526456,
     "end_time": "2022-02-15T16:13:44.196178",
     "exception": false,
     "start_time": "2022-02-15T16:13:43.669722",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch as th\n",
    "from typing import Literal, Optional\n",
    "\n",
    "\n",
    "class FeedForwardLayer(th.nn.Module):\n",
    "    def __init__(\n",
    "            self, *,\n",
    "            input_size: int, \n",
    "            hidden_size: int, \n",
    "            dropout: Optional[float], \n",
    "            activation: Optional[Literal['relu', 'logit', 'softmax']]):\n",
    "        super(FeedForwardLayer, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size  = hidden_size\n",
    "        self.lin = th.nn.Linear(self.input_size, self.hidden_size)\n",
    "\n",
    "        if activation == 'relu':\n",
    "            self.activation = th.nn.ReLU()\n",
    "        elif activation == 'logit':\n",
    "            self.activation = th.nn.Logit()\n",
    "        elif activation == 'softmax':\n",
    "            self.activation = th.nn.Softmax()\n",
    "        else:\n",
    "            self.activation = None\n",
    "\n",
    "        if dropout:\n",
    "            self.dropout = th.nn.Dropout(dropout)\n",
    "        else:\n",
    "            self.dropout = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.lin(x)\n",
    "        if self.dropout:\n",
    "            x = self.dropout(x)\n",
    "        if self.activation:\n",
    "            x = self.activation(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "class MultiLayer(th.nn.Module):\n",
    "    def __init__(self, *, \n",
    "            n_layers: int, \n",
    "            hidden_size: Optional[int]=None, \n",
    "            input_size: int, \n",
    "            output_size: int, \n",
    "            dropout: Optional[float]=None):\n",
    "        super(MultiLayer, self).__init__()\n",
    "        \n",
    "        assert not ((hidden_size == None) and (n_layers > 1))\n",
    "\n",
    "        self.layers = th.nn.Sequential(\n",
    "            *(FeedForwardLayer(\n",
    "                input_size=hidden_size if i > 0 else input_size,\n",
    "                hidden_size=output_size if i == (n_layers - 1) else hidden_size,\n",
    "                dropout=dropout,\n",
    "                activation=None if i == (n_layers - 1) else 'relu'\n",
    "            )\n",
    "            for i in range(n_layers))\n",
    "        )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# weight_decay == L2 regularisation\n",
    "# https://stackoverflow.com/questions/42704283/adding-l1-l2-regularization-in-pytorch\n",
    "from typing import Literal\n",
    "\n",
    "def predict(model, x_enc):\n",
    "    y_pred_logit = model(x_enc)\n",
    "    if model.y_encoding == 'ordinal':\n",
    "        y_pred_proba = th.sigmoid(y_pred_logit).detach().cpu().numpy()\n",
    "        y_pred = ordinal_to_int(y_pred_proba)\n",
    "    elif model.y_encoding in ['onehot', 'numeric']: \n",
    "        y_pred_proba = th.nn.functional.softmax(y_pred_logit, dim=-1).detach().cpu().numpy()\n",
    "        y_pred = onehot_to_int(y_pred_proba)\n",
    "    return y_pred, y_pred_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6659c46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-15T16:13:44.221298Z",
     "iopub.status.busy": "2022-02-15T16:13:44.220802Z",
     "iopub.status.idle": "2022-02-15T16:13:44.264216Z",
     "shell.execute_reply": "2022-02-15T16:13:44.263653Z"
    },
    "papermill": {
     "duration": 0.057011,
     "end_time": "2022-02-15T16:13:44.267525",
     "exception": false,
     "start_time": "2022-02-15T16:13:44.210514",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(data)\n",
    "\n",
    "df['contribution'] = pd.Categorical(\n",
    "    df['contribution'], categories=np.arange(n_contributions), ordered=True\n",
    ")\n",
    "df['punishment'] = pd.Categorical(\n",
    "    df['punishment'], categories=np.arange(n_punishments), ordered=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "825924ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-15T16:13:44.293187Z",
     "iopub.status.busy": "2022-02-15T16:13:44.292690Z",
     "iopub.status.idle": "2022-02-15T16:13:44.327833Z",
     "shell.execute_reply": "2022-02-15T16:13:44.327271Z"
    },
    "papermill": {
     "duration": 0.048849,
     "end_time": "2022-02-15T16:13:44.331260",
     "exception": false,
     "start_time": "2022-02-15T16:13:44.282411",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "    def __init__(self):\n",
    "        self.metrics = []\n",
    "        self.confusion_matrix = []\n",
    "        self.synthetic_predicitions = []\n",
    "\n",
    "    def set_data(self,data):\n",
    "        self.data = data\n",
    "\n",
    "    def set_labels(self, **labels):\n",
    "        self.labels = labels\n",
    "\n",
    "    def eval_set(self, model, set_name):\n",
    "        model.eval()\n",
    "        y_pred, y_pred_proba = predict(model, self.data['x_enc'][set_name])\n",
    "        self.metrics += create_metrics(self.data['y_sr'][set_name], y_pred, set=set_name, **self.labels)\n",
    "        self.confusion_matrix += create_confusion_matrix(\n",
    "            self.data['y_sr'][set_name], y_pred, set=set_name, **self.labels)\n",
    "\n",
    "    def eval_sync(self, model):\n",
    "        model.eval()\n",
    "        x_syn_df = self.data['x_df']['syn'].copy()\n",
    "        y_pred, y_pred_proba = predict(model, self.data['x_enc']['syn'])\n",
    "        if model.y_encoding == 'ordinal':\n",
    "            y_pred_proba = np.concatenate([np.ones_like(y_pred_proba[:,[0]]), y_pred_proba[:,:]], axis=1)\n",
    "        proba_df = using_multiindex(y_pred_proba, ['sample_idx', 'contribution']).rename(columns={'value': 'proba'})\n",
    "        x_syn_df['contribution_pred'] = y_pred\n",
    "        proba_df =  x_syn_df.merge(proba_df)\n",
    "        proba_df['predicted'] = proba_df['contribution_pred'] == proba_df['contribution']\n",
    "        proba_df = proba_df.drop(columns = ['contribution_pred'])\n",
    "        proba_df = add_labels(proba_df, {'set': 'train', 'cv_split': i})\n",
    "        self.synthetic_predicitions += proba_df.to_dict('records')\n",
    "\n",
    "    def add_loss(self, loss):\n",
    "        self.metrics.append(dict(name='loss', value=loss, **self.labels))\n",
    "\n",
    "    def save(self, output_path, labels):\n",
    "        make_dir(output_path)\n",
    "        self._save_metric(self.metrics, 'metrics.parquet', output_path, labels)\n",
    "        self._save_metric(self.confusion_matrix, 'confusion_matrix.parquet', output_path, labels)\n",
    "        self._save_metric(self.synthetic_predicitions, 'synthetic_predicitions.parquet', output_path, labels)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def _save_metric(rec, filename, output_path, labels):\n",
    "        df = pd.DataFrame(rec)\n",
    "        df = add_labels(df, labels)\n",
    "        df.to_parquet(os.path.join(output_path, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f144b88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-15T16:13:44.354450Z",
     "iopub.status.busy": "2022-02-15T16:13:44.354021Z",
     "iopub.status.idle": "2022-02-15T16:14:11.080737Z",
     "shell.execute_reply": "2022-02-15T16:14:11.080147Z"
    },
    "papermill": {
     "duration": 26.739528,
     "end_time": "2022-02-15T16:14:11.084012",
     "exception": false,
     "start_time": "2022-02-15T16:13:44.344484",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV 0 | Epoch 0 | Loss 3.0282807069666244\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV 0 | Epoch 10 | Loss 2.737477055016686\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV 0 | Epoch 20 | Loss 2.299350770606714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV 0 | Epoch 30 | Loss 2.2090599833604165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV 0 | Epoch 40 | Loss 2.1588081803830232\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV 0 | Epoch 50 | Loss 2.1242162626455814\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV 0 | Epoch 60 | Loss 2.098835714950281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV 0 | Epoch 70 | Loss 2.078922360083636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV 0 | Epoch 80 | Loss 2.0625303825911354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV 0 | Epoch 90 | Loss 2.048712911588304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV 1 | Epoch 0 | Loss 2.952941936605117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV 1 | Epoch 10 | Loss 2.625563754053677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV 1 | Epoch 20 | Loss 2.3010482348063412\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV 1 | Epoch 30 | Loss 2.2091709024327644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV 1 | Epoch 40 | Loss 2.153635649558376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV 1 | Epoch 50 | Loss 2.1166726331062176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV 1 | Epoch 60 | Loss 2.0901624436325887\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV 1 | Epoch 70 | Loss 2.069773058593273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV 1 | Epoch 80 | Loss 2.0533034887383965\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV 1 | Epoch 90 | Loss 2.039355210214853\n"
     ]
    }
   ],
   "source": [
    "th_device = th.device(device)\n",
    "\n",
    "metrics = []\n",
    "confusion_matrix = []\n",
    "syn_pred = []\n",
    "ev = Evaluator()\n",
    "\n",
    "x_df, y_sr = split_xy(df)\n",
    "for i, split in enumerate(get_cross_validations(x_df, y_sr, n_cross_val)):\n",
    "    x_train_df, y_train_sr, x_test_df, y_test_sr = split\n",
    "    x_train_df, y_train_sr = get_fraction_of_groups(x_train_df, y_train_sr, fraction_training)\n",
    "    x_syn_df = syn_con_pun(n_contributions, n_punishments)\n",
    "    data = {\n",
    "        'x_df': {'train': x_train_df, 'syn': x_syn_df, 'test': x_test_df },\n",
    "        'y_sr': {'train': y_train_sr, 'test': y_test_sr }}\n",
    "    data['x_enc'] = {\n",
    "        k: th.tensor(joined_encoder(x, x_encoding), dtype=th.float, device=th_device)\n",
    "        for  k, x in data['x_df'].items()\n",
    "    }\n",
    "    data['y_enc'] = {\n",
    "        k: th.tensor(\n",
    "            int_encode(y, encoding=y_encoding),\n",
    "            dtype=th.long if y_encoding == 'numeric' else th.float, device=th_device)\n",
    "        for  k, y in data['y_sr'].items()\n",
    "    }\n",
    "    ev.set_data(data)\n",
    "\n",
    "    output_size = n_contributions if not y_encoding == 'ordinal' else n_contributions - 1\n",
    "    model = MultiLayer(\n",
    "        input_size=data['x_enc']['train'].shape[1], output_size=output_size, **model_args).to(th_device)\n",
    "    model.y_encoding = y_encoding\n",
    "    optimizer = th.optim.Adam(model.parameters(), **optimizer_args)\n",
    "\n",
    "    if y_encoding == 'ordinal':\n",
    "        loss_fn = th.nn.BCEWithLogitsLoss()\n",
    "    elif y_encoding in ['onehot', 'numeric']:\n",
    "        loss_fn = th.nn.CrossEntropyLoss()\n",
    "    \n",
    "    sum_loss = 0\n",
    "    n_steps = 0\n",
    "    batch_size = train_args['batch_size']\n",
    "\n",
    "    for e in range(train_args['epochs']):\n",
    "        ev.set_labels(cv_split=i, epoch=e)\n",
    "        model.train()\n",
    "        for start_idx in range(0, len(data['x_enc']['train']), batch_size):\n",
    "            tx = data['x_enc']['train'][start_idx:start_idx+batch_size]\n",
    "            ty = data['y_enc']['train'][start_idx:start_idx+batch_size]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            py = model(tx)\n",
    "            loss = loss_fn(py, ty)\n",
    "            loss.backward()\n",
    "\n",
    "            if train_args['clamp_grad']:\n",
    "                for param in model.parameters():\n",
    "                    param.grad.data.clamp_(-train_args['clamp_grad'], train_args['clamp_grad'])\n",
    "            optimizer.step()\n",
    "            sum_loss += loss.item()\n",
    "            n_steps +=1\n",
    "        \n",
    "        if e % train_args['eval_period'] == 0:\n",
    "            avg_loss = sum_loss/n_steps\n",
    "            print(f'CV {i} | Epoch {e} | Loss {avg_loss}')\n",
    "            ev.add_loss(avg_loss)\n",
    "            ev.eval_set(model, 'train')\n",
    "            ev.eval_set(model, 'test')\n",
    "            sum_loss = 0\n",
    "            n_steps = 0\n",
    "\n",
    "    ev.eval_sync(model)\n",
    "\n",
    "ev.save(output_path, labels)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b1198fd9370ee0cf82025240fa26724f68bfab1e3f74dbb4acdc06e7861d0dbe"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 30.003052,
   "end_time": "2022-02-15T16:14:11.718838",
   "environment_variables": {},
   "exception": null,
   "input_path": "neural.ipynb",
   "output_path": "neural.ipynb",
   "parameters": {
    "data": "../data/pilot1_player_round_slim.csv",
    "device": "cuda",
    "fraction_training": 1.0,
    "labels": {},
    "model_args": {
     "hidden_size": 40,
     "n_layers": 2
    },
    "n_contributions": 21,
    "n_cross_val": 2,
    "n_punishments": 31,
    "optimizer_args": {
     "lr": 0.0001,
     "weight_decay": 1e-05
    },
    "output_path": "../data/dev",
    "train_args": {
     "batch_size": 40,
     "clamp_grad": 1,
     "epochs": 100,
     "eval_period": 10
    },
    "x_encoding": [
     {
      "column": "prev_contribution",
      "encoding": "ordinal"
     },
     {
      "column": "prev_punishment",
      "encoding": "ordinal"
     }
    ],
    "y_encoding": "onehot"
   },
   "start_time": "2022-02-15T16:13:41.715786",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}