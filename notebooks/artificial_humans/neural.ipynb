{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1b78c98c",
   "metadata": {
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "y_encoding = \"onehot\"\n",
    "x_encoding = [\n",
    "    {\"name\": \"prev_contributions\", \"n_levels\": 21, \"encoding\": \"numeric\"},\n",
    "    {\"name\": \"prev_punishments\", \"n_levels\": 31, \"encoding\": \"numeric\"},\n",
    "]\n",
    "n_contributions = 21\n",
    "n_punishments = 31\n",
    "n_cross_val = 2\n",
    "fraction_training = 1.0\n",
    "data = \"../../data/experiments/pilot_random1_player_round_slim.csv\"\n",
    "output_path = \"../../data/training/dev\"\n",
    "labels = {}\n",
    "model_args = {\"n_layers\": 2, \"hidden_size\": 40}\n",
    "optimizer_args = {\"lr\": 0.0001, \"weight_decay\": 1e-05}\n",
    "train_args = {\"epochs\": 1000, \"batch_size\": 40, \"clamp_grad\": 1, \"eval_period\": 10}\n",
    "device = \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "44582683",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-19T16:41:27.956497Z",
     "iopub.status.busy": "2022-04-19T16:41:27.956108Z",
     "iopub.status.idle": "2022-04-19T16:41:30.638413Z",
     "shell.execute_reply": "2022-04-19T16:41:30.637779Z"
    },
    "papermill": {
     "duration": 2.691081,
     "end_time": "2022-04-19T16:41:30.640419",
     "exception": false,
     "start_time": "2022-04-19T16:41:27.949338",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch as th\n",
    "from aimanager.generic.data import create_syn_data, create_torch_data, get_cross_validations\n",
    "from aimanager.artificial_humans.artificial_humans import ArtificialHuman\n",
    "from aimanager.artificial_humans.evaluation import Evaluator\n",
    "\n",
    "output_path = os.path.join(output_path, 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d840a14e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-19T16:41:30.654333Z",
     "iopub.status.busy": "2022-04-19T16:41:30.654106Z",
     "iopub.status.idle": "2022-04-19T16:41:31.576933Z",
     "shell.execute_reply": "2022-04-19T16:41:31.575973Z"
    },
    "papermill": {
     "duration": 0.932767,
     "end_time": "2022-04-19T16:41:31.578400",
     "exception": true,
     "start_time": "2022-04-19T16:41:30.645633",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(data)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e6659c46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-15T16:13:44.221298Z",
     "iopub.status.busy": "2022-02-15T16:13:44.220802Z",
     "iopub.status.idle": "2022-02-15T16:13:44.264216Z",
     "shell.execute_reply": "2022-02-15T16:13:44.263653Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(data)\n",
    "\n",
    "\n",
    "data = create_torch_data(df)\n",
    "syn_data = create_syn_data(n_contribution=21, n_punishment=31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1f144b88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-15T16:13:44.354450Z",
     "iopub.status.busy": "2022-02-15T16:13:44.354021Z",
     "iopub.status.idle": "2022-02-15T16:14:11.080737Z",
     "shell.execute_reply": "2022-02-15T16:14:11.080147Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV 0 | Epoch 0 | Loss 3.016712725162506\n",
      "CV 0 | Epoch 10 | Loss 3.0002101838588713\n",
      "CV 0 | Epoch 20 | Loss 2.971599143743515\n",
      "CV 0 | Epoch 30 | Loss 2.94235720038414\n",
      "CV 0 | Epoch 40 | Loss 2.91300607919693\n",
      "CV 0 | Epoch 50 | Loss 2.8835654973983766\n",
      "CV 0 | Epoch 60 | Loss 2.858333694934845\n",
      "CV 0 | Epoch 70 | Loss 2.8373749971389772\n",
      "CV 0 | Epoch 80 | Loss 2.807744246721268\n",
      "CV 0 | Epoch 90 | Loss 2.787672257423401\n",
      "CV 0 | Epoch 100 | Loss 2.756312054395676\n",
      "CV 0 | Epoch 110 | Loss 2.727349102497101\n",
      "CV 0 | Epoch 120 | Loss 2.710282415151596\n",
      "CV 0 | Epoch 130 | Loss 2.680332916975021\n",
      "CV 0 | Epoch 140 | Loss 2.665174287557602\n",
      "CV 0 | Epoch 150 | Loss 2.6419453024864197\n",
      "CV 0 | Epoch 160 | Loss 2.6175924241542816\n",
      "CV 0 | Epoch 170 | Loss 2.6129270613193514\n",
      "CV 0 | Epoch 180 | Loss 2.584078860282898\n",
      "CV 0 | Epoch 190 | Loss 2.563257020711899\n",
      "CV 0 | Epoch 200 | Loss 2.548833453655243\n",
      "CV 0 | Epoch 210 | Loss 2.520298808813095\n",
      "CV 0 | Epoch 220 | Loss 2.51597346663475\n",
      "CV 0 | Epoch 230 | Loss 2.490815871953964\n",
      "CV 0 | Epoch 240 | Loss 2.480358284711838\n",
      "CV 0 | Epoch 250 | Loss 2.4817280411720275\n",
      "CV 0 | Epoch 260 | Loss 2.465586304664612\n",
      "CV 0 | Epoch 270 | Loss 2.4445669770240785\n",
      "CV 0 | Epoch 280 | Loss 2.4460107386112213\n",
      "CV 0 | Epoch 290 | Loss 2.421996533870697\n",
      "CV 0 | Epoch 300 | Loss 2.4051780998706818\n",
      "CV 0 | Epoch 310 | Loss 2.3879308462142945\n",
      "CV 0 | Epoch 320 | Loss 2.407673841714859\n",
      "CV 0 | Epoch 330 | Loss 2.395256572961807\n",
      "CV 0 | Epoch 340 | Loss 2.386257696151733\n",
      "CV 0 | Epoch 350 | Loss 2.385832118988037\n",
      "CV 0 | Epoch 360 | Loss 2.3978653609752656\n",
      "CV 0 | Epoch 370 | Loss 2.3781375348567964\n",
      "CV 0 | Epoch 380 | Loss 2.367905506491661\n",
      "CV 0 | Epoch 390 | Loss 2.361358219385147\n",
      "CV 0 | Epoch 400 | Loss 2.3720185220241548\n",
      "CV 0 | Epoch 410 | Loss 2.37047855257988\n",
      "CV 0 | Epoch 420 | Loss 2.360230267047882\n",
      "CV 0 | Epoch 430 | Loss 2.346227192878723\n",
      "CV 0 | Epoch 440 | Loss 2.352865272760391\n",
      "CV 0 | Epoch 450 | Loss 2.3550221264362334\n",
      "CV 0 | Epoch 460 | Loss 2.344870924949646\n",
      "CV 0 | Epoch 470 | Loss 2.3378644078969955\n",
      "CV 0 | Epoch 480 | Loss 2.3451781630516053\n",
      "CV 0 | Epoch 490 | Loss 2.344101297855377\n",
      "CV 0 | Epoch 500 | Loss 2.331047260761261\n",
      "CV 0 | Epoch 510 | Loss 2.331280106306076\n",
      "CV 0 | Epoch 520 | Loss 2.3198647648096085\n",
      "CV 0 | Epoch 530 | Loss 2.3264414012432098\n",
      "CV 0 | Epoch 540 | Loss 2.299960681796074\n",
      "CV 0 | Epoch 550 | Loss 2.3168679624795914\n",
      "CV 0 | Epoch 560 | Loss 2.3420819103717805\n",
      "CV 0 | Epoch 570 | Loss 2.30520883500576\n",
      "CV 0 | Epoch 580 | Loss 2.318381816148758\n",
      "CV 0 | Epoch 590 | Loss 2.3343421459197997\n",
      "CV 0 | Epoch 600 | Loss 2.2965652614831926\n",
      "CV 0 | Epoch 610 | Loss 2.307272028923035\n",
      "CV 0 | Epoch 620 | Loss 2.2919822305440904\n",
      "CV 0 | Epoch 630 | Loss 2.303686058521271\n",
      "CV 0 | Epoch 640 | Loss 2.3042288064956664\n",
      "CV 0 | Epoch 650 | Loss 2.3104090273380278\n",
      "CV 0 | Epoch 660 | Loss 2.3107512831687926\n",
      "CV 0 | Epoch 670 | Loss 2.3193018615245817\n",
      "CV 0 | Epoch 680 | Loss 2.2850701749324798\n",
      "CV 0 | Epoch 690 | Loss 2.2841493010520937\n",
      "CV 0 | Epoch 700 | Loss 2.2940050780773165\n",
      "CV 0 | Epoch 710 | Loss 2.2774017244577407\n",
      "CV 0 | Epoch 720 | Loss 2.2975614249706267\n",
      "CV 0 | Epoch 730 | Loss 2.2754322707653047\n",
      "CV 0 | Epoch 740 | Loss 2.281496512889862\n",
      "CV 0 | Epoch 750 | Loss 2.267200791835785\n",
      "CV 0 | Epoch 760 | Loss 2.262846738100052\n",
      "CV 0 | Epoch 770 | Loss 2.281559517979622\n",
      "CV 0 | Epoch 780 | Loss 2.276010271906853\n",
      "CV 0 | Epoch 790 | Loss 2.2715157121419907\n",
      "CV 0 | Epoch 800 | Loss 2.2690217941999435\n",
      "CV 0 | Epoch 810 | Loss 2.263697975873947\n",
      "CV 0 | Epoch 820 | Loss 2.279726380109787\n",
      "CV 0 | Epoch 830 | Loss 2.2566089600324633\n",
      "CV 0 | Epoch 840 | Loss 2.251273384690285\n",
      "CV 0 | Epoch 850 | Loss 2.266047179698944\n",
      "CV 0 | Epoch 860 | Loss 2.248155665397644\n",
      "CV 0 | Epoch 870 | Loss 2.2615049958229063\n",
      "CV 0 | Epoch 880 | Loss 2.258034211397171\n",
      "CV 0 | Epoch 890 | Loss 2.2487721711397173\n",
      "CV 0 | Epoch 900 | Loss 2.262150007486343\n",
      "CV 0 | Epoch 910 | Loss 2.276898729801178\n",
      "CV 0 | Epoch 920 | Loss 2.2629257649183274\n",
      "CV 0 | Epoch 930 | Loss 2.2618313401937487\n",
      "CV 0 | Epoch 940 | Loss 2.260418337583542\n",
      "CV 0 | Epoch 950 | Loss 2.242196115851402\n",
      "CV 0 | Epoch 960 | Loss 2.24375159740448\n",
      "CV 0 | Epoch 970 | Loss 2.2463392347097395\n",
      "CV 0 | Epoch 980 | Loss 2.243058502674103\n",
      "CV 0 | Epoch 990 | Loss 2.2439621925354003\n",
      "CV 1 | Epoch 0 | Loss 3.1151979565620422\n",
      "CV 1 | Epoch 10 | Loss 3.1030710875988006\n",
      "CV 1 | Epoch 20 | Loss 3.0768072426319124\n",
      "CV 1 | Epoch 30 | Loss 3.0523923218250273\n",
      "CV 1 | Epoch 40 | Loss 3.02704570889473\n",
      "CV 1 | Epoch 50 | Loss 3.0039787232875823\n",
      "CV 1 | Epoch 60 | Loss 2.9802544891834257\n",
      "CV 1 | Epoch 70 | Loss 2.956212055683136\n",
      "CV 1 | Epoch 80 | Loss 2.934819406270981\n",
      "CV 1 | Epoch 90 | Loss 2.9096415519714354\n",
      "CV 1 | Epoch 100 | Loss 2.8922739923000336\n",
      "CV 1 | Epoch 110 | Loss 2.868531364202499\n",
      "CV 1 | Epoch 120 | Loss 2.8443377196788786\n",
      "CV 1 | Epoch 130 | Loss 2.827401030063629\n",
      "CV 1 | Epoch 140 | Loss 2.8050150752067564\n",
      "CV 1 | Epoch 150 | Loss 2.7827497839927675\n",
      "CV 1 | Epoch 160 | Loss 2.7625892877578737\n",
      "CV 1 | Epoch 170 | Loss 2.740797770023346\n",
      "CV 1 | Epoch 180 | Loss 2.7118838906288145\n",
      "CV 1 | Epoch 190 | Loss 2.699468660354614\n",
      "CV 1 | Epoch 200 | Loss 2.6832249402999877\n",
      "CV 1 | Epoch 210 | Loss 2.6659886837005615\n",
      "CV 1 | Epoch 220 | Loss 2.6400761544704436\n",
      "CV 1 | Epoch 230 | Loss 2.6171331644058227\n",
      "CV 1 | Epoch 240 | Loss 2.601721864938736\n",
      "CV 1 | Epoch 250 | Loss 2.5922446846961975\n",
      "CV 1 | Epoch 260 | Loss 2.5684671819210054\n",
      "CV 1 | Epoch 270 | Loss 2.5574420928955077\n",
      "CV 1 | Epoch 280 | Loss 2.537166678905487\n",
      "CV 1 | Epoch 290 | Loss 2.5238748729228973\n",
      "CV 1 | Epoch 300 | Loss 2.514851039648056\n",
      "CV 1 | Epoch 310 | Loss 2.5008613407611846\n",
      "CV 1 | Epoch 320 | Loss 2.493118965625763\n",
      "CV 1 | Epoch 330 | Loss 2.4758679568767548\n",
      "CV 1 | Epoch 340 | Loss 2.4629710495471953\n",
      "CV 1 | Epoch 350 | Loss 2.4499043822288513\n",
      "CV 1 | Epoch 360 | Loss 2.439841616153717\n",
      "CV 1 | Epoch 370 | Loss 2.4382873833179475\n",
      "CV 1 | Epoch 380 | Loss 2.422574335336685\n",
      "CV 1 | Epoch 390 | Loss 2.397561436891556\n",
      "CV 1 | Epoch 400 | Loss 2.4013604491949083\n",
      "CV 1 | Epoch 410 | Loss 2.406462109088898\n",
      "CV 1 | Epoch 420 | Loss 2.4071619510650635\n",
      "CV 1 | Epoch 430 | Loss 2.405674475431442\n",
      "CV 1 | Epoch 440 | Loss 2.3787698656320573\n",
      "CV 1 | Epoch 450 | Loss 2.3809485375881194\n",
      "CV 1 | Epoch 460 | Loss 2.376803058385849\n",
      "CV 1 | Epoch 470 | Loss 2.386205732822418\n",
      "CV 1 | Epoch 480 | Loss 2.373782604932785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[E thread_pool.cpp:113] Exception in thread pool task: mutex lock failed: Invalid argument\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/levinbrinkmann/repros/algorithmic-institutions/notebooks/artificial_humans/neural.ipynb Cell 5'\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/levinbrinkmann/repros/algorithmic-institutions/notebooks/artificial_humans/neural.ipynb#ch0000004?line=66'>67</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(py, y_true)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/levinbrinkmann/repros/algorithmic-institutions/notebooks/artificial_humans/neural.ipynb#ch0000004?line=68'>69</a>\u001b[0m loss \u001b[39m=\u001b[39m (loss \u001b[39m*\u001b[39m mask)\u001b[39m.\u001b[39msum() \u001b[39m/\u001b[39m mask\u001b[39m.\u001b[39msum()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/levinbrinkmann/repros/algorithmic-institutions/notebooks/artificial_humans/neural.ipynb#ch0000004?line=70'>71</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/levinbrinkmann/repros/algorithmic-institutions/notebooks/artificial_humans/neural.ipynb#ch0000004?line=72'>73</a>\u001b[0m \u001b[39mif\u001b[39;00m train_args[\u001b[39m'\u001b[39m\u001b[39mclamp_grad\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/levinbrinkmann/repros/algorithmic-institutions/notebooks/artificial_humans/neural.ipynb#ch0000004?line=73'>74</a>\u001b[0m     \u001b[39mfor\u001b[39;00m param \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mparameters():\n",
      "File \u001b[0;32m~/repros/algorithmic-institutions/.venv/lib/python3.9/site-packages/torch/_tensor.py:307\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/levinbrinkmann/repros/algorithmic-institutions/.venv/lib/python3.9/site-packages/torch/_tensor.py?line=297'>298</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    <a href='file:///Users/levinbrinkmann/repros/algorithmic-institutions/.venv/lib/python3.9/site-packages/torch/_tensor.py?line=298'>299</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    <a href='file:///Users/levinbrinkmann/repros/algorithmic-institutions/.venv/lib/python3.9/site-packages/torch/_tensor.py?line=299'>300</a>\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    <a href='file:///Users/levinbrinkmann/repros/algorithmic-institutions/.venv/lib/python3.9/site-packages/torch/_tensor.py?line=300'>301</a>\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/levinbrinkmann/repros/algorithmic-institutions/.venv/lib/python3.9/site-packages/torch/_tensor.py?line=304'>305</a>\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    <a href='file:///Users/levinbrinkmann/repros/algorithmic-institutions/.venv/lib/python3.9/site-packages/torch/_tensor.py?line=305'>306</a>\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> <a href='file:///Users/levinbrinkmann/repros/algorithmic-institutions/.venv/lib/python3.9/site-packages/torch/_tensor.py?line=306'>307</a>\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/repros/algorithmic-institutions/.venv/lib/python3.9/site-packages/torch/autograd/__init__.py:154\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/levinbrinkmann/repros/algorithmic-institutions/.venv/lib/python3.9/site-packages/torch/autograd/__init__.py?line=150'>151</a>\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/levinbrinkmann/repros/algorithmic-institutions/.venv/lib/python3.9/site-packages/torch/autograd/__init__.py?line=151'>152</a>\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m--> <a href='file:///Users/levinbrinkmann/repros/algorithmic-institutions/.venv/lib/python3.9/site-packages/torch/autograd/__init__.py?line=153'>154</a>\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(\n\u001b[1;32m    <a href='file:///Users/levinbrinkmann/repros/algorithmic-institutions/.venv/lib/python3.9/site-packages/torch/autograd/__init__.py?line=154'>155</a>\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    <a href='file:///Users/levinbrinkmann/repros/algorithmic-institutions/.venv/lib/python3.9/site-packages/torch/autograd/__init__.py?line=155'>156</a>\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "th_device = th.device(device)\n",
    "\n",
    "metrics = []\n",
    "confusion_matrix = []\n",
    "syn_pred = []\n",
    "ev = Evaluator()\n",
    "\n",
    "for i, (train_data, test_data) in enumerate(get_cross_validations(data, n_cross_val)):\n",
    "    model = ArtificialHuman(\n",
    "        y_encoding=y_encoding, n_contributions=n_contributions, n_punishments=n_punishments, x_encoding=x_encoding,\n",
    "        **model_args).to(th_device)\n",
    "\n",
    "    train_data = {\n",
    "        **model.encode_x(**train_data),\n",
    "        **model.encode_y(**train_data),\n",
    "        **train_data\n",
    "    }\n",
    "    train_data = {\n",
    "        k: v.to(device)\n",
    "        for k, v in train_data.items()\n",
    "    }\n",
    "\n",
    "    test_data = {\n",
    "        **model.encode_x(**test_data),\n",
    "        **model.encode_y(**test_data),\n",
    "        **test_data\n",
    "    }\n",
    "    test_data = {\n",
    "        k: v.to(device)\n",
    "        for k, v in test_data.items()\n",
    "    }\n",
    "\n",
    "    syn_data_ = {\n",
    "        **model.encode_x(**syn_data),\n",
    "    }\n",
    "    syn_data_ = {\n",
    "        k: v.to(device)\n",
    "        for k, v in syn_data_.items()\n",
    "    }\n",
    "    ev.set_data(test=test_data, train=train_data, syn=syn_data_)\n",
    "\n",
    "    loss_fn = model.get_lossfn()\n",
    "\n",
    "    optimizer = th.optim.Adam(model.parameters(), **optimizer_args)\n",
    "    sum_loss = 0\n",
    "    n_steps = 0\n",
    "    batch_size = train_args['batch_size']\n",
    "\n",
    "    for e in range(train_args['epochs']):\n",
    "        ev.set_labels(cv_split=i, epoch=e)\n",
    "        model.train()\n",
    "        perm = th.randperm(train_data['ah_y_enc'].size(0))\n",
    "        for start_idx in range(0, train_data['ah_y_enc'].shape[0], batch_size):\n",
    "            end_idx = start_idx+batch_size\n",
    "            idx = perm[start_idx:end_idx]\n",
    "            batch_data = {\n",
    "                 k: v[idx]\n",
    "                for k, v in train_data.items()\n",
    "            }\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            py = model(**batch_data).flatten(end_dim=-2)\n",
    "            y_true = batch_data['ah_y_enc'].flatten(end_dim=-2)\n",
    "            mask = batch_data['valid'].flatten()\n",
    "\n",
    "            loss = loss_fn(py, y_true)\n",
    "\n",
    "            loss = (loss * mask).sum() / mask.sum()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            if train_args['clamp_grad']:\n",
    "                for param in model.parameters():\n",
    "                    param.grad.data.clamp_(-train_args['clamp_grad'], train_args['clamp_grad'])\n",
    "            optimizer.step()\n",
    "            sum_loss += loss.item()\n",
    "            n_steps +=1\n",
    "        \n",
    "        if e % train_args['eval_period'] == 0:\n",
    "            avg_loss = sum_loss/n_steps\n",
    "            print(f'CV {i} | Epoch {e} | Loss {avg_loss}')\n",
    "            ev.add_loss(avg_loss)\n",
    "            ev.eval_set(model, 'train')\n",
    "            ev.eval_set(model, 'test')\n",
    "            sum_loss = 0\n",
    "            n_steps = 0\n",
    "\n",
    "    ev.eval_sync(model)\n",
    "\n",
    "ev.save(output_path, labels)\n",
    "model_path = os.path.join(output_path, 'model.pt')\n",
    "model.save(model_path)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b1198fd9370ee0cf82025240fa26724f68bfab1e3f74dbb4acdc06e7861d0dbe"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5.747884,
   "end_time": "2022-04-19T16:41:32.101998",
   "environment_variables": {},
   "exception": true,
   "input_path": "notebooks/artificial_humans/neural.ipynb",
   "output_path": "notebooks/artificial_humans/neural.ipynb",
   "parameters": {
    "data": "../../data/experiments/pilot_random1_player_round_slim.csv",
    "device": "cpu",
    "fraction_training": 1,
    "labels": {},
    "model_args": {
     "hidden_size": 40,
     "n_layers": 2
    },
    "n_contributions": 21,
    "n_cross_val": 2,
    "n_punishments": 31,
    "optimizer_args": {
     "lr": 0.0001,
     "weight_decay": 0.00001
    },
    "output_path": "../../data/training/dev",
    "train_args": {
     "batch_size": 40,
     "clamp_grad": 1,
     "epochs": 1000,
     "eval_period": 10
    },
    "x_encoding": [
     {
      "encoding": "numeric",
      "n_levels": 21,
      "name": "prev_contributions"
     },
     {
      "encoding": "numeric",
      "n_levels": 31,
      "name": "prev_punishments"
     }
    ],
    "y_encoding": "onehot"
   },
   "start_time": "2022-04-19T16:41:26.354114",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
