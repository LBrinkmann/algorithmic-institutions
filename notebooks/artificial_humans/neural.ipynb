{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44008b65",
   "metadata": {
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "y_encoding = \"onehot\"\n",
    "x_encoding = [\n",
    "    {\"name\": \"prev_contributions\", \"n_levels\": 21, \"encoding\": \"onehot\"},\n",
    "    {\"name\": \"prev_punishments\", \"n_levels\": 31, \"encoding\": \"onehot\"},\n",
    "]\n",
    "n_contributions = 21\n",
    "n_punishments = 31\n",
    "n_cross_val = 2\n",
    "fraction_training = 1.0\n",
    "data = \"../../data/experiments/pilot_random1_player_round_slim.csv\"\n",
    "output_path = \"../../data/training/dev\"\n",
    "labels = {}\n",
    "model_args = {\"n_layers\": 2, \"hidden_size\": 40}\n",
    "optimizer_args = {\"lr\": 0.001, \"weight_decay\": 1e-05}\n",
    "train_args = {\"epochs\": 1000, \"batch_size\": 40, \"clamp_grad\": 1, \"eval_period\": 10}\n",
    "device = \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44582683",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-15T16:13:42.924460Z",
     "iopub.status.busy": "2022-02-15T16:13:42.923968Z",
     "iopub.status.idle": "2022-02-15T16:13:43.653779Z",
     "shell.execute_reply": "2022-02-15T16:13:43.651937Z"
    },
    "papermill": {
     "duration": 0.743457,
     "end_time": "2022-02-15T16:13:43.657200",
     "exception": false,
     "start_time": "2022-02-15T16:13:42.913743",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch as th\n",
    "from aimanager.generic.data import create_syn_data, create_torch_data, get_cross_validations\n",
    "from aimanager.artificial_humans.artificial_humans import ArtificialHuman\n",
    "from aimanager.artificial_humans.evaluation import Evaluator\n",
    "\n",
    "output_path = os.path.join(output_path, 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d840a14e",
   "metadata": {
    "papermill": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(data)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e6659c46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-15T16:13:44.221298Z",
     "iopub.status.busy": "2022-02-15T16:13:44.220802Z",
     "iopub.status.idle": "2022-02-15T16:13:44.264216Z",
     "shell.execute_reply": "2022-02-15T16:13:44.263653Z"
    },
    "papermill": {
     "duration": 0.057011,
     "end_time": "2022-02-15T16:13:44.267525",
     "exception": false,
     "start_time": "2022-02-15T16:13:44.210514",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(data)\n",
    "\n",
    "\n",
    "data = create_torch_data(df)\n",
    "syn_data = create_syn_data(n_contribution=21, n_punishment=31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f144b88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-15T16:13:44.354450Z",
     "iopub.status.busy": "2022-02-15T16:13:44.354021Z",
     "iopub.status.idle": "2022-02-15T16:14:11.080737Z",
     "shell.execute_reply": "2022-02-15T16:14:11.080147Z"
    },
    "papermill": {
     "duration": 26.739528,
     "end_time": "2022-02-15T16:14:11.084012",
     "exception": false,
     "start_time": "2022-02-15T16:13:44.344484",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV 0 | Epoch 0 | Loss 3.008167266845703\n",
      "CV 0 | Epoch 10 | Loss 2.9056486427783965\n",
      "CV 0 | Epoch 20 | Loss 2.6692069232463838\n",
      "CV 0 | Epoch 30 | Loss 2.348678308725357\n",
      "CV 0 | Epoch 40 | Loss 2.2045524954795837\n",
      "CV 0 | Epoch 50 | Loss 2.0594624400138857\n",
      "CV 0 | Epoch 60 | Loss 2.0139424443244933\n",
      "CV 0 | Epoch 70 | Loss 1.9515017569065094\n",
      "CV 0 | Epoch 80 | Loss 1.9002551585435867\n",
      "CV 0 | Epoch 90 | Loss 1.8951942831277848\n",
      "CV 0 | Epoch 100 | Loss 1.8717012673616409\n",
      "CV 0 | Epoch 110 | Loss 1.8645061403512955\n",
      "CV 0 | Epoch 120 | Loss 1.831791776418686\n",
      "CV 0 | Epoch 130 | Loss 1.8247600704431535\n",
      "CV 0 | Epoch 140 | Loss 1.8529171973466874\n",
      "CV 0 | Epoch 150 | Loss 1.8220557063817977\n",
      "CV 0 | Epoch 160 | Loss 1.8436237514019012\n",
      "CV 0 | Epoch 170 | Loss 1.805131185054779\n",
      "CV 0 | Epoch 180 | Loss 1.8566803723573684\n",
      "CV 0 | Epoch 190 | Loss 1.768166247010231\n",
      "CV 0 | Epoch 200 | Loss 1.793150845170021\n",
      "CV 0 | Epoch 210 | Loss 1.786346796154976\n",
      "CV 0 | Epoch 220 | Loss 1.7762766659259797\n",
      "CV 0 | Epoch 230 | Loss 1.797034627199173\n",
      "CV 0 | Epoch 240 | Loss 1.7853275060653686\n",
      "CV 0 | Epoch 250 | Loss 1.7797363847494125\n",
      "CV 0 | Epoch 260 | Loss 1.761296910047531\n",
      "CV 0 | Epoch 270 | Loss 1.7997527778148652\n",
      "CV 0 | Epoch 280 | Loss 1.7655324310064315\n",
      "CV 0 | Epoch 290 | Loss 1.768284010887146\n",
      "CV 0 | Epoch 300 | Loss 1.7833735674619675\n",
      "CV 0 | Epoch 310 | Loss 1.7632402569055556\n",
      "CV 0 | Epoch 320 | Loss 1.7500754952430726\n",
      "CV 0 | Epoch 330 | Loss 1.759340113401413\n",
      "CV 0 | Epoch 340 | Loss 1.7529021978378296\n",
      "CV 0 | Epoch 350 | Loss 1.7696100562810897\n",
      "CV 0 | Epoch 360 | Loss 1.753984698653221\n",
      "CV 0 | Epoch 370 | Loss 1.7516788810491561\n",
      "CV 0 | Epoch 380 | Loss 1.736577770113945\n",
      "CV 0 | Epoch 390 | Loss 1.730301919579506\n",
      "CV 0 | Epoch 400 | Loss 1.7393126010894775\n",
      "CV 0 | Epoch 410 | Loss 1.7292636156082153\n",
      "CV 0 | Epoch 420 | Loss 1.756906419992447\n",
      "CV 0 | Epoch 430 | Loss 1.7442343086004257\n",
      "CV 0 | Epoch 440 | Loss 1.7431866437196732\n",
      "CV 0 | Epoch 450 | Loss 1.7386222422122954\n",
      "CV 0 | Epoch 460 | Loss 1.726595374941826\n",
      "CV 0 | Epoch 470 | Loss 1.732402530312538\n",
      "CV 0 | Epoch 480 | Loss 1.7240680754184723\n",
      "CV 0 | Epoch 490 | Loss 1.7572962164878845\n",
      "CV 0 | Epoch 500 | Loss 1.7049061238765717\n",
      "CV 0 | Epoch 510 | Loss 1.7203751623630523\n",
      "CV 0 | Epoch 520 | Loss 1.7259668737649918\n",
      "CV 0 | Epoch 530 | Loss 1.7135360687971115\n",
      "CV 0 | Epoch 540 | Loss 1.7013571560382843\n",
      "CV 0 | Epoch 550 | Loss 1.7308438926935197\n",
      "CV 0 | Epoch 560 | Loss 1.7212426126003266\n",
      "CV 0 | Epoch 570 | Loss 1.700004056096077\n",
      "CV 0 | Epoch 580 | Loss 1.6938752830028534\n",
      "CV 0 | Epoch 590 | Loss 1.7154563426971436\n",
      "CV 0 | Epoch 600 | Loss 1.7242706805467605\n",
      "CV 0 | Epoch 610 | Loss 1.7238389194011687\n",
      "CV 0 | Epoch 620 | Loss 1.7024210393428802\n",
      "CV 0 | Epoch 630 | Loss 1.7035594254732132\n",
      "CV 0 | Epoch 640 | Loss 1.7088628619909287\n",
      "CV 0 | Epoch 650 | Loss 1.6809907972812652\n",
      "CV 0 | Epoch 660 | Loss 1.6815855711698533\n",
      "CV 0 | Epoch 670 | Loss 1.7248292446136475\n",
      "CV 0 | Epoch 680 | Loss 1.6790792614221572\n",
      "CV 0 | Epoch 690 | Loss 1.727666288614273\n",
      "CV 0 | Epoch 700 | Loss 1.6902453005313873\n",
      "CV 0 | Epoch 710 | Loss 1.683791321516037\n",
      "CV 0 | Epoch 720 | Loss 1.6836542427539825\n",
      "CV 0 | Epoch 730 | Loss 1.6832971185445786\n",
      "CV 0 | Epoch 740 | Loss 1.6629952579736709\n",
      "CV 0 | Epoch 750 | Loss 1.6805569350719451\n",
      "CV 0 | Epoch 760 | Loss 1.6821343183517456\n",
      "CV 0 | Epoch 770 | Loss 1.6774339109659195\n",
      "CV 0 | Epoch 780 | Loss 1.6842410087585449\n",
      "CV 0 | Epoch 790 | Loss 1.6784188121557235\n",
      "CV 0 | Epoch 800 | Loss 1.7106785714626311\n",
      "CV 0 | Epoch 810 | Loss 1.6602319777011871\n",
      "CV 0 | Epoch 820 | Loss 1.6746595174074173\n",
      "CV 0 | Epoch 830 | Loss 1.6624302685260772\n",
      "CV 0 | Epoch 840 | Loss 1.6505930304527283\n",
      "CV 0 | Epoch 850 | Loss 1.6834079712629317\n",
      "CV 0 | Epoch 860 | Loss 1.6603537976741791\n",
      "CV 0 | Epoch 870 | Loss 1.6842917889356612\n",
      "CV 0 | Epoch 880 | Loss 1.671595275402069\n",
      "CV 0 | Epoch 890 | Loss 1.6757180362939834\n",
      "CV 0 | Epoch 900 | Loss 1.6678081005811691\n",
      "CV 0 | Epoch 910 | Loss 1.6751443296670914\n",
      "CV 0 | Epoch 920 | Loss 1.6701107889413833\n",
      "CV 0 | Epoch 930 | Loss 1.6595728367567062\n",
      "CV 0 | Epoch 940 | Loss 1.6471127629280091\n",
      "CV 0 | Epoch 950 | Loss 1.6764066755771636\n",
      "CV 0 | Epoch 960 | Loss 1.6737512052059174\n",
      "CV 0 | Epoch 970 | Loss 1.6843981921672821\n",
      "CV 0 | Epoch 980 | Loss 1.6153258711099625\n",
      "CV 0 | Epoch 990 | Loss 1.6789108514785767\n",
      "CV 1 | Epoch 0 | Loss 3.0549670457839966\n",
      "CV 1 | Epoch 10 | Loss 2.9635928153991697\n",
      "CV 1 | Epoch 20 | Loss 2.7277548551559447\n",
      "CV 1 | Epoch 30 | Loss 2.418205976486206\n",
      "CV 1 | Epoch 40 | Loss 2.207624059915543\n",
      "CV 1 | Epoch 50 | Loss 2.072326749563217\n",
      "CV 1 | Epoch 60 | Loss 2.0075135946273805\n",
      "CV 1 | Epoch 70 | Loss 1.9548635631799698\n",
      "CV 1 | Epoch 80 | Loss 1.92762551009655\n",
      "CV 1 | Epoch 90 | Loss 1.8845632463693618\n",
      "CV 1 | Epoch 100 | Loss 1.8676720678806304\n",
      "CV 1 | Epoch 110 | Loss 1.8609833329916001\n",
      "CV 1 | Epoch 120 | Loss 1.8312778502702713\n",
      "CV 1 | Epoch 130 | Loss 1.8037163019180298\n",
      "CV 1 | Epoch 140 | Loss 1.8224311470985413\n",
      "CV 1 | Epoch 150 | Loss 1.8248746514320373\n",
      "CV 1 | Epoch 160 | Loss 1.8104115158319474\n",
      "CV 1 | Epoch 170 | Loss 1.8342009395360948\n",
      "CV 1 | Epoch 180 | Loss 1.8034644365310668\n",
      "CV 1 | Epoch 190 | Loss 1.8000522583723069\n",
      "CV 1 | Epoch 200 | Loss 1.7846151530742644\n",
      "CV 1 | Epoch 210 | Loss 1.7841622889041902\n",
      "CV 1 | Epoch 220 | Loss 1.7824191987514495\n",
      "CV 1 | Epoch 230 | Loss 1.7758297592401504\n",
      "CV 1 | Epoch 240 | Loss 1.8061889499425887\n",
      "CV 1 | Epoch 250 | Loss 1.7939152032136918\n",
      "CV 1 | Epoch 260 | Loss 1.7939007610082627\n",
      "CV 1 | Epoch 270 | Loss 1.7965066701173782\n",
      "CV 1 | Epoch 280 | Loss 1.7489332377910614\n",
      "CV 1 | Epoch 290 | Loss 1.807735800743103\n",
      "CV 1 | Epoch 300 | Loss 1.7532207816839218\n",
      "CV 1 | Epoch 310 | Loss 1.7662155240774156\n",
      "CV 1 | Epoch 320 | Loss 1.7787198156118393\n",
      "CV 1 | Epoch 330 | Loss 1.7648316860198974\n",
      "CV 1 | Epoch 340 | Loss 1.7934863358736037\n",
      "CV 1 | Epoch 350 | Loss 1.7737028896808624\n",
      "CV 1 | Epoch 360 | Loss 1.762459173798561\n",
      "CV 1 | Epoch 370 | Loss 1.7698662906885148\n",
      "CV 1 | Epoch 380 | Loss 1.7606752187013626\n",
      "CV 1 | Epoch 390 | Loss 1.7603148639202117\n",
      "CV 1 | Epoch 400 | Loss 1.7467961668968202\n",
      "CV 1 | Epoch 410 | Loss 1.719277125597\n",
      "CV 1 | Epoch 420 | Loss 1.7372267872095108\n",
      "CV 1 | Epoch 430 | Loss 1.7581430286169053\n",
      "CV 1 | Epoch 440 | Loss 1.7568913906812669\n",
      "CV 1 | Epoch 450 | Loss 1.72515509724617\n",
      "CV 1 | Epoch 460 | Loss 1.7550903618335725\n",
      "CV 1 | Epoch 470 | Loss 1.709060162305832\n",
      "CV 1 | Epoch 480 | Loss 1.7413725852966309\n",
      "CV 1 | Epoch 490 | Loss 1.7439029544591904\n",
      "CV 1 | Epoch 500 | Loss 1.7254862248897553\n",
      "CV 1 | Epoch 510 | Loss 1.7263858437538147\n",
      "CV 1 | Epoch 520 | Loss 1.7165116161108016\n",
      "CV 1 | Epoch 530 | Loss 1.671497678756714\n",
      "CV 1 | Epoch 540 | Loss 1.7250099956989289\n",
      "CV 1 | Epoch 550 | Loss 1.7246351540088654\n",
      "CV 1 | Epoch 560 | Loss 1.6933360904455186\n",
      "CV 1 | Epoch 570 | Loss 1.7152073442935944\n",
      "CV 1 | Epoch 580 | Loss 1.706614837050438\n",
      "CV 1 | Epoch 590 | Loss 1.7102469772100448\n",
      "CV 1 | Epoch 600 | Loss 1.7040894895792007\n",
      "CV 1 | Epoch 610 | Loss 1.728691041469574\n",
      "CV 1 | Epoch 620 | Loss 1.7103274822235108\n",
      "CV 1 | Epoch 630 | Loss 1.7466887384653091\n",
      "CV 1 | Epoch 640 | Loss 1.7071377635002136\n",
      "CV 1 | Epoch 650 | Loss 1.7130352884531022\n",
      "CV 1 | Epoch 660 | Loss 1.7061624467372893\n",
      "CV 1 | Epoch 670 | Loss 1.6838679671287538\n",
      "CV 1 | Epoch 680 | Loss 1.6841624617576598\n",
      "CV 1 | Epoch 690 | Loss 1.681289866566658\n",
      "CV 1 | Epoch 700 | Loss 1.699308741092682\n",
      "CV 1 | Epoch 710 | Loss 1.7090823292732238\n",
      "CV 1 | Epoch 720 | Loss 1.6892887026071548\n",
      "CV 1 | Epoch 730 | Loss 1.674195870757103\n",
      "CV 1 | Epoch 740 | Loss 1.6906797111034393\n",
      "CV 1 | Epoch 750 | Loss 1.6828398019075395\n",
      "CV 1 | Epoch 760 | Loss 1.6653909504413604\n",
      "CV 1 | Epoch 770 | Loss 1.697216087579727\n",
      "CV 1 | Epoch 780 | Loss 1.6812038958072661\n",
      "CV 1 | Epoch 790 | Loss 1.6719404697418212\n",
      "CV 1 | Epoch 800 | Loss 1.6907281637191773\n",
      "CV 1 | Epoch 810 | Loss 1.6719969451427459\n",
      "CV 1 | Epoch 820 | Loss 1.7046241581439971\n",
      "CV 1 | Epoch 830 | Loss 1.640721082687378\n",
      "CV 1 | Epoch 840 | Loss 1.682091271877289\n",
      "CV 1 | Epoch 850 | Loss 1.663248074054718\n",
      "CV 1 | Epoch 860 | Loss 1.67589328289032\n",
      "CV 1 | Epoch 870 | Loss 1.68374103307724\n",
      "CV 1 | Epoch 880 | Loss 1.6799887657165526\n",
      "CV 1 | Epoch 890 | Loss 1.6674767196178437\n",
      "CV 1 | Epoch 900 | Loss 1.6725909620523454\n",
      "CV 1 | Epoch 910 | Loss 1.6833236873149873\n",
      "CV 1 | Epoch 920 | Loss 1.6543099373579024\n",
      "CV 1 | Epoch 930 | Loss 1.656935003399849\n",
      "CV 1 | Epoch 940 | Loss 1.6756728917360306\n",
      "CV 1 | Epoch 950 | Loss 1.6589676529169082\n",
      "CV 1 | Epoch 960 | Loss 1.6572378486394883\n",
      "CV 1 | Epoch 970 | Loss 1.6540030598640443\n",
      "CV 1 | Epoch 980 | Loss 1.670785367488861\n",
      "CV 1 | Epoch 990 | Loss 1.6606788694858552\n"
     ]
    }
   ],
   "source": [
    "th_device = th.device(device)\n",
    "\n",
    "metrics = []\n",
    "confusion_matrix = []\n",
    "syn_pred = []\n",
    "ev = Evaluator()\n",
    "\n",
    "for i, (train_data, test_data) in enumerate(get_cross_validations(data, n_cross_val)):\n",
    "    model = ArtificialHuman(\n",
    "        y_encoding=y_encoding, n_contributions=n_contributions, n_punishments=n_punishments, x_encoding=x_encoding,\n",
    "        **model_args).to(th_device)\n",
    "\n",
    "    train_data = {\n",
    "        **model.encode_x(**train_data),\n",
    "        **model.encode_y(**train_data),\n",
    "        **train_data\n",
    "    }\n",
    "    train_data = {\n",
    "        k: v.to(device)\n",
    "        for k, v in train_data.items()\n",
    "    }\n",
    "\n",
    "    test_data = {\n",
    "        **model.encode_x(**test_data),\n",
    "        **model.encode_y(**test_data),\n",
    "        **test_data\n",
    "    }\n",
    "    test_data = {\n",
    "        k: v.to(device)\n",
    "        for k, v in test_data.items()\n",
    "    }\n",
    "\n",
    "    syn_data_ = {\n",
    "        **model.encode_x(**syn_data),\n",
    "    }\n",
    "    syn_data_ = {\n",
    "        k: v.to(device)\n",
    "        for k, v in syn_data_.items()\n",
    "    }\n",
    "    ev.set_data(test=test_data, train=train_data, syn=syn_data_)\n",
    "\n",
    "    loss_fn = model.get_lossfn()\n",
    "\n",
    "    optimizer = th.optim.Adam(model.parameters(), **optimizer_args)\n",
    "    sum_loss = 0\n",
    "    n_steps = 0\n",
    "    batch_size = train_args['batch_size']\n",
    "\n",
    "\n",
    "\n",
    "    for e in range(train_args['epochs']):\n",
    "        ev.set_labels(cv_split=i, epoch=e)\n",
    "        model.train()\n",
    "        for start_idx in range(0, train_data['ah_y_enc'].shape[0], batch_size):\n",
    "            perm = th.randperm(train_data['ah_y_enc'].size(0))\n",
    "            idx = perm[:batch_size]\n",
    "            batch_data = {\n",
    "                 k: v[idx]\n",
    "                for k, v in train_data.items()\n",
    "            }\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            py = model(**batch_data).flatten(end_dim=-2)\n",
    "            y_true = batch_data['ah_y_enc'].flatten(end_dim=-2)\n",
    "            mask = batch_data['valid'].flatten()\n",
    "\n",
    "            loss = loss_fn(py, y_true)\n",
    "\n",
    "            loss = (loss * mask).sum() / mask.sum()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            if train_args['clamp_grad']:\n",
    "                for param in model.parameters():\n",
    "                    param.grad.data.clamp_(-train_args['clamp_grad'], train_args['clamp_grad'])\n",
    "            optimizer.step()\n",
    "            sum_loss += loss.item()\n",
    "            n_steps +=1\n",
    "        \n",
    "        if e % train_args['eval_period'] == 0:\n",
    "            avg_loss = sum_loss/n_steps\n",
    "            print(f'CV {i} | Epoch {e} | Loss {avg_loss}')\n",
    "            ev.add_loss(avg_loss)\n",
    "            ev.eval_set(model, 'train')\n",
    "            ev.eval_set(model, 'test')\n",
    "            sum_loss = 0\n",
    "            n_steps = 0\n",
    "\n",
    "    ev.eval_sync(model)\n",
    "\n",
    "ev.save(output_path, labels)\n",
    "model_path = os.path.join(output_path, 'model.pt')\n",
    "model.save(model_path)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b1198fd9370ee0cf82025240fa26724f68bfab1e3f74dbb4acdc06e7861d0dbe"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 30.003052,
   "end_time": "2022-02-15T16:14:11.718838",
   "environment_variables": {},
   "exception": null,
   "input_path": "notebooks/artificial_humans/neural.ipynb",
   "output_path": "notebooks/artificial_humans/neural.ipynb",
   "parameters": {
    "data": "../../data/experiments/pilot_random1_player_round_slim.csv",
    "device": "cpu",
    "fraction_training": 1,
    "labels": {},
    "model_args": {
     "hidden_size": 40,
     "n_layers": 2
    },
    "n_contributions": 21,
    "n_cross_val": 2,
    "n_punishments": 31,
    "optimizer_args": {
     "lr": 0.0001,
     "weight_decay": 0.00001
    },
    "output_path": "../../data/training/dev",
    "train_args": {
     "batch_size": 40,
     "clamp_grad": 1,
     "epochs": 100,
     "eval_period": 10
    },
    "x_encoding": [
     {
      "encoding": "onehot",
      "n_levels": 21,
      "name": "prev_contributions"
     },
     {
      "encoding": "onehot",
      "n_levels": 31,
      "name": "prev_punishments"
     }
    ],
    "y_encoding": "onehot"
   },
   "start_time": "2022-02-15T16:13:41.715786",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
