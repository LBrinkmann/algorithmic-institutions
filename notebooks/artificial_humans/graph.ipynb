{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6d3f840",
   "metadata": {
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "x_encoding = [\n",
    "    {\"name\": \"prev_contributions\", \"n_levels\": 21, \"encoding\": \"numeric\"},\n",
    "    {\"name\": \"prev_punishments\", \"n_levels\": 31, \"encoding\": \"numeric\"},\n",
    "    {\"name\": \"round_number\", \"n_levels\": 16, \"encoding\": \"numeric\"},\n",
    "    {\"name\": \"prev_common_good\", \"norm\": 128, \"etype\": \"float\"},\n",
    "    {\"name\": \"prev_valid\", \"etype\": \"bool\"},\n",
    "]\n",
    "n_contributions = 21\n",
    "n_punishments = 31\n",
    "n_cross_val = 2\n",
    "fraction_training = 1.0\n",
    "data_file = \"../../data/experiments/pilot_random1_player_round_slim.csv\"\n",
    "output_path = \"../../data/training/dev\"\n",
    "labels = {}\n",
    "model_name = \"graph\"\n",
    "model_args = {\"n_layers\": 2, \"hidden_size\": 40}\n",
    "optimizer_args = {\"lr\": 0.0001, \"weight_decay\": 1e-05}\n",
    "train_args = {\"epochs\": 1000, \"batch_size\": 40, \"clamp_grad\": 1, \"eval_period\": 10}\n",
    "device = \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "44582683",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-19T16:41:27.956497Z",
     "iopub.status.busy": "2022-04-19T16:41:27.956108Z",
     "iopub.status.idle": "2022-04-19T16:41:30.638413Z",
     "shell.execute_reply": "2022-04-19T16:41:30.637779Z"
    },
    "papermill": {
     "duration": 2.691081,
     "end_time": "2022-04-19T16:41:30.640419",
     "exception": false,
     "start_time": "2022-04-19T16:41:27.949338",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch as th\n",
    "from aimanager.generic.data import create_syn_data, create_torch_data, get_cross_validations\n",
    "from aimanager.artificial_humans import AH_MODELS\n",
    "from aimanager.artificial_humans.evaluation import Evaluator\n",
    "from aimanager.utils.array_to_df import using_multiindex\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "output_path = os.path.join(output_path, 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e6659c46",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-15T16:13:44.221298Z",
     "iopub.status.busy": "2022-02-15T16:13:44.220802Z",
     "iopub.status.idle": "2022-02-15T16:13:44.264216Z",
     "shell.execute_reply": "2022-02-15T16:13:44.263653Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_file)\n",
    "\n",
    "\n",
    "data = create_torch_data(df)\n",
    "syn_data = create_syn_data(n_contribution=21, n_punishment=31)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660e103c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Sequential as Seq, Linear as Lin, Tanh\n",
    "import torch as th\n",
    "from torch_scatter import scatter_mean\n",
    "from torch_geometric.nn import MetaLayer\n",
    "\n",
    "class EdgeModel(th.nn.Module):\n",
    "    def __init__(self, x_features, edge_features, u_features, out_features, n_rounds):\n",
    "        super().__init__()\n",
    "        in_features = 2*x_features+edge_features+u_features\n",
    "        self.edge_mlp = Seq(Lin(in_features=in_features, out_features=out_features), Tanh())\n",
    "\n",
    "    def forward(self, src, dest, edge_attr, u, batch):\n",
    "        # src, dest: [E, F_x], where E is the number of edges.\n",
    "        # edge_attr: [E, F_e]\n",
    "        # u: [B, F_u], where B is the number of graphs.\n",
    "        # batch: [E] with max entry B - 1.\n",
    "        out = th.cat([src, dest, edge_attr, u[batch]], 1)\n",
    "        return self.edge_mlp(out)\n",
    "\n",
    "\n",
    "class NodeModel(th.nn.Module):\n",
    "    def __init__(self, x_features, edge_features, u_features, out_features, n_rounds):\n",
    "        super().__init__()\n",
    "        in_features = x_features+edge_features+u_features\n",
    "        self.node_mlp = Seq(Lin(in_features=in_features, out_features=out_features), Tanh())\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, u, batch):\n",
    "        # x: [N, F_x], where N is the number of nodes.\n",
    "        # edge_index: [2, E] with max entry N - 1.\n",
    "        # edge_attr: [E, F_e]\n",
    "        # u: [B, F_u]\n",
    "        # batch: [N] with max entry B - 1.\n",
    "        row, col = edge_index\n",
    "        out = scatter_mean(edge_attr, col, dim=0, dim_size=x.size(0))\n",
    "        out = th.cat([x, out, u[batch]], dim=1)\n",
    "        return self.node_mlp(out)\n",
    "\n",
    "class GlobalModel(th.nn.Module):\n",
    "    def __init__(self, x_features, edge_features, u_features, out_features, n_rounds):\n",
    "        super().__init__()\n",
    "        in_features = u_features+x_features\n",
    "        self.global_mlp = Seq(Lin(in_features=in_features, out_features=out_features), Tanh())\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, u, batch):\n",
    "        # x: [N, F_x], where N is the number of nodes.\n",
    "        # edge_index: [2, E] with max entry N - 1.\n",
    "        # edge_attr: [E, F_e]\n",
    "        # u: [B, F_u]\n",
    "        # batch: [N] with max entry B - 1.\n",
    "        out = th.cat([u, scatter_mean(x, batch, dim=0)], dim=1)\n",
    "        return self.global_mlp(out)\n",
    "\n",
    "class GraphNetwork(th.nn.Module):\n",
    "    def __init__(\n",
    "            self, x_features, edge_features, u_features, n_rounds, n_units, out_features, y_encoding):\n",
    "        super().__init__()\n",
    "        self.out_features = out_features\n",
    "        self.y_encoding = y_encoding\n",
    "        self.op1 = MetaLayer(\n",
    "            EdgeModel(\n",
    "                x_features=x_features, edge_features=edge_features, \n",
    "                u_features=u_features, out_features=n_units, n_rounds=n_rounds), \n",
    "            NodeModel(\n",
    "                x_features=x_features, edge_features=n_units, \n",
    "                u_features=u_features, out_features=n_units, n_rounds=n_rounds), \n",
    "            GlobalModel(\n",
    "                x_features=n_units, edge_features=n_units, \n",
    "                u_features=u_features, out_features=n_units, n_rounds=n_rounds)\n",
    "        )\n",
    "        self.op2 = MetaLayer(\n",
    "            EdgeModel(\n",
    "                x_features=n_units, edge_features=n_units, \n",
    "                u_features=n_units, out_features=n_units, n_rounds=n_rounds), \n",
    "            NodeModel(\n",
    "                x_features=n_units, edge_features=n_units, \n",
    "                u_features=n_units, out_features=out_features, n_rounds=n_rounds), \n",
    "            None\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, edge_index, edge_attr, u, batch):\n",
    "        x, edge_attr, u = self.op1(x, edge_index, edge_attr, u, batch)\n",
    "        x, edge_attr, u = self.op2(x, edge_index, edge_attr, u, batch)\n",
    "        return x\n",
    "\n",
    "AH_MODELS['graph'] = GraphNetwork√ü"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f144b88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-15T16:13:44.354450Z",
     "iopub.status.busy": "2022-02-15T16:13:44.354021Z",
     "iopub.status.idle": "2022-02-15T16:14:11.080737Z",
     "shell.execute_reply": "2022-02-15T16:14:11.080147Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135\n",
      "CV 0 | Epoch 0 | Loss 3.115206480026245\n",
      "CV 0 | Epoch 10 | Loss 3.0957974672317503\n",
      "CV 0 | Epoch 20 | Loss 3.0666980385780334\n",
      "CV 0 | Epoch 30 | Loss 3.0376184403896334\n",
      "CV 0 | Epoch 40 | Loss 3.009313315153122\n",
      "CV 0 | Epoch 50 | Loss 2.980314838886261\n",
      "CV 0 | Epoch 60 | Loss 2.9513711750507357\n",
      "CV 0 | Epoch 70 | Loss 2.9172434389591215\n",
      "CV 0 | Epoch 80 | Loss 2.8855172634124755\n",
      "CV 0 | Epoch 90 | Loss 2.8571395874023438\n",
      "CV 0 | Epoch 100 | Loss 2.821339935064316\n",
      "CV 0 | Epoch 110 | Loss 2.7867123603820803\n",
      "CV 0 | Epoch 120 | Loss 2.753342938423157\n",
      "CV 0 | Epoch 130 | Loss 2.7268692314624787\n",
      "CV 0 | Epoch 140 | Loss 2.6794094324111937\n",
      "CV 0 | Epoch 150 | Loss 2.665165364742279\n",
      "CV 0 | Epoch 160 | Loss 2.625143676996231\n",
      "CV 0 | Epoch 170 | Loss 2.5937470972537993\n",
      "CV 0 | Epoch 180 | Loss 2.57244336605072\n",
      "CV 0 | Epoch 190 | Loss 2.5502085030078887\n",
      "CV 0 | Epoch 200 | Loss 2.5398742318153382\n",
      "CV 0 | Epoch 210 | Loss 2.5190130293369295\n",
      "CV 0 | Epoch 220 | Loss 2.49164297580719\n",
      "CV 0 | Epoch 230 | Loss 2.4791192710399628\n",
      "CV 0 | Epoch 240 | Loss 2.466059762239456\n",
      "CV 0 | Epoch 250 | Loss 2.4603545546531675\n",
      "CV 0 | Epoch 260 | Loss 2.4586063086986543\n",
      "CV 0 | Epoch 270 | Loss 2.429061108827591\n",
      "CV 0 | Epoch 280 | Loss 2.4319620788097382\n",
      "CV 0 | Epoch 290 | Loss 2.4279969573020934\n",
      "CV 0 | Epoch 300 | Loss 2.41852924823761\n",
      "CV 0 | Epoch 310 | Loss 2.418073296546936\n",
      "CV 0 | Epoch 320 | Loss 2.4078825771808625\n",
      "CV 0 | Epoch 330 | Loss 2.389535218477249\n",
      "CV 0 | Epoch 340 | Loss 2.404777628183365\n",
      "CV 0 | Epoch 350 | Loss 2.400478893518448\n",
      "CV 0 | Epoch 360 | Loss 2.377877300977707\n",
      "CV 0 | Epoch 370 | Loss 2.38896124958992\n",
      "CV 0 | Epoch 380 | Loss 2.35613067150116\n",
      "CV 0 | Epoch 390 | Loss 2.371334818005562\n",
      "CV 0 | Epoch 400 | Loss 2.356059378385544\n",
      "CV 0 | Epoch 410 | Loss 2.3553552448749544\n",
      "CV 0 | Epoch 420 | Loss 2.344926792383194\n",
      "CV 0 | Epoch 430 | Loss 2.3519753515720367\n",
      "CV 0 | Epoch 440 | Loss 2.3553178757429123\n",
      "CV 0 | Epoch 450 | Loss 2.32229123711586\n",
      "CV 0 | Epoch 460 | Loss 2.3293352514505385\n",
      "CV 0 | Epoch 470 | Loss 2.3277630269527436\n",
      "CV 0 | Epoch 480 | Loss 2.3274669528007506\n",
      "CV 0 | Epoch 490 | Loss 2.3042730152606965\n",
      "CV 0 | Epoch 500 | Loss 2.317604738473892\n",
      "CV 0 | Epoch 510 | Loss 2.284350723028183\n",
      "CV 0 | Epoch 520 | Loss 2.313753843307495\n",
      "CV 0 | Epoch 530 | Loss 2.30177825987339\n",
      "CV 0 | Epoch 540 | Loss 2.2954196989536286\n",
      "CV 0 | Epoch 550 | Loss 2.27976356446743\n",
      "CV 0 | Epoch 560 | Loss 2.3029437005519866\n",
      "CV 0 | Epoch 570 | Loss 2.2831007301807404\n",
      "CV 0 | Epoch 580 | Loss 2.288251852989197\n",
      "CV 0 | Epoch 590 | Loss 2.2917437613010407\n",
      "CV 0 | Epoch 600 | Loss 2.2836366415023805\n",
      "CV 0 | Epoch 610 | Loss 2.2697778970003126\n",
      "CV 0 | Epoch 620 | Loss 2.2619692116975783\n",
      "CV 0 | Epoch 630 | Loss 2.2671879887580872\n",
      "CV 0 | Epoch 640 | Loss 2.268136340379715\n",
      "CV 0 | Epoch 650 | Loss 2.2648718118667603\n",
      "CV 0 | Epoch 660 | Loss 2.259901538491249\n",
      "CV 0 | Epoch 670 | Loss 2.2593311309814452\n",
      "CV 0 | Epoch 680 | Loss 2.2634458154439927\n",
      "CV 0 | Epoch 690 | Loss 2.27554988861084\n",
      "CV 0 | Epoch 700 | Loss 2.23620765209198\n",
      "CV 0 | Epoch 710 | Loss 2.250484085083008\n",
      "CV 0 | Epoch 720 | Loss 2.2392856568098067\n",
      "CV 0 | Epoch 730 | Loss 2.239807653427124\n",
      "CV 0 | Epoch 740 | Loss 2.2444214463233947\n",
      "CV 0 | Epoch 750 | Loss 2.2152448505163194\n",
      "CV 0 | Epoch 760 | Loss 2.256651481986046\n",
      "CV 0 | Epoch 770 | Loss 2.225168099999428\n",
      "CV 0 | Epoch 780 | Loss 2.231620192527771\n",
      "CV 0 | Epoch 790 | Loss 2.230086678266525\n",
      "CV 0 | Epoch 800 | Loss 2.223938465118408\n",
      "CV 0 | Epoch 810 | Loss 2.2215582937002183\n",
      "CV 0 | Epoch 820 | Loss 2.228239822387695\n",
      "CV 0 | Epoch 830 | Loss 2.2218436896800995\n",
      "CV 0 | Epoch 840 | Loss 2.228189504146576\n",
      "CV 0 | Epoch 850 | Loss 2.222962936758995\n",
      "CV 0 | Epoch 860 | Loss 2.2134114801883698\n",
      "CV 0 | Epoch 870 | Loss 2.2095605045557023\n",
      "CV 0 | Epoch 880 | Loss 2.232699143886566\n",
      "CV 0 | Epoch 890 | Loss 2.2039091140031815\n",
      "CV 0 | Epoch 900 | Loss 2.230544862151146\n",
      "CV 0 | Epoch 910 | Loss 2.1988559275865556\n",
      "CV 0 | Epoch 920 | Loss 2.2061639755964277\n",
      "CV 0 | Epoch 930 | Loss 2.2013395756483076\n",
      "CV 0 | Epoch 940 | Loss 2.180186924338341\n",
      "CV 0 | Epoch 950 | Loss 2.1867948293685915\n",
      "CV 0 | Epoch 960 | Loss 2.1988752484321594\n",
      "CV 0 | Epoch 970 | Loss 2.1978473484516146\n",
      "CV 0 | Epoch 980 | Loss 2.1976610004901884\n",
      "CV 0 | Epoch 990 | Loss 2.1831372559070585\n",
      "135\n",
      "CV 1 | Epoch 0 | Loss 3.2523444294929504\n",
      "CV 1 | Epoch 10 | Loss 3.2284771740436553\n",
      "CV 1 | Epoch 20 | Loss 3.1930106341838838\n",
      "CV 1 | Epoch 30 | Loss 3.166291880607605\n",
      "CV 1 | Epoch 40 | Loss 3.137337100505829\n",
      "CV 1 | Epoch 50 | Loss 3.1084728598594666\n",
      "CV 1 | Epoch 60 | Loss 3.0803574085235597\n",
      "CV 1 | Epoch 70 | Loss 3.047680324316025\n",
      "CV 1 | Epoch 80 | Loss 3.01830992102623\n",
      "CV 1 | Epoch 90 | Loss 2.9867193758487702\n",
      "CV 1 | Epoch 100 | Loss 2.9532104313373564\n",
      "CV 1 | Epoch 110 | Loss 2.9193916022777557\n",
      "CV 1 | Epoch 120 | Loss 2.882156175374985\n",
      "CV 1 | Epoch 130 | Loss 2.851964122056961\n",
      "CV 1 | Epoch 140 | Loss 2.8177076876163483\n",
      "CV 1 | Epoch 150 | Loss 2.7840208172798158\n",
      "CV 1 | Epoch 160 | Loss 2.7377172112464905\n",
      "CV 1 | Epoch 170 | Loss 2.705582720041275\n",
      "CV 1 | Epoch 180 | Loss 2.665409433841705\n",
      "CV 1 | Epoch 190 | Loss 2.6296684622764586\n",
      "CV 1 | Epoch 200 | Loss 2.597640556097031\n",
      "CV 1 | Epoch 210 | Loss 2.5582413256168364\n",
      "CV 1 | Epoch 220 | Loss 2.522696393728256\n",
      "CV 1 | Epoch 230 | Loss 2.506886887550354\n",
      "CV 1 | Epoch 240 | Loss 2.492684042453766\n",
      "CV 1 | Epoch 250 | Loss 2.4643264949321746\n",
      "CV 1 | Epoch 260 | Loss 2.454686921834946\n",
      "CV 1 | Epoch 270 | Loss 2.4426947236061096\n",
      "CV 1 | Epoch 280 | Loss 2.4051823914051056\n",
      "CV 1 | Epoch 290 | Loss 2.430289614200592\n",
      "CV 1 | Epoch 300 | Loss 2.4239717453718184\n",
      "CV 1 | Epoch 310 | Loss 2.409915494918823\n",
      "CV 1 | Epoch 320 | Loss 2.409641134738922\n",
      "CV 1 | Epoch 330 | Loss 2.385805344581604\n",
      "CV 1 | Epoch 340 | Loss 2.4087628722190857\n",
      "CV 1 | Epoch 350 | Loss 2.388979434967041\n",
      "CV 1 | Epoch 360 | Loss 2.3943855822086335\n",
      "CV 1 | Epoch 370 | Loss 2.370925045013428\n",
      "CV 1 | Epoch 380 | Loss 2.377282387018204\n",
      "CV 1 | Epoch 390 | Loss 2.3749710977077485\n",
      "CV 1 | Epoch 400 | Loss 2.3663636207580567\n",
      "CV 1 | Epoch 410 | Loss 2.3666108012199403\n",
      "CV 1 | Epoch 420 | Loss 2.349843513965607\n",
      "CV 1 | Epoch 430 | Loss 2.3605244517326356\n",
      "CV 1 | Epoch 440 | Loss 2.354597330093384\n",
      "CV 1 | Epoch 450 | Loss 2.350688338279724\n",
      "CV 1 | Epoch 460 | Loss 2.3415451407432557\n",
      "CV 1 | Epoch 470 | Loss 2.3373814642429354\n",
      "CV 1 | Epoch 480 | Loss 2.334148946404457\n",
      "CV 1 | Epoch 490 | Loss 2.3274703502655028\n",
      "CV 1 | Epoch 500 | Loss 2.32229323387146\n",
      "CV 1 | Epoch 510 | Loss 2.3274027049541473\n",
      "CV 1 | Epoch 520 | Loss 2.322120261192322\n",
      "CV 1 | Epoch 530 | Loss 2.3217201113700865\n",
      "CV 1 | Epoch 540 | Loss 2.3023715257644652\n",
      "CV 1 | Epoch 550 | Loss 2.3212854474782945\n",
      "CV 1 | Epoch 560 | Loss 2.315723565220833\n",
      "CV 1 | Epoch 570 | Loss 2.320423421263695\n",
      "CV 1 | Epoch 580 | Loss 2.304093712568283\n",
      "CV 1 | Epoch 590 | Loss 2.2901907533407213\n",
      "CV 1 | Epoch 600 | Loss 2.2852130830287933\n",
      "CV 1 | Epoch 610 | Loss 2.295195829868317\n",
      "CV 1 | Epoch 620 | Loss 2.306444936990738\n",
      "CV 1 | Epoch 630 | Loss 2.2897118508815764\n",
      "CV 1 | Epoch 640 | Loss 2.2831416189670564\n",
      "CV 1 | Epoch 650 | Loss 2.2845909714698793\n",
      "CV 1 | Epoch 660 | Loss 2.279073366522789\n",
      "CV 1 | Epoch 670 | Loss 2.2818537056446075\n",
      "CV 1 | Epoch 680 | Loss 2.2607849717140196\n",
      "CV 1 | Epoch 690 | Loss 2.2690608978271483\n",
      "CV 1 | Epoch 700 | Loss 2.2683579206466673\n",
      "CV 1 | Epoch 710 | Loss 2.2737975239753725\n",
      "CV 1 | Epoch 720 | Loss 2.263316589593887\n",
      "CV 1 | Epoch 730 | Loss 2.2694921523332594\n",
      "CV 1 | Epoch 740 | Loss 2.2740727424621583\n",
      "CV 1 | Epoch 750 | Loss 2.274351328611374\n",
      "CV 1 | Epoch 760 | Loss 2.2633409678936003\n",
      "CV 1 | Epoch 770 | Loss 2.2544964820146562\n",
      "CV 1 | Epoch 780 | Loss 2.2532102286815645\n",
      "CV 1 | Epoch 790 | Loss 2.2562197506427766\n",
      "CV 1 | Epoch 800 | Loss 2.2469289630651472\n",
      "CV 1 | Epoch 810 | Loss 2.2536991357803347\n",
      "CV 1 | Epoch 820 | Loss 2.242489364743233\n",
      "CV 1 | Epoch 830 | Loss 2.239577516913414\n",
      "CV 1 | Epoch 840 | Loss 2.233991539478302\n",
      "CV 1 | Epoch 850 | Loss 2.2336738616228105\n",
      "CV 1 | Epoch 860 | Loss 2.228227213025093\n",
      "CV 1 | Epoch 870 | Loss 2.2457653164863585\n",
      "CV 1 | Epoch 880 | Loss 2.240547609329224\n",
      "CV 1 | Epoch 890 | Loss 2.2386355102062225\n",
      "CV 1 | Epoch 900 | Loss 2.2165959924459457\n",
      "CV 1 | Epoch 910 | Loss 2.228529065847397\n",
      "CV 1 | Epoch 920 | Loss 2.221451061964035\n",
      "CV 1 | Epoch 930 | Loss 2.222691512107849\n",
      "CV 1 | Epoch 940 | Loss 2.2218375742435454\n",
      "CV 1 | Epoch 950 | Loss 2.2235813558101656\n",
      "CV 1 | Epoch 960 | Loss 2.2024894535541533\n",
      "CV 1 | Epoch 970 | Loss 2.2184053480625154\n",
      "CV 1 | Epoch 980 | Loss 2.2111982643604278\n",
      "CV 1 | Epoch 990 | Loss 2.227074259519577\n"
     ]
    }
   ],
   "source": [
    "th_device = th.device(device)\n",
    "\n",
    "metrics = []\n",
    "confusion_matrix = []\n",
    "syn_pred = []\n",
    "ev = Evaluator()\n",
    "\n",
    "th_device = th.device(device)\n",
    "\n",
    "syn_index = ['prev_punishments', 'prev_contributions']\n",
    "\n",
    "def create_fully_connected(n_nodes):\n",
    "    return th.tensor([[i,j]\n",
    "        for i in range(n_nodes)\n",
    "        for j in range(n_nodes)\n",
    "    ]).T\n",
    "\n",
    "def encode(model, data, *, mask=True, index=False, x_encode=True, y_encode=True, u_encode=False, device, n_player=4):\n",
    "    data = {\n",
    "        'mask': data['valid'] if mask else None,\n",
    "        'x': model.x_encoder(**data) if x_encode else None,\n",
    "        'y_enc': model.y_encoder(**data) if y_encode else None,\n",
    "        'y': data['contributions'] if y_encode else None,\n",
    "        'u': model.u_encoder(**data) if u_encode else None,\n",
    "        'info': th.stack([data[c] for c in syn_index], dim=-1) if index else None,\n",
    "    }\n",
    "    data = {\n",
    "        k: v.to(device)\n",
    "        for k, v in data.items()\n",
    "        if v is not None\n",
    "    }\n",
    "\n",
    "    edge_attr = th.zeros(n_player*n_player,0)\n",
    "    edge_index = create_fully_connected(n_player)\n",
    "\n",
    "    n_episodes = list(data.values())[0].shape[0]\n",
    "    dataset = [\n",
    "        Data(**{k: v[i] for k, v in data.items()}, edge_attr=edge_attr, edge_index=edge_index, idx=i, group_idx=i, num_nodes=n_player)\n",
    "        for i in range(n_episodes)\n",
    "    ]\n",
    "    return dataset\n",
    "\n",
    "\n",
    "for i, (train_data, test_data) in enumerate(get_cross_validations(data, n_cross_val, fraction_training)):\n",
    "    model = AH_MODELS[model_name](\n",
    "        n_contributions=n_contributions, n_punishments=n_punishments, x_encoding=x_encoding,\n",
    "        **model_args).to(th_device)\n",
    "\n",
    "    train_data_ = encode(model, train_data, mask=True, device=th_device)\n",
    "    test_data_ = encode(model, test_data, mask=True, device=th_device)\n",
    "    syn_data_ = encode(model, syn_data, mask=False, y_encode=False, index=True, device=th_device)\n",
    "\n",
    "    print(len(train_data_))\n",
    "\n",
    "    syn_df = using_multiindex(\n",
    "        Batch.from_data_list(syn_data_)['info'], ['idx', 'round_number'], syn_index)\n",
    "\n",
    "    ev.set_data(test=test_data_, train=train_data_, syn=syn_data_, syn_df=syn_df)\n",
    "\n",
    "    optimizer = th.optim.Adam(model.parameters(), **optimizer_args)\n",
    "    loss_fn = th.nn.CrossEntropyLoss(reduction='none')\n",
    "    sum_loss = 0\n",
    "    n_steps = 0\n",
    "\n",
    "    for e in range(train_args['epochs']):\n",
    "        ev.set_labels(cv_split=i, epoch=e)\n",
    "        model.train()\n",
    "        for j, batch_data in enumerate(iter(DataLoader(train_data_, shuffle=True, batch_size=train_args['batch_size']))):\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            py = model(batch_data).flatten(end_dim=-2)\n",
    "            y_true = batch_data['y_enc'].flatten(end_dim=-2)\n",
    "            mask = batch_data['mask'].flatten()\n",
    "            loss = loss_fn(py, y_true)\n",
    "            loss = (loss * mask).sum() / mask.sum()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            if train_args['clamp_grad']:\n",
    "                for param in model.parameters():\n",
    "                    param.grad.data.clamp_(-train_args['clamp_grad'], train_args['clamp_grad'])\n",
    "            optimizer.step()\n",
    "            sum_loss += loss.item()\n",
    "            n_steps +=1\n",
    "        \n",
    "        if e % train_args['eval_period'] == 0:\n",
    "            avg_loss = sum_loss/n_steps\n",
    "            print(f'CV {i} | Epoch {e} | Loss {avg_loss}')\n",
    "            ev.add_loss(avg_loss)\n",
    "            ev.eval_set(model, 'train')\n",
    "            ev.eval_set(model, 'test')\n",
    "            sum_loss = 0\n",
    "            n_steps = 0\n",
    "\n",
    "    ev.eval_sync(model, syn_index=syn_index)\n",
    "\n",
    "ev.save(output_path, labels)\n",
    "model_path = os.path.join(output_path, 'model.pt')\n",
    "model.save(model_path)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b1198fd9370ee0cf82025240fa26724f68bfab1e3f74dbb4acdc06e7861d0dbe"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 5.747884,
   "end_time": "2022-04-19T16:41:32.101998",
   "environment_variables": {},
   "exception": true,
   "input_path": "notebooks/artificial_humans/graph.ipynb",
   "output_path": "notebooks/artificial_humans/graph.ipynb",
   "parameters": {
    "data_file": "../../data/experiments/pilot_random1_player_round_slim.csv",
    "device": "cpu",
    "fraction_training": 1,
    "labels": {},
    "model_args": {
     "hidden_size": 40,
     "n_layers": 2
    },
    "model_name": "mlp",
    "n_contributions": 21,
    "n_cross_val": 2,
    "n_punishments": 31,
    "optimizer_args": {
     "lr": 0.0001,
     "weight_decay": 0.00001
    },
    "output_path": "../../data/training/dev",
    "train_args": {
     "batch_size": 40,
     "clamp_grad": 1,
     "epochs": 1000,
     "eval_period": 10
    },
    "x_encoding": [
     {
      "encoding": "numeric",
      "n_levels": 21,
      "name": "prev_contributions"
     },
     {
      "encoding": "numeric",
      "n_levels": 31,
      "name": "prev_punishments"
     },
     {
      "encoding": "numeric",
      "n_levels": 16,
      "name": "round_number"
     },
     {
      "etype": "float",
      "name": "prev_common_good",
      "norm": 128
     },
     {
      "etype": "bool",
      "name": "prev_valid"
     }
    ]
   },
   "start_time": "2022-04-19T16:41:26.354114",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
